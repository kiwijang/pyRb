{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 21:15:09.309730: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 21:15:09.375754: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-20 21:15:09.720400: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 21:15:09.720436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 21:15:09.720439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split#將資料分成訓練組及測試組\n",
    "import os\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "dataframe = pd.read_csv('df_USE_lstm_train.csv',\n",
    "                         encoding = \"Big5\",na_filter=True).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>公司碼</th>\n",
       "      <th>TSE 產業別</th>\n",
       "      <th>年月</th>\n",
       "      <th>X01_Ln資產總額</th>\n",
       "      <th>X02_流動佔總資產比率</th>\n",
       "      <th>X03_流動佔營業淨收入比率</th>\n",
       "      <th>X04_流動比率</th>\n",
       "      <th>X05_速動比率</th>\n",
       "      <th>X06_總資產週轉率</th>\n",
       "      <th>X07_現金流量比率</th>\n",
       "      <th>...</th>\n",
       "      <th>X14_ROE</th>\n",
       "      <th>X15_利息保障倍數</th>\n",
       "      <th>X16_稅前淨利率</th>\n",
       "      <th>X17_負債比率</th>\n",
       "      <th>X18_長期負債總比率</th>\n",
       "      <th>X19_ROA</th>\n",
       "      <th>N01_是否四大</th>\n",
       "      <th>Y_繼續經營疑慮</th>\n",
       "      <th>Lstm</th>\n",
       "      <th>Datamining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1101</td>\n",
       "      <td>1</td>\n",
       "      <td>200803.0</td>\n",
       "      <td>17.860166</td>\n",
       "      <td>0.300649</td>\n",
       "      <td>3.188306</td>\n",
       "      <td>191.69</td>\n",
       "      <td>149.68</td>\n",
       "      <td>0.09</td>\n",
       "      <td>8.36</td>\n",
       "      <td>...</td>\n",
       "      <td>1.67</td>\n",
       "      <td>4.31</td>\n",
       "      <td>11.31</td>\n",
       "      <td>46.86</td>\n",
       "      <td>0.311761</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1101</td>\n",
       "      <td>1</td>\n",
       "      <td>200806.0</td>\n",
       "      <td>17.811181</td>\n",
       "      <td>0.284241</td>\n",
       "      <td>2.644814</td>\n",
       "      <td>127.04</td>\n",
       "      <td>102.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>4.89</td>\n",
       "      <td>...</td>\n",
       "      <td>2.70</td>\n",
       "      <td>5.09</td>\n",
       "      <td>10.78</td>\n",
       "      <td>51.40</td>\n",
       "      <td>0.290287</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>1</td>\n",
       "      <td>200809.0</td>\n",
       "      <td>17.637446</td>\n",
       "      <td>0.247127</td>\n",
       "      <td>2.032282</td>\n",
       "      <td>115.39</td>\n",
       "      <td>81.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>16.79</td>\n",
       "      <td>...</td>\n",
       "      <td>1.48</td>\n",
       "      <td>3.34</td>\n",
       "      <td>7.04</td>\n",
       "      <td>52.06</td>\n",
       "      <td>0.306412</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1101</td>\n",
       "      <td>1</td>\n",
       "      <td>200812.0</td>\n",
       "      <td>17.594278</td>\n",
       "      <td>0.234187</td>\n",
       "      <td>2.496075</td>\n",
       "      <td>111.71</td>\n",
       "      <td>84.44</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.08</td>\n",
       "      <td>4.31</td>\n",
       "      <td>51.70</td>\n",
       "      <td>0.307405</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1101</td>\n",
       "      <td>1</td>\n",
       "      <td>200903.0</td>\n",
       "      <td>17.697198</td>\n",
       "      <td>0.251429</td>\n",
       "      <td>2.684157</td>\n",
       "      <td>125.29</td>\n",
       "      <td>96.44</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7.61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.76</td>\n",
       "      <td>51.81</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46704</th>\n",
       "      <td>9962</td>\n",
       "      <td>10</td>\n",
       "      <td>201612.0</td>\n",
       "      <td>13.596558</td>\n",
       "      <td>0.713315</td>\n",
       "      <td>1.300904</td>\n",
       "      <td>628.99</td>\n",
       "      <td>132.60</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-27.24</td>\n",
       "      <td>...</td>\n",
       "      <td>3.17</td>\n",
       "      <td>407.33</td>\n",
       "      <td>5.92</td>\n",
       "      <td>13.18</td>\n",
       "      <td>0.018407</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46705</th>\n",
       "      <td>9962</td>\n",
       "      <td>10</td>\n",
       "      <td>201703.0</td>\n",
       "      <td>13.634926</td>\n",
       "      <td>0.734175</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>870.96</td>\n",
       "      <td>203.10</td>\n",
       "      <td>0.58</td>\n",
       "      <td>68.72</td>\n",
       "      <td>...</td>\n",
       "      <td>4.12</td>\n",
       "      <td>271.80</td>\n",
       "      <td>7.50</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46706</th>\n",
       "      <td>9962</td>\n",
       "      <td>10</td>\n",
       "      <td>201706.0</td>\n",
       "      <td>13.678957</td>\n",
       "      <td>0.748912</td>\n",
       "      <td>1.573833</td>\n",
       "      <td>466.70</td>\n",
       "      <td>105.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>35.69</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-150.78</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>17.81</td>\n",
       "      <td>0.017651</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46707</th>\n",
       "      <td>9962</td>\n",
       "      <td>10</td>\n",
       "      <td>201709.0</td>\n",
       "      <td>13.564384</td>\n",
       "      <td>0.719438</td>\n",
       "      <td>1.428747</td>\n",
       "      <td>805.51</td>\n",
       "      <td>219.06</td>\n",
       "      <td>0.48</td>\n",
       "      <td>76.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>76.93</td>\n",
       "      <td>1.60</td>\n",
       "      <td>10.84</td>\n",
       "      <td>0.019082</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46708</th>\n",
       "      <td>9962</td>\n",
       "      <td>10</td>\n",
       "      <td>201712.0</td>\n",
       "      <td>13.642764</td>\n",
       "      <td>0.738271</td>\n",
       "      <td>1.595750</td>\n",
       "      <td>628.01</td>\n",
       "      <td>143.90</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-11.10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.11</td>\n",
       "      <td>442.32</td>\n",
       "      <td>4.69</td>\n",
       "      <td>13.65</td>\n",
       "      <td>0.018916</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46709 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        公司碼  TSE 產業別        年月  X01_Ln資產總額  X02_流動佔總資產比率  X03_流動佔營業淨收入比率  \\\n",
       "0      1101        1  200803.0   17.860166      0.300649        3.188306   \n",
       "1      1101        1  200806.0   17.811181      0.284241        2.644814   \n",
       "2      1101        1  200809.0   17.637446      0.247127        2.032282   \n",
       "3      1101        1  200812.0   17.594278      0.234187        2.496075   \n",
       "4      1101        1  200903.0   17.697198      0.251429        2.684157   \n",
       "...     ...      ...       ...         ...           ...             ...   \n",
       "46704  9962       10  201612.0   13.596558      0.713315        1.300904   \n",
       "46705  9962       10  201703.0   13.634926      0.734175        1.263158   \n",
       "46706  9962       10  201706.0   13.678957      0.748912        1.573833   \n",
       "46707  9962       10  201709.0   13.564384      0.719438        1.428747   \n",
       "46708  9962       10  201712.0   13.642764      0.738271        1.595750   \n",
       "\n",
       "       X04_流動比率  X05_速動比率  X06_總資產週轉率  X07_現金流量比率  ...  X14_ROE  X15_利息保障倍數  \\\n",
       "0        191.69    149.68        0.09        8.36  ...     1.67        4.31   \n",
       "1        127.04    102.02        0.11        4.89  ...     2.70        5.09   \n",
       "2        115.39     81.03        0.12       16.79  ...     1.48        3.34   \n",
       "3        111.71     84.44        0.09        3.01  ...     0.90        2.08   \n",
       "4        125.29     96.44        0.10        7.61  ...     0.53        1.81   \n",
       "...         ...       ...         ...         ...  ...      ...         ...   \n",
       "46704    628.99    132.60        0.56      -27.24  ...     3.17      407.33   \n",
       "46705    870.96    203.10        0.58       68.72  ...     4.12      271.80   \n",
       "46706    466.70    105.00        0.48       35.69  ...    -0.94     -150.78   \n",
       "46707    805.51    219.06        0.48       76.82  ...     0.77       76.93   \n",
       "46708    628.01    143.90        0.47      -11.10  ...     2.11      442.32   \n",
       "\n",
       "       X16_稅前淨利率  X17_負債比率  X18_長期負債總比率  X19_ROA  N01_是否四大  Y_繼續經營疑慮  Lstm  \\\n",
       "0          11.31     46.86     0.311761     1.13         1         0     1   \n",
       "1          10.78     51.40     0.290287     1.58         1         0     1   \n",
       "2           7.04     52.06     0.306412     0.98         1         0     1   \n",
       "3           4.31     51.70     0.307405     0.72         1         0     1   \n",
       "4           2.76     51.81     0.317400     0.50         1         0     1   \n",
       "...          ...       ...          ...      ...       ...       ...   ...   \n",
       "46704       5.92     13.18     0.018407     2.79         0         0     1   \n",
       "46705       7.50     10.24     0.018091     3.65         0         0     1   \n",
       "46706      -2.03     17.81     0.017651    -0.80         0         0     2   \n",
       "46707       1.60     10.84     0.019082     0.67         0         0     2   \n",
       "46708       4.69     13.65     0.018916     1.85         0         0     2   \n",
       "\n",
       "       Datamining  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "...           ...  \n",
       "46704           1  \n",
       "46705           1  \n",
       "46706           1  \n",
       "46707           1  \n",
       "46708           1  \n",
       "\n",
       "[46709 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########################驗證集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_va = pd.read_csv('df_USE_final_Verification.csv',\n",
    "                         encoding = \"Big5\",na_filter=True).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25629, 26)\n",
      "(25629, 23)\n",
      "0    25476\n",
      "1      153\n",
      "Name: Y_繼續經營疑慮, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataframe_va.shape)\n",
    "\n",
    "dataframe_va=dataframe_va.drop(['TSE 產業別','Lstm','Datamining'], axis=1)#刪除非必要欄位\n",
    "\n",
    "total_va = dataframe_va.shape  #資料總變數與總資料筆數\n",
    "total_class_va = dataframe_va.Y_繼續經營疑慮.value_counts()\n",
    "print(total_va)\n",
    "print(total_class_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_va = []\n",
    "Y_va = []\n",
    "j_va = 0\n",
    "for j in range(len(dataframe_va )-5):\n",
    "    if dataframe_va .iloc[j,0] == dataframe_va .iloc[j+5,0]:  \n",
    "        X_va.append(dataframe_va.iloc[j:j+4,2:22].values)  #scaled_dataset_\n",
    "        Y_va.append(dataframe_va.iloc[j+5,22])\n",
    "    j_va += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    17236\n",
      "1       67\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Y_va_c =pd.DataFrame(Y_va)\n",
    "total_class_va_c = Y_va_c.value_counts()\n",
    "print(total_class_va_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17303, 4, 20)\n",
      "(17303,)\n"
     ]
    }
   ],
   "source": [
    "X_va =np.array(X_va)\n",
    "Y_va =np.array(Y_va)\n",
    "print(X_va.shape)\n",
    "print(Y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################################END 驗證集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46709, 26)\n",
      "(46709, 23)\n",
      "0    46111\n",
      "1      598\n",
      "Name: Y_繼續經營疑慮, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.shape)\n",
    "\n",
    "dataframe=dataframe.drop(['TSE 產業別','Lstm','Datamining'], axis=1)#刪除非必要欄位\n",
    "\n",
    "total = dataframe.shape  #資料總變數與總資料筆數\n",
    "total_class = dataframe.Y_繼續經營疑慮.value_counts()\n",
    "print(total)\n",
    "print(total_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>公司碼</th>\n",
       "      <th>年月</th>\n",
       "      <th>X01_Ln資產總額</th>\n",
       "      <th>X02_流動佔總資產比率</th>\n",
       "      <th>X03_流動佔營業淨收入比率</th>\n",
       "      <th>X04_流動比率</th>\n",
       "      <th>X05_速動比率</th>\n",
       "      <th>X06_總資產週轉率</th>\n",
       "      <th>X07_現金流量比率</th>\n",
       "      <th>X08_銷貨毛利率</th>\n",
       "      <th>X09_負債權益比</th>\n",
       "      <th>X10_存貨週轉次數</th>\n",
       "      <th>X11_應收帳款週轉次數</th>\n",
       "      <th>X12_淨值佔資產比率</th>\n",
       "      <th>X13_保留盈餘對總資產比例</th>\n",
       "      <th>X14_ROE</th>\n",
       "      <th>X15_利息保障倍數</th>\n",
       "      <th>X16_稅前淨利率</th>\n",
       "      <th>X17_負債比率</th>\n",
       "      <th>X18_長期負債總比率</th>\n",
       "      <th>X19_ROA</th>\n",
       "      <th>N01_是否四大</th>\n",
       "      <th>Y_繼續經營疑慮</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>4.670900e+04</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "      <td>46709.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4190.379520</td>\n",
       "      <td>201275.857907</td>\n",
       "      <td>14.843288</td>\n",
       "      <td>0.594193</td>\n",
       "      <td>32.624221</td>\n",
       "      <td>250.970424</td>\n",
       "      <td>169.394443</td>\n",
       "      <td>0.253485</td>\n",
       "      <td>7.740377</td>\n",
       "      <td>21.011511</td>\n",
       "      <td>1.075441</td>\n",
       "      <td>16.689591</td>\n",
       "      <td>15.377842</td>\n",
       "      <td>57.183854</td>\n",
       "      <td>0.113850</td>\n",
       "      <td>1.815703</td>\n",
       "      <td>8.883992e+02</td>\n",
       "      <td>-22.034004</td>\n",
       "      <td>42.816146</td>\n",
       "      <td>0.103589</td>\n",
       "      <td>1.207438</td>\n",
       "      <td>0.879809</td>\n",
       "      <td>0.012803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2303.639462</td>\n",
       "      <td>284.957147</td>\n",
       "      <td>1.418051</td>\n",
       "      <td>0.202376</td>\n",
       "      <td>1977.043693</td>\n",
       "      <td>633.960245</td>\n",
       "      <td>409.724750</td>\n",
       "      <td>0.199360</td>\n",
       "      <td>78.572711</td>\n",
       "      <td>273.769772</td>\n",
       "      <td>4.713158</td>\n",
       "      <td>1007.823544</td>\n",
       "      <td>973.436881</td>\n",
       "      <td>16.260036</td>\n",
       "      <td>0.413443</td>\n",
       "      <td>14.142130</td>\n",
       "      <td>2.114723e+04</td>\n",
       "      <td>1315.850408</td>\n",
       "      <td>16.260036</td>\n",
       "      <td>0.107288</td>\n",
       "      <td>3.563543</td>\n",
       "      <td>0.325188</td>\n",
       "      <td>0.112423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1101.000000</td>\n",
       "      <td>200803.000000</td>\n",
       "      <td>8.608313</td>\n",
       "      <td>0.007005</td>\n",
       "      <td>0.098168</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-11155.200000</td>\n",
       "      <td>-19775.160000</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>-10.650000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>-38.510625</td>\n",
       "      <td>-2215.580000</td>\n",
       "      <td>-2.485306e+06</td>\n",
       "      <td>-150190.000000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-431.410000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2396.000000</td>\n",
       "      <td>201012.000000</td>\n",
       "      <td>13.874244</td>\n",
       "      <td>0.457896</td>\n",
       "      <td>1.909710</td>\n",
       "      <td>139.670000</td>\n",
       "      <td>80.160000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>-1.280000</td>\n",
       "      <td>11.180000</td>\n",
       "      <td>0.450792</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>45.910000</td>\n",
       "      <td>0.052018</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>2.240000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.070000</td>\n",
       "      <td>0.024044</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3501.000000</td>\n",
       "      <td>201306.000000</td>\n",
       "      <td>14.678191</td>\n",
       "      <td>0.606752</td>\n",
       "      <td>2.631614</td>\n",
       "      <td>181.610000</td>\n",
       "      <td>118.840000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>5.830000</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>0.745364</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>57.290000</td>\n",
       "      <td>0.117626</td>\n",
       "      <td>1.910000</td>\n",
       "      <td>1.449000e+01</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>42.710000</td>\n",
       "      <td>0.069857</td>\n",
       "      <td>1.140000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6105.000000</td>\n",
       "      <td>201509.000000</td>\n",
       "      <td>15.635702</td>\n",
       "      <td>0.744009</td>\n",
       "      <td>3.734403</td>\n",
       "      <td>255.200000</td>\n",
       "      <td>178.660000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>15.210000</td>\n",
       "      <td>30.090000</td>\n",
       "      <td>1.177941</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>68.930000</td>\n",
       "      <td>0.203391</td>\n",
       "      <td>3.920000</td>\n",
       "      <td>6.143000e+01</td>\n",
       "      <td>12.340000</td>\n",
       "      <td>54.090000</td>\n",
       "      <td>0.153558</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9962.000000</td>\n",
       "      <td>201712.000000</td>\n",
       "      <td>21.733461</td>\n",
       "      <td>0.999645</td>\n",
       "      <td>339244.100000</td>\n",
       "      <td>55079.490000</td>\n",
       "      <td>26744.130000</td>\n",
       "      <td>4.460000</td>\n",
       "      <td>3261.760000</td>\n",
       "      <td>52580.000000</td>\n",
       "      <td>593.802050</td>\n",
       "      <td>173462.000000</td>\n",
       "      <td>141607.000000</td>\n",
       "      <td>99.440000</td>\n",
       "      <td>0.750217</td>\n",
       "      <td>817.740000</td>\n",
       "      <td>2.629103e+06</td>\n",
       "      <td>44890.830000</td>\n",
       "      <td>99.830000</td>\n",
       "      <td>0.957482</td>\n",
       "      <td>92.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                公司碼             年月    X01_Ln資產總額  X02_流動佔總資產比率  \\\n",
       "count  46709.000000   46709.000000  46709.000000  46709.000000   \n",
       "mean    4190.379520  201275.857907     14.843288      0.594193   \n",
       "std     2303.639462     284.957147      1.418051      0.202376   \n",
       "min     1101.000000  200803.000000      8.608313      0.007005   \n",
       "25%     2396.000000  201012.000000     13.874244      0.457896   \n",
       "50%     3501.000000  201306.000000     14.678191      0.606752   \n",
       "75%     6105.000000  201509.000000     15.635702      0.744009   \n",
       "max     9962.000000  201712.000000     21.733461      0.999645   \n",
       "\n",
       "       X03_流動佔營業淨收入比率      X04_流動比率      X05_速動比率    X06_總資產週轉率    X07_現金流量比率  \\\n",
       "count    46709.000000  46709.000000  46709.000000  46709.000000  46709.000000   \n",
       "mean        32.624221    250.970424    169.394443      0.253485      7.740377   \n",
       "std       1977.043693    633.960245    409.724750      0.199360     78.572711   \n",
       "min          0.098168      0.810000      0.020000      0.000000 -11155.200000   \n",
       "25%          1.909710    139.670000     80.160000      0.140000     -1.280000   \n",
       "50%          2.631614    181.610000    118.840000      0.210000      5.830000   \n",
       "75%          3.734403    255.200000    178.660000      0.310000     15.210000   \n",
       "max     339244.100000  55079.490000  26744.130000      4.460000   3261.760000   \n",
       "\n",
       "          X08_銷貨毛利率     X09_負債權益比     X10_存貨週轉次數   X11_應收帳款週轉次數   X12_淨值佔資產比率  \\\n",
       "count  46709.000000  46709.000000   46709.000000   46709.000000  46709.000000   \n",
       "mean      21.011511      1.075441      16.689591      15.377842     57.183854   \n",
       "std      273.769772      4.713158    1007.823544     973.436881     16.260036   \n",
       "min   -19775.160000      0.005666     -10.650000       0.000000      0.170000   \n",
       "25%       11.180000      0.450792       0.760000       0.960000     45.910000   \n",
       "50%       19.250000      0.745364       1.310000       1.390000     57.290000   \n",
       "75%       30.090000      1.177941       2.170000       2.200000     68.930000   \n",
       "max    52580.000000    593.802050  173462.000000  141607.000000     99.440000   \n",
       "\n",
       "       X13_保留盈餘對總資產比例       X14_ROE    X15_利息保障倍數      X16_稅前淨利率  \\\n",
       "count    46709.000000  46709.000000  4.670900e+04   46709.000000   \n",
       "mean         0.113850      1.815703  8.883992e+02     -22.034004   \n",
       "std          0.413443     14.142130  2.114723e+04    1315.850408   \n",
       "min        -38.510625  -2215.580000 -2.485306e+06 -150190.000000   \n",
       "25%          0.052018      0.230000  2.240000e+00       1.000000   \n",
       "50%          0.117626      1.910000  1.449000e+01       5.690000   \n",
       "75%          0.203391      3.920000  6.143000e+01      12.340000   \n",
       "max          0.750217    817.740000  2.629103e+06   44890.830000   \n",
       "\n",
       "           X17_負債比率   X18_長期負債總比率       X19_ROA      N01_是否四大      Y_繼續經營疑慮  \n",
       "count  46709.000000  46709.000000  46709.000000  46709.000000  46709.000000  \n",
       "mean      42.816146      0.103589      1.207438      0.879809      0.012803  \n",
       "std       16.260036      0.107288      3.563543      0.325188      0.112423  \n",
       "min        0.560000      0.000000   -431.410000      0.000000      0.000000  \n",
       "25%       31.070000      0.024044      0.230000      1.000000      0.000000  \n",
       "50%       42.710000      0.069857      1.140000      1.000000      0.000000  \n",
       "75%       54.090000      0.153558      2.270000      1.000000      0.000000  \n",
       "max       99.830000      0.957482     92.750000      1.000000      1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>公司碼</th>\n",
       "      <th>年月</th>\n",
       "      <th>X01_Ln資產總額</th>\n",
       "      <th>X02_流動佔總資產比率</th>\n",
       "      <th>X03_流動佔營業淨收入比率</th>\n",
       "      <th>X04_流動比率</th>\n",
       "      <th>X05_速動比率</th>\n",
       "      <th>X06_總資產週轉率</th>\n",
       "      <th>X07_現金流量比率</th>\n",
       "      <th>X08_銷貨毛利率</th>\n",
       "      <th>X09_負債權益比</th>\n",
       "      <th>X10_存貨週轉次數</th>\n",
       "      <th>X11_應收帳款週轉次數</th>\n",
       "      <th>X12_淨值佔資產比率</th>\n",
       "      <th>X13_保留盈餘對總資產比例</th>\n",
       "      <th>X14_ROE</th>\n",
       "      <th>X15_利息保障倍數</th>\n",
       "      <th>X16_稅前淨利率</th>\n",
       "      <th>X17_負債比率</th>\n",
       "      <th>X18_長期負債總比率</th>\n",
       "      <th>X19_ROA</th>\n",
       "      <th>N01_是否四大</th>\n",
       "      <th>Y_繼續經營疑慮</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1101</td>\n",
       "      <td>200803.0</td>\n",
       "      <td>17.860166</td>\n",
       "      <td>0.300649</td>\n",
       "      <td>3.188306</td>\n",
       "      <td>191.69</td>\n",
       "      <td>149.68</td>\n",
       "      <td>0.09</td>\n",
       "      <td>8.36</td>\n",
       "      <td>13.91</td>\n",
       "      <td>0.881842</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.08</td>\n",
       "      <td>53.14</td>\n",
       "      <td>0.086722</td>\n",
       "      <td>1.67</td>\n",
       "      <td>4.31</td>\n",
       "      <td>11.31</td>\n",
       "      <td>46.86</td>\n",
       "      <td>0.311761</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1101</td>\n",
       "      <td>200806.0</td>\n",
       "      <td>17.811181</td>\n",
       "      <td>0.284241</td>\n",
       "      <td>2.644814</td>\n",
       "      <td>127.04</td>\n",
       "      <td>102.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>4.89</td>\n",
       "      <td>10.66</td>\n",
       "      <td>1.057768</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.40</td>\n",
       "      <td>48.60</td>\n",
       "      <td>0.061806</td>\n",
       "      <td>2.70</td>\n",
       "      <td>5.09</td>\n",
       "      <td>10.78</td>\n",
       "      <td>51.40</td>\n",
       "      <td>0.290287</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>200809.0</td>\n",
       "      <td>17.637446</td>\n",
       "      <td>0.247127</td>\n",
       "      <td>2.032282</td>\n",
       "      <td>115.39</td>\n",
       "      <td>81.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>16.79</td>\n",
       "      <td>9.57</td>\n",
       "      <td>1.085863</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.39</td>\n",
       "      <td>47.94</td>\n",
       "      <td>0.069877</td>\n",
       "      <td>1.48</td>\n",
       "      <td>3.34</td>\n",
       "      <td>7.04</td>\n",
       "      <td>52.06</td>\n",
       "      <td>0.306412</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1101</td>\n",
       "      <td>200812.0</td>\n",
       "      <td>17.594278</td>\n",
       "      <td>0.234187</td>\n",
       "      <td>2.496075</td>\n",
       "      <td>111.71</td>\n",
       "      <td>84.44</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.01</td>\n",
       "      <td>5.35</td>\n",
       "      <td>1.070584</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.98</td>\n",
       "      <td>48.30</td>\n",
       "      <td>0.075107</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.08</td>\n",
       "      <td>4.31</td>\n",
       "      <td>51.70</td>\n",
       "      <td>0.307405</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1101</td>\n",
       "      <td>200903.0</td>\n",
       "      <td>17.697198</td>\n",
       "      <td>0.251429</td>\n",
       "      <td>2.684157</td>\n",
       "      <td>125.29</td>\n",
       "      <td>96.44</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7.61</td>\n",
       "      <td>9.73</td>\n",
       "      <td>1.075014</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.03</td>\n",
       "      <td>48.19</td>\n",
       "      <td>0.073696</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.76</td>\n",
       "      <td>51.81</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46704</th>\n",
       "      <td>9962</td>\n",
       "      <td>201612.0</td>\n",
       "      <td>13.596558</td>\n",
       "      <td>0.713315</td>\n",
       "      <td>1.300904</td>\n",
       "      <td>628.99</td>\n",
       "      <td>132.60</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-27.24</td>\n",
       "      <td>8.55</td>\n",
       "      <td>0.151827</td>\n",
       "      <td>0.97</td>\n",
       "      <td>6.03</td>\n",
       "      <td>86.82</td>\n",
       "      <td>0.061342</td>\n",
       "      <td>3.17</td>\n",
       "      <td>407.33</td>\n",
       "      <td>5.92</td>\n",
       "      <td>13.18</td>\n",
       "      <td>0.018407</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46705</th>\n",
       "      <td>9962</td>\n",
       "      <td>201703.0</td>\n",
       "      <td>13.634926</td>\n",
       "      <td>0.734175</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>870.96</td>\n",
       "      <td>203.10</td>\n",
       "      <td>0.58</td>\n",
       "      <td>68.72</td>\n",
       "      <td>10.27</td>\n",
       "      <td>0.114064</td>\n",
       "      <td>0.95</td>\n",
       "      <td>7.99</td>\n",
       "      <td>89.76</td>\n",
       "      <td>0.096990</td>\n",
       "      <td>4.12</td>\n",
       "      <td>271.80</td>\n",
       "      <td>7.50</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46706</th>\n",
       "      <td>9962</td>\n",
       "      <td>201706.0</td>\n",
       "      <td>13.678957</td>\n",
       "      <td>0.748912</td>\n",
       "      <td>1.573833</td>\n",
       "      <td>466.70</td>\n",
       "      <td>105.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>35.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.216720</td>\n",
       "      <td>0.85</td>\n",
       "      <td>9.23</td>\n",
       "      <td>82.19</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-150.78</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>17.81</td>\n",
       "      <td>0.017651</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46707</th>\n",
       "      <td>9962</td>\n",
       "      <td>201709.0</td>\n",
       "      <td>13.564384</td>\n",
       "      <td>0.719438</td>\n",
       "      <td>1.428747</td>\n",
       "      <td>805.51</td>\n",
       "      <td>219.06</td>\n",
       "      <td>0.48</td>\n",
       "      <td>76.82</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.121574</td>\n",
       "      <td>0.85</td>\n",
       "      <td>8.85</td>\n",
       "      <td>89.16</td>\n",
       "      <td>0.050125</td>\n",
       "      <td>0.77</td>\n",
       "      <td>76.93</td>\n",
       "      <td>1.60</td>\n",
       "      <td>10.84</td>\n",
       "      <td>0.019082</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46708</th>\n",
       "      <td>9962</td>\n",
       "      <td>201712.0</td>\n",
       "      <td>13.642764</td>\n",
       "      <td>0.738271</td>\n",
       "      <td>1.595750</td>\n",
       "      <td>628.01</td>\n",
       "      <td>143.90</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-11.10</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.158041</td>\n",
       "      <td>0.81</td>\n",
       "      <td>10.05</td>\n",
       "      <td>86.35</td>\n",
       "      <td>0.064836</td>\n",
       "      <td>2.11</td>\n",
       "      <td>442.32</td>\n",
       "      <td>4.69</td>\n",
       "      <td>13.65</td>\n",
       "      <td>0.018916</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46709 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        公司碼        年月  X01_Ln資產總額  X02_流動佔總資產比率  X03_流動佔營業淨收入比率  X04_流動比率  \\\n",
       "0      1101  200803.0   17.860166      0.300649        3.188306    191.69   \n",
       "1      1101  200806.0   17.811181      0.284241        2.644814    127.04   \n",
       "2      1101  200809.0   17.637446      0.247127        2.032282    115.39   \n",
       "3      1101  200812.0   17.594278      0.234187        2.496075    111.71   \n",
       "4      1101  200903.0   17.697198      0.251429        2.684157    125.29   \n",
       "...     ...       ...         ...           ...             ...       ...   \n",
       "46704  9962  201612.0   13.596558      0.713315        1.300904    628.99   \n",
       "46705  9962  201703.0   13.634926      0.734175        1.263158    870.96   \n",
       "46706  9962  201706.0   13.678957      0.748912        1.573833    466.70   \n",
       "46707  9962  201709.0   13.564384      0.719438        1.428747    805.51   \n",
       "46708  9962  201712.0   13.642764      0.738271        1.595750    628.01   \n",
       "\n",
       "       X05_速動比率  X06_總資產週轉率  X07_現金流量比率  X08_銷貨毛利率  X09_負債權益比  X10_存貨週轉次數  \\\n",
       "0        149.68        0.09        8.36      13.91   0.881842        2.16   \n",
       "1        102.02        0.11        4.89      10.66   1.057768        2.60   \n",
       "2         81.03        0.12       16.79       9.57   1.085863        2.64   \n",
       "3         84.44        0.09        3.01       5.35   1.070584        2.15   \n",
       "4         96.44        0.10        7.61       9.73   1.075014        2.62   \n",
       "...         ...         ...         ...        ...        ...         ...   \n",
       "46704    132.60        0.56      -27.24       8.55   0.151827        0.97   \n",
       "46705    203.10        0.58       68.72      10.27   0.114064        0.95   \n",
       "46706    105.00        0.48       35.69       0.69   0.216720        0.85   \n",
       "46707    219.06        0.48       76.82       4.04   0.121574        0.85   \n",
       "46708    143.90        0.47      -11.10       8.17   0.158041        0.81   \n",
       "\n",
       "       X11_應收帳款週轉次數  X12_淨值佔資產比率  X13_保留盈餘對總資產比例  X14_ROE  X15_利息保障倍數  \\\n",
       "0              2.08        53.14        0.086722     1.67        4.31   \n",
       "1              2.40        48.60        0.061806     2.70        5.09   \n",
       "2              2.39        47.94        0.069877     1.48        3.34   \n",
       "3              1.98        48.30        0.075107     0.90        2.08   \n",
       "4              2.03        48.19        0.073696     0.53        1.81   \n",
       "...             ...          ...             ...      ...         ...   \n",
       "46704          6.03        86.82        0.061342     3.17      407.33   \n",
       "46705          7.99        89.76        0.096990     4.12      271.80   \n",
       "46706          9.23        82.19        0.040200    -0.94     -150.78   \n",
       "46707          8.85        89.16        0.050125     0.77       76.93   \n",
       "46708         10.05        86.35        0.064836     2.11      442.32   \n",
       "\n",
       "       X16_稅前淨利率  X17_負債比率  X18_長期負債總比率  X19_ROA  N01_是否四大  Y_繼續經營疑慮  \n",
       "0          11.31     46.86     0.311761     1.13         1         0  \n",
       "1          10.78     51.40     0.290287     1.58         1         0  \n",
       "2           7.04     52.06     0.306412     0.98         1         0  \n",
       "3           4.31     51.70     0.307405     0.72         1         0  \n",
       "4           2.76     51.81     0.317400     0.50         1         0  \n",
       "...          ...       ...          ...      ...       ...       ...  \n",
       "46704       5.92     13.18     0.018407     2.79         0         0  \n",
       "46705       7.50     10.24     0.018091     3.65         0         0  \n",
       "46706      -2.03     17.81     0.017651    -0.80         0         0  \n",
       "46707       1.60     10.84     0.019082     0.67         0         0  \n",
       "46708       4.69     13.65     0.018916     1.85         0         0  \n",
       "\n",
       "[46709 rows x 23 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46709\n",
      "<class 'list'>\n",
      "1724\n"
     ]
    }
   ],
   "source": [
    "#建立公司名單集合\n",
    "Name = []\n",
    "i = 0\n",
    "for i in range(len(dataframe)):\n",
    "    Name.append(dataframe.iloc[i,0])\n",
    "    i += 1\n",
    "print(len(Name))\n",
    "print(type(Name))\n",
    "Nameset = set(Name)\n",
    "print(len(Nameset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# window 建立四期觀測資料集\n",
    "X = []\n",
    "Y = []\n",
    "j = 0\n",
    "for j in range(len(dataframe)-5):\n",
    "    if dataframe.iloc[j,0] == dataframe.iloc[j+5,0]:  \n",
    "        X.append(dataframe.iloc[j:j+4,2:22].values)  #scaled_dataset_\n",
    "        Y.append(dataframe.iloc[j+5,22])\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    37804\n",
      "1      320\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Y_c =pd.DataFrame(Y)\n",
    "total_class_c = Y_c.value_counts()\n",
    "print(total_class_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46704 38124 38124\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(j, len(X), len(Y))\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38124, 4, 20)\n",
      "(38124,)\n"
     ]
    }
   ],
   "source": [
    "X =np.array(X)\n",
    "Y =np.array(Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #將資料打散，而非照日期排序\n",
    "\n",
    "def shuffle(X,Y):\n",
    "  np.random.seed(10)\n",
    "  randomList = np.arange(X.shape[0])\n",
    "  np.random.shuffle(randomList)\n",
    "  return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #將Training Data取一部份當作Test Data\n",
    "def splitData(X,Y,rate):\n",
    "  x_train = X[int(X.shape[0]*rate):]\n",
    "  y_train = Y[int(Y.shape[0]*rate):]\n",
    "  x_test = X[:int(X.shape[0]*rate)]\n",
    "  y_test = Y[:int(Y.shape[0]*rate)]\n",
    "  return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料型別 <class 'numpy.ndarray'>\n",
      "陣列元素資料型別： float64\n",
      "陣列元素總數： 2440000\n",
      "陣列形狀： (30500, 4, 20)\n",
      "陣列的維度數目 3\n",
      "資料型別 <class 'numpy.ndarray'>\n",
      "陣列元素資料型別： int64\n",
      "陣列元素總數： 30500\n",
      "陣列形狀： (30500,)\n",
      "陣列的維度數目 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# shuffle the data, and random seed is 10\n",
    "X_train, Y_train = shuffle(X, Y)\n",
    "\n",
    "# split training data and validation data\n",
    "x_train, y_train, x_test, y_test = splitData(X_train, Y_train, 0.2)\n",
    "\n",
    "print(\"資料型別\",type(x_train))      #列印陣列資料型別\n",
    "print(\"陣列元素資料型別：\",x_train.dtype) #列印陣列元素資料型別\n",
    "print(\"陣列元素總數：\",x_train.size)   #列印陣列尺寸，即陣列元素總數\n",
    "print(\"陣列形狀：\",x_train.shape)     #列印陣列形狀\n",
    "print(\"陣列的維度數目\",x_train.ndim)   #列印陣列的維度數目\n",
    "print(\"資料型別\",type(y_train))      #列印陣列資料型別\n",
    "print(\"陣列元素資料型別：\",y_train.dtype) #列印陣列元素資料型別\n",
    "print(\"陣列元素總數：\",y_train.size)   #列印陣列尺寸，即陣列元素總數\n",
    "print(\"陣列形狀：\",y_train.shape)     #列印陣列形狀\n",
    "print(\"陣列的維度數目\",y_train.ndim)   #列印陣列的維度數目\n",
    "# print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                21760     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,513\n",
      "Trainable params: 24,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 21:15:17.984003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 21:15:18.002089: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-20 21:15:18.002104: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-20 21:15:18.002427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.constraints import MaxNorm as maxnorm\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import TimeDistributed\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='tanh', input_shape=(4, 20), return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error'\n",
    "              ,optimizer='adam'\n",
    "              )\n",
    "# ,metrics=['accuracy']\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0045\n",
      "Epoch 1: val_loss improved from inf to 0.18511, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 1s 25ms/step - loss: 0.0045 - val_loss: 0.1851\n",
      "Epoch 2/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 0.0045\n",
      "Epoch 2: val_loss did not improve from 0.18511\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0044 - val_loss: 0.1880\n",
      "Epoch 3/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 0.0043\n",
      "Epoch 3: val_loss did not improve from 0.18511\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0044 - val_loss: 0.1855\n",
      "Epoch 4/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 0.0041\n",
      "Epoch 4: val_loss improved from 0.18511 to 0.18127, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0042 - val_loss: 0.1813\n",
      "Epoch 5/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0041\n",
      "Epoch 5: val_loss improved from 0.18127 to 0.17426, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0041 - val_loss: 0.1743\n",
      "Epoch 6/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0038\n",
      "Epoch 6: val_loss improved from 0.17426 to 0.15829, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0038 - val_loss: 0.1583\n",
      "Epoch 7/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0035\n",
      "Epoch 7: val_loss improved from 0.15829 to 0.14230, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0035 - val_loss: 0.1423\n",
      "Epoch 8/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 0.0031\n",
      "Epoch 8: val_loss improved from 0.14230 to 0.12320, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0031 - val_loss: 0.1232\n",
      "Epoch 9/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 0.0028\n",
      "Epoch 9: val_loss improved from 0.12320 to 0.10765, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 0.1077\n",
      "Epoch 10/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0025\n",
      "Epoch 10: val_loss improved from 0.10765 to 0.09233, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 0.0923\n",
      "Epoch 11/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0023\n",
      "Epoch 11: val_loss improved from 0.09233 to 0.07840, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0784\n",
      "Epoch 12/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0021\n",
      "Epoch 12: val_loss improved from 0.07840 to 0.06725, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0020 - val_loss: 0.0672\n",
      "Epoch 13/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 13: val_loss improved from 0.06725 to 0.06009, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0018 - val_loss: 0.0601\n",
      "Epoch 14/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 14: val_loss improved from 0.06009 to 0.05383, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0017 - val_loss: 0.0538\n",
      "Epoch 15/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 0.0016\n",
      "Epoch 15: val_loss improved from 0.05383 to 0.04971, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0016 - val_loss: 0.0497\n",
      "Epoch 16/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 0.0015   \n",
      "Epoch 16: val_loss improved from 0.04971 to 0.04307, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0015 - val_loss: 0.0431\n",
      "Epoch 17/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0014\n",
      "Epoch 17: val_loss improved from 0.04307 to 0.03833, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 0.0383\n",
      "Epoch 18/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 0.0013\n",
      "Epoch 18: val_loss improved from 0.03833 to 0.03725, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0013 - val_loss: 0.0372\n",
      "Epoch 19/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0012\n",
      "Epoch 19: val_loss improved from 0.03725 to 0.03103, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 0.0310\n",
      "Epoch 20/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 0.0011\n",
      "Epoch 20: val_loss did not improve from 0.03103\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0012 - val_loss: 0.0346\n",
      "Epoch 21/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0012\n",
      "Epoch 21: val_loss did not improve from 0.03103\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0012 - val_loss: 0.0311\n",
      "Epoch 22/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 0.0010    \n",
      "Epoch 22: val_loss improved from 0.03103 to 0.02645, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0011 - val_loss: 0.0265\n",
      "Epoch 23/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 0.0011\n",
      "Epoch 23: val_loss did not improve from 0.02645\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 0.0296\n",
      "Epoch 24/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 9.6753e-04\n",
      "Epoch 24: val_loss improved from 0.02645 to 0.02595, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0010 - val_loss: 0.0259\n",
      "Epoch 25/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 0.0010\n",
      "Epoch 25: val_loss improved from 0.02595 to 0.02551, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.9907e-04 - val_loss: 0.0255\n",
      "Epoch 26/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.4110e-04\n",
      "Epoch 26: val_loss did not improve from 0.02551\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 9.8550e-04 - val_loss: 0.0282\n",
      "Epoch 27/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.4496e-04\n",
      "Epoch 27: val_loss did not improve from 0.02551\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 9.2487e-04 - val_loss: 0.0255\n",
      "Epoch 28/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.7571e-04\n",
      "Epoch 28: val_loss improved from 0.02551 to 0.02275, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.9905e-04 - val_loss: 0.0228\n",
      "Epoch 29/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 8.6678e-04\n",
      "Epoch 29: val_loss did not improve from 0.02275\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 8.7453e-04 - val_loss: 0.0237\n",
      "Epoch 30/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.6643e-04\n",
      "Epoch 30: val_loss did not improve from 0.02275\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 8.5720e-04 - val_loss: 0.0269\n",
      "Epoch 31/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 8.4715e-04\n",
      "Epoch 31: val_loss improved from 0.02275 to 0.02123, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.5360e-04 - val_loss: 0.0212\n",
      "Epoch 32/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 7.8269e-04\n",
      "Epoch 32: val_loss improved from 0.02123 to 0.02077, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.2534e-04 - val_loss: 0.0208\n",
      "Epoch 33/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.0542e-04\n",
      "Epoch 33: val_loss improved from 0.02077 to 0.02028, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 7.9937e-04 - val_loss: 0.0203\n",
      "Epoch 34/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 8.0228e-04\n",
      "Epoch 34: val_loss improved from 0.02028 to 0.01875, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.0281e-04 - val_loss: 0.0188\n",
      "Epoch 35/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 7.7905e-04\n",
      "Epoch 35: val_loss did not improve from 0.01875\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.7365e-04 - val_loss: 0.0190\n",
      "Epoch 36/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.7958e-04\n",
      "Epoch 36: val_loss did not improve from 0.01875\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.7539e-04 - val_loss: 0.0191\n",
      "Epoch 37/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.7003e-04\n",
      "Epoch 37: val_loss did not improve from 0.01875\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 7.6735e-04 - val_loss: 0.0215\n",
      "Epoch 38/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 7.4768e-04\n",
      "Epoch 38: val_loss improved from 0.01875 to 0.01791, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.6421e-04 - val_loss: 0.0179\n",
      "Epoch 39/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 6.9541e-04\n",
      "Epoch 39: val_loss did not improve from 0.01791\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 8.1366e-04 - val_loss: 0.0202\n",
      "Epoch 40/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.8043e-04\n",
      "Epoch 40: val_loss improved from 0.01791 to 0.01716, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 7.4278e-04 - val_loss: 0.0172\n",
      "Epoch 41/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 8.3889e-04\n",
      "Epoch 41: val_loss did not improve from 0.01716\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.9936e-04 - val_loss: 0.0179\n",
      "Epoch 42/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 8.1866e-04\n",
      "Epoch 42: val_loss did not improve from 0.01716\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.5550e-04 - val_loss: 0.0192\n",
      "Epoch 43/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 6.9977e-04\n",
      "Epoch 43: val_loss improved from 0.01716 to 0.01595, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 7.1376e-04 - val_loss: 0.0160\n",
      "Epoch 44/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 7.6126e-04\n",
      "Epoch 44: val_loss did not improve from 0.01595\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.5461e-04 - val_loss: 0.0208\n",
      "Epoch 45/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 7.6947e-04\n",
      "Epoch 45: val_loss did not improve from 0.01595\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 7.0183e-04 - val_loss: 0.0179\n",
      "Epoch 46/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 6.7397e-04\n",
      "Epoch 46: val_loss did not improve from 0.01595\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.4711e-04 - val_loss: 0.0171\n",
      "Epoch 47/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.3581e-04\n",
      "Epoch 47: val_loss did not improve from 0.01595\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 7.0063e-04 - val_loss: 0.0185\n",
      "Epoch 48/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 6.7783e-04\n",
      "Epoch 48: val_loss improved from 0.01595 to 0.01527, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.7719e-04 - val_loss: 0.0153\n",
      "Epoch 49/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 7.1475e-04\n",
      "Epoch 49: val_loss did not improve from 0.01527\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.6524e-04 - val_loss: 0.0201\n",
      "Epoch 50/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 7.3590e-04\n",
      "Epoch 50: val_loss improved from 0.01527 to 0.01401, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.7056e-04 - val_loss: 0.0140\n",
      "Epoch 51/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.9327e-04\n",
      "Epoch 51: val_loss did not improve from 0.01401\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.9327e-04 - val_loss: 0.0175\n",
      "Epoch 52/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 7.4712e-04\n",
      "Epoch 52: val_loss did not improve from 0.01401\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.7211e-04 - val_loss: 0.0148\n",
      "Epoch 53/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 6.2507e-04\n",
      "Epoch 53: val_loss improved from 0.01401 to 0.01150, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 6.2320e-04 - val_loss: 0.0115\n",
      "Epoch 54/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 6.8410e-04\n",
      "Epoch 54: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.8088e-04 - val_loss: 0.0139\n",
      "Epoch 55/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.3863e-04\n",
      "Epoch 55: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.3863e-04 - val_loss: 0.0122\n",
      "Epoch 56/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.2087e-04\n",
      "Epoch 56: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.2087e-04 - val_loss: 0.0164\n",
      "Epoch 57/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 7.2424e-04\n",
      "Epoch 57: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.6907e-04 - val_loss: 0.0128\n",
      "Epoch 58/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 6.8074e-04\n",
      "Epoch 58: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 6.5468e-04 - val_loss: 0.0143\n",
      "Epoch 59/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 5.7075e-04\n",
      "Epoch 59: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.5060e-04 - val_loss: 0.0154\n",
      "Epoch 60/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 6.3122e-04\n",
      "Epoch 60: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 6.4337e-04 - val_loss: 0.0139\n",
      "Epoch 61/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 6.5766e-04\n",
      "Epoch 61: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 6.5986e-04 - val_loss: 0.0179\n",
      "Epoch 62/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 5.6928e-04\n",
      "Epoch 62: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 6.1950e-04 - val_loss: 0.0123\n",
      "Epoch 63/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 6.1197e-04\n",
      "Epoch 63: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.0568e-04 - val_loss: 0.0147\n",
      "Epoch 64/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 6.2766e-04\n",
      "Epoch 64: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.9641e-04 - val_loss: 0.0174\n",
      "Epoch 65/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 6.3000e-04\n",
      "Epoch 65: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.0535e-04 - val_loss: 0.0152\n",
      "Epoch 66/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 5.4890e-04\n",
      "Epoch 66: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 5.9882e-04 - val_loss: 0.0147\n",
      "Epoch 67/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 5.7849e-04\n",
      "Epoch 67: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 6.0094e-04 - val_loss: 0.0143\n",
      "Epoch 68/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 5.9710e-04\n",
      "Epoch 68: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.2552e-04 - val_loss: 0.0156\n",
      "Epoch 69/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 5.5874e-04\n",
      "Epoch 69: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.7113e-04 - val_loss: 0.0134\n",
      "Epoch 70/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 5.3332e-04\n",
      "Epoch 70: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.5597e-04 - val_loss: 0.0129\n",
      "Epoch 71/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 6.0307e-04\n",
      "Epoch 71: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.6691e-04 - val_loss: 0.0118\n",
      "Epoch 72/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 5.3699e-04\n",
      "Epoch 72: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.4344e-04 - val_loss: 0.0140\n",
      "Epoch 73/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.9264e-04\n",
      "Epoch 73: val_loss did not improve from 0.01150\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.5701e-04 - val_loss: 0.0119\n",
      "Epoch 74/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 5.8881e-04\n",
      "Epoch 74: val_loss improved from 0.01150 to 0.01149, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 5.8240e-04 - val_loss: 0.0115\n",
      "Epoch 75/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 5.6587e-04\n",
      "Epoch 75: val_loss did not improve from 0.01149\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 5.6326e-04 - val_loss: 0.0150\n",
      "Epoch 76/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 7.2324e-04\n",
      "Epoch 76: val_loss did not improve from 0.01149\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.3098e-04 - val_loss: 0.0157\n",
      "Epoch 77/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 6.0605e-04\n",
      "Epoch 77: val_loss did not improve from 0.01149\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 6.0067e-04 - val_loss: 0.0152\n",
      "Epoch 78/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 5.3835e-04\n",
      "Epoch 78: val_loss did not improve from 0.01149\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.4006e-04 - val_loss: 0.0117\n",
      "Epoch 79/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 5.5604e-04\n",
      "Epoch 79: val_loss did not improve from 0.01149\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.8873e-04 - val_loss: 0.0119\n",
      "Epoch 80/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.7312e-04\n",
      "Epoch 80: val_loss did not improve from 0.01149\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.6044e-04 - val_loss: 0.0120\n",
      "Epoch 81/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 5.5992e-04\n",
      "Epoch 81: val_loss improved from 0.01149 to 0.01136, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 5.7564e-04 - val_loss: 0.0114\n",
      "Epoch 82/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 5.3406e-04\n",
      "Epoch 82: val_loss did not improve from 0.01136\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 5.2772e-04 - val_loss: 0.0119\n",
      "Epoch 83/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 5.1803e-04\n",
      "Epoch 83: val_loss did not improve from 0.01136\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.7952e-04 - val_loss: 0.0123\n",
      "Epoch 84/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 5.3470e-04\n",
      "Epoch 84: val_loss improved from 0.01136 to 0.01115, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.2893e-04 - val_loss: 0.0111\n",
      "Epoch 85/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 5.3783e-04\n",
      "Epoch 85: val_loss did not improve from 0.01115\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 5.1281e-04 - val_loss: 0.0114\n",
      "Epoch 86/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 5.2617e-04\n",
      "Epoch 86: val_loss did not improve from 0.01115\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 5.2392e-04 - val_loss: 0.0136\n",
      "Epoch 87/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 5.7206e-04\n",
      "Epoch 87: val_loss improved from 0.01115 to 0.01045, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.2140e-04 - val_loss: 0.0104\n",
      "Epoch 88/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 5.3779e-04\n",
      "Epoch 88: val_loss did not improve from 0.01045\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.2878e-04 - val_loss: 0.0118\n",
      "Epoch 89/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.7078e-04\n",
      "Epoch 89: val_loss improved from 0.01045 to 0.01031, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.7805e-04 - val_loss: 0.0103\n",
      "Epoch 90/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 5.5282e-04\n",
      "Epoch 90: val_loss did not improve from 0.01031\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 5.4753e-04 - val_loss: 0.0116\n",
      "Epoch 91/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 5.1575e-04\n",
      "Epoch 91: val_loss did not improve from 0.01031\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.9433e-04 - val_loss: 0.0117\n",
      "Epoch 92/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.5013e-04\n",
      "Epoch 92: val_loss did not improve from 0.01031\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.9781e-04 - val_loss: 0.0108\n",
      "Epoch 93/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 5.2477e-04\n",
      "Epoch 93: val_loss improved from 0.01031 to 0.00996, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.7362e-04 - val_loss: 0.0100\n",
      "Epoch 94/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.9522e-04\n",
      "Epoch 94: val_loss improved from 0.00996 to 0.00991, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 4.9522e-04 - val_loss: 0.0099\n",
      "Epoch 95/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.4991e-04\n",
      "Epoch 95: val_loss improved from 0.00991 to 0.00933, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.5480e-04 - val_loss: 0.0093\n",
      "Epoch 96/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 4.3734e-04\n",
      "Epoch 96: val_loss did not improve from 0.00933\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.8781e-04 - val_loss: 0.0144\n",
      "Epoch 97/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.5327e-04\n",
      "Epoch 97: val_loss improved from 0.00933 to 0.00894, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.7918e-04 - val_loss: 0.0089\n",
      "Epoch 98/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 5.4200e-04\n",
      "Epoch 98: val_loss did not improve from 0.00894\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 5.0826e-04 - val_loss: 0.0104\n",
      "Epoch 99/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 5.2652e-04\n",
      "Epoch 99: val_loss did not improve from 0.00894\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.6049e-04 - val_loss: 0.0103\n",
      "Epoch 100/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 5.3364e-04\n",
      "Epoch 100: val_loss did not improve from 0.00894\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 4.9925e-04 - val_loss: 0.0104\n",
      "Epoch 101/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.4649e-04\n",
      "Epoch 101: val_loss did not improve from 0.00894\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 4.4207e-04 - val_loss: 0.0092\n",
      "Epoch 102/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 5.0245e-04\n",
      "Epoch 102: val_loss did not improve from 0.00894\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.7068e-04 - val_loss: 0.0096\n",
      "Epoch 103/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.6709e-04\n",
      "Epoch 103: val_loss did not improve from 0.00894\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 4.7237e-04 - val_loss: 0.0117\n",
      "Epoch 104/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.0635e-04\n",
      "Epoch 104: val_loss did not improve from 0.00894\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 5.0082e-04 - val_loss: 0.0094\n",
      "Epoch 105/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 5.4387e-04\n",
      "Epoch 105: val_loss improved from 0.00894 to 0.00875, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.1653e-04 - val_loss: 0.0087\n",
      "Epoch 106/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.6462e-04\n",
      "Epoch 106: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.8575e-04 - val_loss: 0.0102\n",
      "Epoch 107/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.9269e-04\n",
      "Epoch 107: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.7516e-04 - val_loss: 0.0102\n",
      "Epoch 108/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 5.3041e-04\n",
      "Epoch 108: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.6489e-04 - val_loss: 0.0178\n",
      "Epoch 109/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 6.8557e-04\n",
      "Epoch 109: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.9060e-04 - val_loss: 0.0153\n",
      "Epoch 110/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 5.9301e-04\n",
      "Epoch 110: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.7516e-04 - val_loss: 0.0154\n",
      "Epoch 111/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 5.3538e-04\n",
      "Epoch 111: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 5.3067e-04 - val_loss: 0.0136\n",
      "Epoch 112/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 5.2844e-04\n",
      "Epoch 112: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.9539e-04 - val_loss: 0.0092\n",
      "Epoch 113/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.7415e-04\n",
      "Epoch 113: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.7534e-04 - val_loss: 0.0097\n",
      "Epoch 114/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.9380e-04\n",
      "Epoch 114: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 5.0579e-04 - val_loss: 0.0089\n",
      "Epoch 115/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 5.1794e-04\n",
      "Epoch 115: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.5147e-04 - val_loss: 0.0143\n",
      "Epoch 116/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 6.0432e-04\n",
      "Epoch 116: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 5.3742e-04 - val_loss: 0.0095\n",
      "Epoch 117/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.7782e-04\n",
      "Epoch 117: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.6627e-04 - val_loss: 0.0091\n",
      "Epoch 118/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 5.3685e-04\n",
      "Epoch 118: val_loss did not improve from 0.00875\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 5.2880e-04 - val_loss: 0.0104\n",
      "Epoch 119/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 3.7149e-04\n",
      "Epoch 119: val_loss improved from 0.00875 to 0.00868, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.3952e-04 - val_loss: 0.0087\n",
      "Epoch 120/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.8919e-04\n",
      "Epoch 120: val_loss improved from 0.00868 to 0.00835, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 4.8919e-04 - val_loss: 0.0084\n",
      "Epoch 121/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 4.3483e-04\n",
      "Epoch 121: val_loss did not improve from 0.00835\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.4082e-04 - val_loss: 0.0090\n",
      "Epoch 122/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 5.0372e-04\n",
      "Epoch 122: val_loss did not improve from 0.00835\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.9116e-04 - val_loss: 0.0088\n",
      "Epoch 123/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.8613e-04\n",
      "Epoch 123: val_loss did not improve from 0.00835\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.2356e-04 - val_loss: 0.0085\n",
      "Epoch 124/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.5596e-04\n",
      "Epoch 124: val_loss did not improve from 0.00835\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.5107e-04 - val_loss: 0.0121\n",
      "Epoch 125/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.2368e-04\n",
      "Epoch 125: val_loss did not improve from 0.00835\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.4005e-04 - val_loss: 0.0087\n",
      "Epoch 126/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 4.4240e-04\n",
      "Epoch 126: val_loss did not improve from 0.00835\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.6372e-04 - val_loss: 0.0103\n",
      "Epoch 127/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.8453e-04\n",
      "Epoch 127: val_loss did not improve from 0.00835\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 4.7953e-04 - val_loss: 0.0094\n",
      "Epoch 128/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.5250e-04\n",
      "Epoch 128: val_loss did not improve from 0.00835\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.4922e-04 - val_loss: 0.0107\n",
      "Epoch 129/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 4.9919e-04\n",
      "Epoch 129: val_loss did not improve from 0.00835\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.7889e-04 - val_loss: 0.0094\n",
      "Epoch 130/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.7385e-04\n",
      "Epoch 130: val_loss did not improve from 0.00835\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.6782e-04 - val_loss: 0.0089\n",
      "Epoch 131/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.9543e-04\n",
      "Epoch 131: val_loss did not improve from 0.00835\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 4.9543e-04 - val_loss: 0.0100\n",
      "Epoch 132/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.3091e-04\n",
      "Epoch 132: val_loss improved from 0.00835 to 0.00742, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 4.6405e-04 - val_loss: 0.0074\n",
      "Epoch 133/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.2356e-04\n",
      "Epoch 133: val_loss did not improve from 0.00742\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.2356e-04 - val_loss: 0.0083\n",
      "Epoch 134/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.4850e-04\n",
      "Epoch 134: val_loss did not improve from 0.00742\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.6001e-04 - val_loss: 0.0088\n",
      "Epoch 135/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.2167e-04\n",
      "Epoch 135: val_loss improved from 0.00742 to 0.00727, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.1583e-04 - val_loss: 0.0073\n",
      "Epoch 136/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.7820e-04\n",
      "Epoch 136: val_loss did not improve from 0.00727\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.6240e-04 - val_loss: 0.0141\n",
      "Epoch 137/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.8390e-04\n",
      "Epoch 137: val_loss did not improve from 0.00727\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 5.1864e-04 - val_loss: 0.0084\n",
      "Epoch 138/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.2172e-04\n",
      "Epoch 138: val_loss did not improve from 0.00727\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 4.4707e-04 - val_loss: 0.0089\n",
      "Epoch 139/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.8373e-04\n",
      "Epoch 139: val_loss did not improve from 0.00727\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 4.7774e-04 - val_loss: 0.0126\n",
      "Epoch 140/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 5.1432e-04\n",
      "Epoch 140: val_loss did not improve from 0.00727\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 4.8518e-04 - val_loss: 0.0094\n",
      "Epoch 141/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.7150e-04\n",
      "Epoch 141: val_loss improved from 0.00727 to 0.00704, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.7150e-04 - val_loss: 0.0070\n",
      "Epoch 142/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.4709e-04\n",
      "Epoch 142: val_loss did not improve from 0.00704\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.4152e-04 - val_loss: 0.0084\n",
      "Epoch 143/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.8024e-04\n",
      "Epoch 143: val_loss did not improve from 0.00704\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.5838e-04 - val_loss: 0.0071\n",
      "Epoch 144/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.1339e-04\n",
      "Epoch 144: val_loss did not improve from 0.00704\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.2764e-04 - val_loss: 0.0075\n",
      "Epoch 145/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.5553e-04\n",
      "Epoch 145: val_loss did not improve from 0.00704\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.4166e-04 - val_loss: 0.0075\n",
      "Epoch 146/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 4.2357e-04\n",
      "Epoch 146: val_loss did not improve from 0.00704\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 4.2281e-04 - val_loss: 0.0076\n",
      "Epoch 147/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.1457e-04\n",
      "Epoch 147: val_loss improved from 0.00704 to 0.00658, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.1661e-04 - val_loss: 0.0066\n",
      "Epoch 148/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.5621e-04\n",
      "Epoch 148: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.2727e-04 - val_loss: 0.0072\n",
      "Epoch 149/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.3943e-04\n",
      "Epoch 149: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.0625e-04 - val_loss: 0.0068\n",
      "Epoch 150/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.3271e-04\n",
      "Epoch 150: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.4476e-04 - val_loss: 0.0085\n",
      "Epoch 151/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.1709e-04\n",
      "Epoch 151: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.6283e-04 - val_loss: 0.0076\n",
      "Epoch 152/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.8731e-04\n",
      "Epoch 152: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.3518e-04 - val_loss: 0.0091\n",
      "Epoch 153/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 4.0037e-04\n",
      "Epoch 153: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.1249e-04 - val_loss: 0.0069\n",
      "Epoch 154/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 4.5623e-04\n",
      "Epoch 154: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.5442e-04 - val_loss: 0.0079\n",
      "Epoch 155/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.3300e-04\n",
      "Epoch 155: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.2783e-04 - val_loss: 0.0075\n",
      "Epoch 156/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 5.0456e-04\n",
      "Epoch 156: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.3344e-04 - val_loss: 0.0070\n",
      "Epoch 157/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.6276e-04\n",
      "Epoch 157: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.7178e-04 - val_loss: 0.0103\n",
      "Epoch 158/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 4.2683e-04\n",
      "Epoch 158: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.7239e-04 - val_loss: 0.0092\n",
      "Epoch 159/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.6447e-04\n",
      "Epoch 159: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.5835e-04 - val_loss: 0.0075\n",
      "Epoch 160/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.7139e-04\n",
      "Epoch 160: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.1347e-04 - val_loss: 0.0070\n",
      "Epoch 161/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.5664e-04\n",
      "Epoch 161: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.4274e-04 - val_loss: 0.0074\n",
      "Epoch 162/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.0784e-04\n",
      "Epoch 162: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 4.0395e-04 - val_loss: 0.0069\n",
      "Epoch 163/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.7387e-04\n",
      "Epoch 163: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.4080e-04 - val_loss: 0.0077\n",
      "Epoch 164/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.2478e-04\n",
      "Epoch 164: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 4.3455e-04 - val_loss: 0.0071\n",
      "Epoch 165/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.1521e-04\n",
      "Epoch 165: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 4.0974e-04 - val_loss: 0.0083\n",
      "Epoch 166/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.0681e-04\n",
      "Epoch 166: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 4.1059e-04 - val_loss: 0.0071\n",
      "Epoch 167/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.3352e-04\n",
      "Epoch 167: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.4768e-04 - val_loss: 0.0075\n",
      "Epoch 168/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.2552e-04\n",
      "Epoch 168: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.3513e-04 - val_loss: 0.0091\n",
      "Epoch 169/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.8167e-04\n",
      "Epoch 169: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.3273e-04 - val_loss: 0.0068\n",
      "Epoch 170/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.4181e-04\n",
      "Epoch 170: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.2734e-04 - val_loss: 0.0073\n",
      "Epoch 171/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 4.4297e-04\n",
      "Epoch 171: val_loss did not improve from 0.00658\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.1319e-04 - val_loss: 0.0072\n",
      "Epoch 172/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.2295e-04\n",
      "Epoch 172: val_loss improved from 0.00658 to 0.00638, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 4.2295e-04 - val_loss: 0.0064\n",
      "Epoch 173/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 4.9198e-04\n",
      "Epoch 173: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.8502e-04 - val_loss: 0.0079\n",
      "Epoch 174/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.6042e-04\n",
      "Epoch 174: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.1178e-04 - val_loss: 0.0084\n",
      "Epoch 175/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.3636e-04\n",
      "Epoch 175: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.3046e-04 - val_loss: 0.0076\n",
      "Epoch 176/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.4729e-04\n",
      "Epoch 176: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.7475e-04 - val_loss: 0.0078\n",
      "Epoch 177/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.2778e-04\n",
      "Epoch 177: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.2316e-04 - val_loss: 0.0081\n",
      "Epoch 178/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.3317e-04\n",
      "Epoch 178: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.2836e-04 - val_loss: 0.0086\n",
      "Epoch 179/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.0885e-04\n",
      "Epoch 179: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.0408e-04 - val_loss: 0.0069\n",
      "Epoch 180/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.8725e-04\n",
      "Epoch 180: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.1272e-04 - val_loss: 0.0080\n",
      "Epoch 181/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 2.8973e-04\n",
      "Epoch 181: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.8672e-04 - val_loss: 0.0066\n",
      "Epoch 182/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.9395e-04\n",
      "Epoch 182: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.8850e-04 - val_loss: 0.0068\n",
      "Epoch 183/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.1448e-04\n",
      "Epoch 183: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.1448e-04 - val_loss: 0.0079\n",
      "Epoch 184/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.7285e-04\n",
      "Epoch 184: val_loss did not improve from 0.00638\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.9818e-04 - val_loss: 0.0071\n",
      "Epoch 185/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.7911e-04\n",
      "Epoch 185: val_loss improved from 0.00638 to 0.00626, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.7911e-04 - val_loss: 0.0063\n",
      "Epoch 186/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.1259e-04\n",
      "Epoch 186: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.1660e-04 - val_loss: 0.0078\n",
      "Epoch 187/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 4.2783e-04\n",
      "Epoch 187: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.8029e-04 - val_loss: 0.0075\n",
      "Epoch 188/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.0273e-04\n",
      "Epoch 188: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.0273e-04 - val_loss: 0.0078\n",
      "Epoch 189/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 4.9896e-04\n",
      "Epoch 189: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.5848e-04 - val_loss: 0.0079\n",
      "Epoch 190/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 5.2882e-04\n",
      "Epoch 190: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.2315e-04 - val_loss: 0.0082\n",
      "Epoch 191/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 5.0089e-04\n",
      "Epoch 191: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.9501e-04 - val_loss: 0.0078\n",
      "Epoch 192/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.6990e-04\n",
      "Epoch 192: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.1350e-04 - val_loss: 0.0093\n",
      "Epoch 193/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 5.0871e-04\n",
      "Epoch 193: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.3988e-04 - val_loss: 0.0099\n",
      "Epoch 194/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 4.9383e-04\n",
      "Epoch 194: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.4353e-04 - val_loss: 0.0067\n",
      "Epoch 195/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.5236e-04\n",
      "Epoch 195: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.4627e-04 - val_loss: 0.0083\n",
      "Epoch 196/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 4.3058e-04\n",
      "Epoch 196: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.5062e-04 - val_loss: 0.0087\n",
      "Epoch 197/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.2008e-04\n",
      "Epoch 197: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.2008e-04 - val_loss: 0.0086\n",
      "Epoch 198/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.3228e-04\n",
      "Epoch 198: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.3228e-04 - val_loss: 0.0115\n",
      "Epoch 199/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.3937e-04\n",
      "Epoch 199: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.3286e-04 - val_loss: 0.0080\n",
      "Epoch 200/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.4500e-04\n",
      "Epoch 200: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.3757e-04 - val_loss: 0.0066\n",
      "Epoch 201/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.0708e-04\n",
      "Epoch 201: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.0177e-04 - val_loss: 0.0066\n",
      "Epoch 202/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 5.0594e-04\n",
      "Epoch 202: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.0873e-04 - val_loss: 0.0083\n",
      "Epoch 203/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.4273e-04\n",
      "Epoch 203: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.4273e-04 - val_loss: 0.0078\n",
      "Epoch 204/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.3017e-04\n",
      "Epoch 204: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.3017e-04 - val_loss: 0.0074\n",
      "Epoch 205/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 3.0056e-04\n",
      "Epoch 205: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.5663e-04 - val_loss: 0.0097\n",
      "Epoch 206/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.3714e-04\n",
      "Epoch 206: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.6855e-04 - val_loss: 0.0089\n",
      "Epoch 207/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.4795e-04\n",
      "Epoch 207: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.6280e-04 - val_loss: 0.0107\n",
      "Epoch 208/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 5.4471e-04\n",
      "Epoch 208: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.3167e-04 - val_loss: 0.0079\n",
      "Epoch 209/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.9232e-04\n",
      "Epoch 209: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.8015e-04 - val_loss: 0.0093\n",
      "Epoch 210/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.9847e-04\n",
      "Epoch 210: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.9328e-04 - val_loss: 0.0075\n",
      "Epoch 211/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.2816e-04\n",
      "Epoch 211: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.1429e-04 - val_loss: 0.0066\n",
      "Epoch 212/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.7692e-04\n",
      "Epoch 212: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.2933e-04 - val_loss: 0.0068\n",
      "Epoch 213/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 2.8724e-04\n",
      "Epoch 213: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.8117e-04 - val_loss: 0.0076\n",
      "Epoch 214/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.0273e-04\n",
      "Epoch 214: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.0273e-04 - val_loss: 0.0071\n",
      "Epoch 215/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.6668e-04\n",
      "Epoch 215: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.9324e-04 - val_loss: 0.0068\n",
      "Epoch 216/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.0482e-04\n",
      "Epoch 216: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.1802e-04 - val_loss: 0.0080\n",
      "Epoch 217/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 1.5300e-04\n",
      "Epoch 217: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.6914e-04 - val_loss: 0.0064\n",
      "Epoch 218/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.6527e-04\n",
      "Epoch 218: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.1219e-04 - val_loss: 0.0073\n",
      "Epoch 219/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.0404e-04\n",
      "Epoch 219: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.0404e-04 - val_loss: 0.0075\n",
      "Epoch 220/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 4.1124e-04\n",
      "Epoch 220: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.9984e-04 - val_loss: 0.0074\n",
      "Epoch 221/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.2417e-04\n",
      "Epoch 221: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.2417e-04 - val_loss: 0.0068\n",
      "Epoch 222/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 2.4988e-04\n",
      "Epoch 222: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.9047e-04 - val_loss: 0.0066\n",
      "Epoch 223/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.0303e-04\n",
      "Epoch 223: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.9801e-04 - val_loss: 0.0064\n",
      "Epoch 224/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.3811e-04\n",
      "Epoch 224: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.8460e-04 - val_loss: 0.0069\n",
      "Epoch 225/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.0991e-04\n",
      "Epoch 225: val_loss improved from 0.00626 to 0.00626, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.9070e-04 - val_loss: 0.0063\n",
      "Epoch 226/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 5.6884e-04\n",
      "Epoch 226: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.3532e-04 - val_loss: 0.0086\n",
      "Epoch 227/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.5853e-04\n",
      "Epoch 227: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.2970e-04 - val_loss: 0.0080\n",
      "Epoch 228/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.2243e-04\n",
      "Epoch 228: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.1720e-04 - val_loss: 0.0078\n",
      "Epoch 229/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.8143e-04\n",
      "Epoch 229: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.8143e-04 - val_loss: 0.0071\n",
      "Epoch 230/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.1103e-04\n",
      "Epoch 230: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.8711e-04 - val_loss: 0.0064\n",
      "Epoch 231/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 5.4160e-04\n",
      "Epoch 231: val_loss did not improve from 0.00626\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.1388e-04 - val_loss: 0.0071\n",
      "Epoch 232/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.8629e-04\n",
      "Epoch 232: val_loss improved from 0.00626 to 0.00600, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.7931e-04 - val_loss: 0.0060\n",
      "Epoch 233/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.3748e-04\n",
      "Epoch 233: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.8335e-04 - val_loss: 0.0068\n",
      "Epoch 234/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.1096e-04\n",
      "Epoch 234: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.0536e-04 - val_loss: 0.0073\n",
      "Epoch 235/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 4.2958e-04\n",
      "Epoch 235: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.8172e-04 - val_loss: 0.0070\n",
      "Epoch 236/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.6896e-04\n",
      "Epoch 236: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.5942e-04 - val_loss: 0.0067\n",
      "Epoch 237/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.1999e-04\n",
      "Epoch 237: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.9904e-04 - val_loss: 0.0068\n",
      "Epoch 238/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.7153e-04\n",
      "Epoch 238: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.7730e-04 - val_loss: 0.0075\n",
      "Epoch 239/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.1398e-04\n",
      "Epoch 239: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 4.1165e-04 - val_loss: 0.0076\n",
      "Epoch 240/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.9723e-04\n",
      "Epoch 240: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.0340e-04 - val_loss: 0.0098\n",
      "Epoch 241/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.9497e-04\n",
      "Epoch 241: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 4.4959e-04 - val_loss: 0.0101\n",
      "Epoch 242/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.6528e-04\n",
      "Epoch 242: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.5978e-04 - val_loss: 0.0094\n",
      "Epoch 243/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 4.8451e-04\n",
      "Epoch 243: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.1068e-04 - val_loss: 0.0071\n",
      "Epoch 244/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.7292e-04\n",
      "Epoch 244: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.4945e-04 - val_loss: 0.0114\n",
      "Epoch 245/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 4.2875e-04\n",
      "Epoch 245: val_loss did not improve from 0.00600\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.8920e-04 - val_loss: 0.0065\n",
      "Epoch 246/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.9587e-04\n",
      "Epoch 246: val_loss improved from 0.00600 to 0.00595, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.7705e-04 - val_loss: 0.0060\n",
      "Epoch 247/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.5836e-04\n",
      "Epoch 247: val_loss improved from 0.00595 to 0.00585, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.4577e-04 - val_loss: 0.0059\n",
      "Epoch 248/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.9494e-04\n",
      "Epoch 248: val_loss improved from 0.00585 to 0.00549, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.7551e-04 - val_loss: 0.0055\n",
      "Epoch 249/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.1136e-04\n",
      "Epoch 249: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.0594e-04 - val_loss: 0.0068\n",
      "Epoch 250/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 2.9368e-04\n",
      "Epoch 250: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.7844e-04 - val_loss: 0.0090\n",
      "Epoch 251/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.5283e-04\n",
      "Epoch 251: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.4886e-04 - val_loss: 0.0067\n",
      "Epoch 252/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.0401e-04\n",
      "Epoch 252: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.0036e-04 - val_loss: 0.0076\n",
      "Epoch 253/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.1417e-04\n",
      "Epoch 253: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.0210e-04 - val_loss: 0.0091\n",
      "Epoch 254/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.9084e-04\n",
      "Epoch 254: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 4.4023e-04 - val_loss: 0.0096\n",
      "Epoch 255/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.3126e-04\n",
      "Epoch 255: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.9003e-04 - val_loss: 0.0079\n",
      "Epoch 256/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.5487e-04\n",
      "Epoch 256: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.8196e-04 - val_loss: 0.0078\n",
      "Epoch 257/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.9207e-04\n",
      "Epoch 257: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.5200e-04 - val_loss: 0.0075\n",
      "Epoch 258/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.5312e-04\n",
      "Epoch 258: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.5312e-04 - val_loss: 0.0077\n",
      "Epoch 259/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 3.9282e-04\n",
      "Epoch 259: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.7177e-04 - val_loss: 0.0086\n",
      "Epoch 260/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.4572e-04\n",
      "Epoch 260: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.7223e-04 - val_loss: 0.0058\n",
      "Epoch 261/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.2907e-04\n",
      "Epoch 261: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.5587e-04 - val_loss: 0.0066\n",
      "Epoch 262/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.4438e-04\n",
      "Epoch 262: val_loss did not improve from 0.00549\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.4305e-04 - val_loss: 0.0079\n",
      "Epoch 263/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.7728e-04\n",
      "Epoch 263: val_loss improved from 0.00549 to 0.00545, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.1453e-04 - val_loss: 0.0054\n",
      "Epoch 264/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 4.0760e-04\n",
      "Epoch 264: val_loss did not improve from 0.00545\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.0267e-04 - val_loss: 0.0066\n",
      "Epoch 265/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.2363e-04\n",
      "Epoch 265: val_loss improved from 0.00545 to 0.00544, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.4766e-04 - val_loss: 0.0054\n",
      "Epoch 266/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.7864e-04\n",
      "Epoch 266: val_loss did not improve from 0.00544\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.7359e-04 - val_loss: 0.0059\n",
      "Epoch 267/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.4884e-04\n",
      "Epoch 267: val_loss improved from 0.00544 to 0.00541, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.4884e-04 - val_loss: 0.0054\n",
      "Epoch 268/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 3.2228e-04\n",
      "Epoch 268: val_loss did not improve from 0.00541\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.3734e-04 - val_loss: 0.0069\n",
      "Epoch 269/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.7750e-04\n",
      "Epoch 269: val_loss did not improve from 0.00541\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.3209e-04 - val_loss: 0.0076\n",
      "Epoch 270/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.4433e-04\n",
      "Epoch 270: val_loss did not improve from 0.00541\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.4433e-04 - val_loss: 0.0057\n",
      "Epoch 271/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.4032e-04\n",
      "Epoch 271: val_loss did not improve from 0.00541\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.3724e-04 - val_loss: 0.0096\n",
      "Epoch 272/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.7582e-04\n",
      "Epoch 272: val_loss did not improve from 0.00541\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 3.5946e-04 - val_loss: 0.0070\n",
      "Epoch 273/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.6879e-04\n",
      "Epoch 273: val_loss did not improve from 0.00541\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.6621e-04 - val_loss: 0.0055\n",
      "Epoch 274/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.1147e-04\n",
      "Epoch 274: val_loss did not improve from 0.00541\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.2789e-04 - val_loss: 0.0062\n",
      "Epoch 275/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.3811e-04\n",
      "Epoch 275: val_loss did not improve from 0.00541\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.3326e-04 - val_loss: 0.0061\n",
      "Epoch 276/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 3.1031e-04\n",
      "Epoch 276: val_loss improved from 0.00541 to 0.00524, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.9030e-04 - val_loss: 0.0052\n",
      "Epoch 277/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 2.9452e-04\n",
      "Epoch 277: val_loss did not improve from 0.00524\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.0219e-04 - val_loss: 0.0065\n",
      "Epoch 278/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.4270e-04\n",
      "Epoch 278: val_loss did not improve from 0.00524\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.3801e-04 - val_loss: 0.0064\n",
      "Epoch 279/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.0298e-04\n",
      "Epoch 279: val_loss did not improve from 0.00524\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 3.6181e-04 - val_loss: 0.0084\n",
      "Epoch 280/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.7482e-04\n",
      "Epoch 280: val_loss did not improve from 0.00524\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.6419e-04 - val_loss: 0.0077\n",
      "Epoch 281/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.5979e-04\n",
      "Epoch 281: val_loss did not improve from 0.00524\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.3963e-04 - val_loss: 0.0055\n",
      "Epoch 282/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.2676e-04\n",
      "Epoch 282: val_loss did not improve from 0.00524\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 3.2280e-04 - val_loss: 0.0071\n",
      "Epoch 283/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 4.0539e-04\n",
      "Epoch 283: val_loss did not improve from 0.00524\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.3739e-04 - val_loss: 0.0076\n",
      "Epoch 284/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.7519e-04\n",
      "Epoch 284: val_loss did not improve from 0.00524\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.9788e-04 - val_loss: 0.0067\n",
      "Epoch 285/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 3.2191e-04\n",
      "Epoch 285: val_loss improved from 0.00524 to 0.00507, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.1502e-04 - val_loss: 0.0051\n",
      "Epoch 286/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.3996e-04\n",
      "Epoch 286: val_loss did not improve from 0.00507\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.3061e-04 - val_loss: 0.0056\n",
      "Epoch 287/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 3.2137e-04\n",
      "Epoch 287: val_loss improved from 0.00507 to 0.00463, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.9942e-04 - val_loss: 0.0046\n",
      "Epoch 288/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.5807e-04\n",
      "Epoch 288: val_loss did not improve from 0.00463\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.4473e-04 - val_loss: 0.0057\n",
      "Epoch 289/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.0708e-04\n",
      "Epoch 289: val_loss did not improve from 0.00463\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.0337e-04 - val_loss: 0.0050\n",
      "Epoch 290/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.8062e-04\n",
      "Epoch 290: val_loss did not improve from 0.00463\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.8326e-04 - val_loss: 0.0052\n",
      "Epoch 291/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.9236e-04\n",
      "Epoch 291: val_loss did not improve from 0.00463\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.7179e-04 - val_loss: 0.0047\n",
      "Epoch 292/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.2308e-04\n",
      "Epoch 292: val_loss did not improve from 0.00463\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.8248e-04 - val_loss: 0.0048\n",
      "Epoch 293/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.2930e-04\n",
      "Epoch 293: val_loss did not improve from 0.00463\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.2454e-04 - val_loss: 0.0054\n",
      "Epoch 294/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.2773e-04\n",
      "Epoch 294: val_loss improved from 0.00463 to 0.00404, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.1675e-04 - val_loss: 0.0040\n",
      "Epoch 295/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.4844e-04\n",
      "Epoch 295: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.4462e-04 - val_loss: 0.0051\n",
      "Epoch 296/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.0216e-04\n",
      "Epoch 296: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.7966e-04 - val_loss: 0.0088\n",
      "Epoch 297/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.9912e-04\n",
      "Epoch 297: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 3.1133e-04 - val_loss: 0.0053\n",
      "Epoch 298/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.4294e-04\n",
      "Epoch 298: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.3970e-04 - val_loss: 0.0059\n",
      "Epoch 299/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 3.0739e-04\n",
      "Epoch 299: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.1742e-04 - val_loss: 0.0049\n",
      "Epoch 300/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.4598e-04\n",
      "Epoch 300: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.4598e-04 - val_loss: 0.0050\n",
      "Epoch 301/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.0646e-04\n",
      "Epoch 301: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.3308e-04 - val_loss: 0.0065\n",
      "Epoch 302/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 3.1156e-04\n",
      "Epoch 302: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.9205e-04 - val_loss: 0.0054\n",
      "Epoch 303/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.7084e-04\n",
      "Epoch 303: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.7285e-04 - val_loss: 0.0054\n",
      "Epoch 304/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 3.0435e-04\n",
      "Epoch 304: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.8877e-04 - val_loss: 0.0071\n",
      "Epoch 305/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.7946e-04\n",
      "Epoch 305: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.7561e-04 - val_loss: 0.0081\n",
      "Epoch 306/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.2643e-04\n",
      "Epoch 306: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.8983e-04 - val_loss: 0.0061\n",
      "Epoch 307/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.7273e-04\n",
      "Epoch 307: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 3.6842e-04 - val_loss: 0.0068\n",
      "Epoch 308/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.8247e-04\n",
      "Epoch 308: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.9420e-04 - val_loss: 0.0058\n",
      "Epoch 309/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.9601e-04\n",
      "Epoch 309: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 2.9258e-04 - val_loss: 0.0059\n",
      "Epoch 310/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.8112e-04\n",
      "Epoch 310: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.8211e-04 - val_loss: 0.0053\n",
      "Epoch 311/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.6167e-04\n",
      "Epoch 311: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.5767e-04 - val_loss: 0.0049\n",
      "Epoch 312/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.0437e-04\n",
      "Epoch 312: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 3.0018e-04 - val_loss: 0.0047\n",
      "Epoch 313/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.1321e-04\n",
      "Epoch 313: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.8911e-04 - val_loss: 0.0058\n",
      "Epoch 314/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.6875e-04\n",
      "Epoch 314: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.4120e-04 - val_loss: 0.0049\n",
      "Epoch 315/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.9889e-04\n",
      "Epoch 315: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.9784e-04 - val_loss: 0.0114\n",
      "Epoch 316/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.8622e-04\n",
      "Epoch 316: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.8306e-04 - val_loss: 0.0055\n",
      "Epoch 317/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.1804e-04\n",
      "Epoch 317: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 3.2447e-04 - val_loss: 0.0085\n",
      "Epoch 318/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 3.5445e-04\n",
      "Epoch 318: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.0402e-04 - val_loss: 0.0067\n",
      "Epoch 319/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.0985e-04\n",
      "Epoch 319: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.6422e-04 - val_loss: 0.0064\n",
      "Epoch 320/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.2205e-04\n",
      "Epoch 320: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.3170e-04 - val_loss: 0.0061\n",
      "Epoch 321/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.2483e-04\n",
      "Epoch 321: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.2085e-04 - val_loss: 0.0055\n",
      "Epoch 322/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.6832e-04\n",
      "Epoch 322: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.5615e-04 - val_loss: 0.0054\n",
      "Epoch 323/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.9540e-04\n",
      "Epoch 323: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.9122e-04 - val_loss: 0.0059\n",
      "Epoch 324/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 3.0657e-04\n",
      "Epoch 324: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 3.2430e-04 - val_loss: 0.0075\n",
      "Epoch 325/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 2.0805e-04\n",
      "Epoch 325: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.6551e-04 - val_loss: 0.0091\n",
      "Epoch 326/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.3011e-04\n",
      "Epoch 326: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 2.2023e-04 - val_loss: 0.0053\n",
      "Epoch 327/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.9831e-04\n",
      "Epoch 327: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.6210e-04 - val_loss: 0.0096\n",
      "Epoch 328/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 3.5363e-04\n",
      "Epoch 328: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 3.5796e-04 - val_loss: 0.0062\n",
      "Epoch 329/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.6790e-04\n",
      "Epoch 329: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 3.1011e-04 - val_loss: 0.0065\n",
      "Epoch 330/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.8501e-04\n",
      "Epoch 330: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.1575e-04 - val_loss: 0.0223\n",
      "Epoch 331/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.4099e-04\n",
      "Epoch 331: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.6425e-04 - val_loss: 0.0055\n",
      "Epoch 332/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.9945e-04\n",
      "Epoch 332: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 3.5154e-04 - val_loss: 0.0082\n",
      "Epoch 333/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 3.2515e-04\n",
      "Epoch 333: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.8660e-04 - val_loss: 0.0067\n",
      "Epoch 334/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.0834e-04\n",
      "Epoch 334: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.4416e-04 - val_loss: 0.0077\n",
      "Epoch 335/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.7020e-04\n",
      "Epoch 335: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.6023e-04 - val_loss: 0.0062\n",
      "Epoch 336/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.7726e-04\n",
      "Epoch 336: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.3565e-04 - val_loss: 0.0054\n",
      "Epoch 337/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.3407e-04\n",
      "Epoch 337: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.4273e-04 - val_loss: 0.0065\n",
      "Epoch 338/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.0016e-04\n",
      "Epoch 338: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.3744e-04 - val_loss: 0.0053\n",
      "Epoch 339/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.2197e-04\n",
      "Epoch 339: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.4759e-04 - val_loss: 0.0058\n",
      "Epoch 340/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.1039e-04\n",
      "Epoch 340: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.0852e-04 - val_loss: 0.0055\n",
      "Epoch 341/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.0666e-04\n",
      "Epoch 341: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2875e-04 - val_loss: 0.0059\n",
      "Epoch 342/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.5798e-04\n",
      "Epoch 342: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.6827e-04 - val_loss: 0.0059\n",
      "Epoch 343/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.9622e-04\n",
      "Epoch 343: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.2444e-04 - val_loss: 0.0070\n",
      "Epoch 344/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.5234e-04\n",
      "Epoch 344: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.3815e-04 - val_loss: 0.0076\n",
      "Epoch 345/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 2.4518e-04\n",
      "Epoch 345: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.2716e-04 - val_loss: 0.0047\n",
      "Epoch 346/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3649e-04\n",
      "Epoch 346: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.3649e-04 - val_loss: 0.0054\n",
      "Epoch 347/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.7350e-04\n",
      "Epoch 347: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.1561e-04 - val_loss: 0.0051\n",
      "Epoch 348/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.2610e-04\n",
      "Epoch 348: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.0816e-04 - val_loss: 0.0072\n",
      "Epoch 349/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.6900e-04\n",
      "Epoch 349: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.6086e-04 - val_loss: 0.0064\n",
      "Epoch 350/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.7759e-04\n",
      "Epoch 350: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.6236e-04 - val_loss: 0.0050\n",
      "Epoch 351/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.9957e-04\n",
      "Epoch 351: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.2144e-04 - val_loss: 0.0054\n",
      "Epoch 352/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.8325e-04\n",
      "Epoch 352: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.1718e-04 - val_loss: 0.0061\n",
      "Epoch 353/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.6841e-04\n",
      "Epoch 353: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.5244e-04 - val_loss: 0.0056\n",
      "Epoch 354/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.0768e-04\n",
      "Epoch 354: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.8503e-04 - val_loss: 0.0096\n",
      "Epoch 355/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.7915e-04\n",
      "Epoch 355: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.7915e-04 - val_loss: 0.0074\n",
      "Epoch 356/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.9931e-04\n",
      "Epoch 356: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.9196e-04 - val_loss: 0.0045\n",
      "Epoch 357/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.9717e-04\n",
      "Epoch 357: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.9541e-04 - val_loss: 0.0056\n",
      "Epoch 358/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.7636e-04\n",
      "Epoch 358: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.2782e-04 - val_loss: 0.0044\n",
      "Epoch 359/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3023e-04\n",
      "Epoch 359: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.3023e-04 - val_loss: 0.0084\n",
      "Epoch 360/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.6423e-04\n",
      "Epoch 360: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.3362e-04 - val_loss: 0.0048\n",
      "Epoch 361/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.0474e-04\n",
      "Epoch 361: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.9008e-04 - val_loss: 0.0052\n",
      "Epoch 362/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.0385e-04\n",
      "Epoch 362: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.0174e-04 - val_loss: 0.0051\n",
      "Epoch 363/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.9287e-04\n",
      "Epoch 363: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.1197e-04 - val_loss: 0.0050\n",
      "Epoch 364/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 2.1134e-04\n",
      "Epoch 364: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.2569e-04 - val_loss: 0.0056\n",
      "Epoch 365/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.1327e-04\n",
      "Epoch 365: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.0175e-04 - val_loss: 0.0058\n",
      "Epoch 366/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 2.3732e-04\n",
      "Epoch 366: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.9327e-04 - val_loss: 0.0056\n",
      "Epoch 367/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.7519e-04\n",
      "Epoch 367: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.7370e-04 - val_loss: 0.0057\n",
      "Epoch 368/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.2739e-04\n",
      "Epoch 368: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.0341e-04 - val_loss: 0.0086\n",
      "Epoch 369/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.7466e-04\n",
      "Epoch 369: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.5109e-04 - val_loss: 0.0064\n",
      "Epoch 370/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.9457e-04\n",
      "Epoch 370: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.1550e-04 - val_loss: 0.0048\n",
      "Epoch 371/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.5295e-04\n",
      "Epoch 371: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.5157e-04 - val_loss: 0.0051\n",
      "Epoch 372/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.7171e-04\n",
      "Epoch 372: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.7171e-04 - val_loss: 0.0060\n",
      "Epoch 373/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 2.5568e-04\n",
      "Epoch 373: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.1167e-04 - val_loss: 0.0047\n",
      "Epoch 374/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.6629e-04\n",
      "Epoch 374: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.6364e-04 - val_loss: 0.0072\n",
      "Epoch 375/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.5174e-04\n",
      "Epoch 375: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.1920e-04 - val_loss: 0.0049\n",
      "Epoch 376/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.9506e-04\n",
      "Epoch 376: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.1109e-04 - val_loss: 0.0056\n",
      "Epoch 377/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.9868e-04\n",
      "Epoch 377: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.1871e-04 - val_loss: 0.0056\n",
      "Epoch 378/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.3018e-04\n",
      "Epoch 378: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.0005e-04 - val_loss: 0.0067\n",
      "Epoch 379/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.4092e-04\n",
      "Epoch 379: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.3020e-04 - val_loss: 0.0058\n",
      "Epoch 380/2000\n",
      " 8/16 [==============>...............] - ETA: 0s - loss: 1.9034e-04\n",
      "Epoch 380: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.2988e-04 - val_loss: 0.0085\n",
      "Epoch 381/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.2134e-04\n",
      "Epoch 381: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.0972e-04 - val_loss: 0.0073\n",
      "Epoch 382/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.0382e-04\n",
      "Epoch 382: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.0271e-04 - val_loss: 0.0052\n",
      "Epoch 383/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.5650e-04\n",
      "Epoch 383: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.4085e-04 - val_loss: 0.0076\n",
      "Epoch 384/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.8402e-04\n",
      "Epoch 384: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.8264e-04 - val_loss: 0.0050\n",
      "Epoch 385/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.0647e-04\n",
      "Epoch 385: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.0647e-04 - val_loss: 0.0053\n",
      "Epoch 386/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.7009e-04\n",
      "Epoch 386: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.4587e-04 - val_loss: 0.0063\n",
      "Epoch 387/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.3170e-04\n",
      "Epoch 387: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.1926e-04 - val_loss: 0.0064\n",
      "Epoch 388/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.9430e-04\n",
      "Epoch 388: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.7925e-04 - val_loss: 0.0049\n",
      "Epoch 389/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.1396e-04\n",
      "Epoch 389: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.8237e-04 - val_loss: 0.0043\n",
      "Epoch 390/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.2177e-04\n",
      "Epoch 390: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.2090e-04 - val_loss: 0.0054\n",
      "Epoch 391/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.9532e-04\n",
      "Epoch 391: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.3047e-04 - val_loss: 0.0050\n",
      "Epoch 392/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.3550e-04\n",
      "Epoch 392: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.3396e-04 - val_loss: 0.0055\n",
      "Epoch 393/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.8948e-04\n",
      "Epoch 393: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.8685e-04 - val_loss: 0.0045\n",
      "Epoch 394/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.8925e-04\n",
      "Epoch 394: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.7093e-04 - val_loss: 0.0052\n",
      "Epoch 395/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.2162e-04\n",
      "Epoch 395: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.0931e-04 - val_loss: 0.0053\n",
      "Epoch 396/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.2284e-04\n",
      "Epoch 396: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.1228e-04 - val_loss: 0.0056\n",
      "Epoch 397/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.0799e-04\n",
      "Epoch 397: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.1907e-04 - val_loss: 0.0061\n",
      "Epoch 398/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 2.2049e-04\n",
      "Epoch 398: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.7809e-04 - val_loss: 0.0052\n",
      "Epoch 399/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.1250e-04\n",
      "Epoch 399: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.3170e-04 - val_loss: 0.0079\n",
      "Epoch 400/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 2.1866e-04\n",
      "Epoch 400: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.1958e-04 - val_loss: 0.0060\n",
      "Epoch 401/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.7936e-04\n",
      "Epoch 401: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.8420e-04 - val_loss: 0.0046\n",
      "Epoch 402/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.3584e-04\n",
      "Epoch 402: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.9887e-04 - val_loss: 0.0046\n",
      "Epoch 403/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.8818e-04\n",
      "Epoch 403: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.7183e-04 - val_loss: 0.0045\n",
      "Epoch 404/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.3946e-04\n",
      "Epoch 404: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.6516e-04 - val_loss: 0.0051\n",
      "Epoch 405/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.6810e-04\n",
      "Epoch 405: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.9335e-04 - val_loss: 0.0057\n",
      "Epoch 406/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.1889e-04\n",
      "Epoch 406: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.9217e-04 - val_loss: 0.0052\n",
      "Epoch 407/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.2313e-04\n",
      "Epoch 407: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.1605e-04 - val_loss: 0.0048\n",
      "Epoch 408/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.2907e-04\n",
      "Epoch 408: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.2277e-04 - val_loss: 0.0051\n",
      "Epoch 409/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.5311e-04\n",
      "Epoch 409: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.5311e-04 - val_loss: 0.0041\n",
      "Epoch 410/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.9822e-04\n",
      "Epoch 410: val_loss did not improve from 0.00404\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.9785e-04 - val_loss: 0.0066\n",
      "Epoch 411/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.7170e-04\n",
      "Epoch 411: val_loss improved from 0.00404 to 0.00398, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.1354e-04 - val_loss: 0.0040\n",
      "Epoch 412/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3128e-04\n",
      "Epoch 412: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.3128e-04 - val_loss: 0.0051\n",
      "Epoch 413/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.2958e-04\n",
      "Epoch 413: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.8123e-04 - val_loss: 0.0083\n",
      "Epoch 414/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.6983e-04\n",
      "Epoch 414: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.5314e-04 - val_loss: 0.0054\n",
      "Epoch 415/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.0728e-04\n",
      "Epoch 415: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.9777e-04 - val_loss: 0.0049\n",
      "Epoch 416/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.7132e-04\n",
      "Epoch 416: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.7370e-04 - val_loss: 0.0045\n",
      "Epoch 417/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.3393e-04\n",
      "Epoch 417: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2038e-04 - val_loss: 0.0048\n",
      "Epoch 418/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.7638e-04\n",
      "Epoch 418: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.7464e-04 - val_loss: 0.0044\n",
      "Epoch 419/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.6977e-04\n",
      "Epoch 419: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.9151e-04 - val_loss: 0.0048\n",
      "Epoch 420/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 9.6560e-05\n",
      "Epoch 420: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.5364e-04 - val_loss: 0.0044\n",
      "Epoch 421/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.1052e-04\n",
      "Epoch 421: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.1466e-04 - val_loss: 0.0050\n",
      "Epoch 422/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.4628e-04\n",
      "Epoch 422: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.7631e-04 - val_loss: 0.0060\n",
      "Epoch 423/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0909e-04\n",
      "Epoch 423: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.7013e-04 - val_loss: 0.0045\n",
      "Epoch 424/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.0894e-04\n",
      "Epoch 424: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.9662e-04 - val_loss: 0.0069\n",
      "Epoch 425/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5721e-04\n",
      "Epoch 425: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.4931e-04 - val_loss: 0.0049\n",
      "Epoch 426/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.1107e-04\n",
      "Epoch 426: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.7232e-04 - val_loss: 0.0041\n",
      "Epoch 427/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.9037e-04\n",
      "Epoch 427: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.9876e-04 - val_loss: 0.0047\n",
      "Epoch 428/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.6921e-04\n",
      "Epoch 428: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.5055e-04 - val_loss: 0.0042\n",
      "Epoch 429/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5218e-04\n",
      "Epoch 429: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.4903e-04 - val_loss: 0.0042\n",
      "Epoch 430/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.9291e-04\n",
      "Epoch 430: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.8034e-04 - val_loss: 0.0052\n",
      "Epoch 431/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.6558e-04\n",
      "Epoch 431: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.6307e-04 - val_loss: 0.0061\n",
      "Epoch 432/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.1853e-04\n",
      "Epoch 432: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.1588e-04 - val_loss: 0.0046\n",
      "Epoch 433/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.1033e-04\n",
      "Epoch 433: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.1033e-04 - val_loss: 0.0045\n",
      "Epoch 434/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.7661e-04\n",
      "Epoch 434: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.7152e-04 - val_loss: 0.0075\n",
      "Epoch 435/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 2.0882e-04\n",
      "Epoch 435: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.9256e-04 - val_loss: 0.0051\n",
      "Epoch 436/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.9943e-04\n",
      "Epoch 436: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.9695e-04 - val_loss: 0.0044\n",
      "Epoch 437/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.8843e-04\n",
      "Epoch 437: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.8375e-04 - val_loss: 0.0094\n",
      "Epoch 438/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.1010e-04\n",
      "Epoch 438: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.0741e-04 - val_loss: 0.0050\n",
      "Epoch 439/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.0019e-04\n",
      "Epoch 439: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2121e-04 - val_loss: 0.0053\n",
      "Epoch 440/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.5227e-04\n",
      "Epoch 440: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.5213e-04 - val_loss: 0.0108\n",
      "Epoch 441/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.7156e-04\n",
      "Epoch 441: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.7156e-04 - val_loss: 0.0054\n",
      "Epoch 442/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.9874e-04\n",
      "Epoch 442: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.9874e-04 - val_loss: 0.0052\n",
      "Epoch 443/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.8235e-04\n",
      "Epoch 443: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 2.6832e-04 - val_loss: 0.0125\n",
      "Epoch 444/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.7264e-04\n",
      "Epoch 444: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.5879e-04 - val_loss: 0.0071\n",
      "Epoch 445/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.6365e-04\n",
      "Epoch 445: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.6289e-04 - val_loss: 0.0084\n",
      "Epoch 446/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 2.1109e-04\n",
      "Epoch 446: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.5021e-04 - val_loss: 0.0074\n",
      "Epoch 447/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.0335e-04\n",
      "Epoch 447: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.0335e-04 - val_loss: 0.0051\n",
      "Epoch 448/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.9511e-04\n",
      "Epoch 448: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.9511e-04 - val_loss: 0.0047\n",
      "Epoch 449/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.6972e-04\n",
      "Epoch 449: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.5226e-04 - val_loss: 0.0051\n",
      "Epoch 450/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.5892e-04\n",
      "Epoch 450: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.5892e-04 - val_loss: 0.0048\n",
      "Epoch 451/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.6818e-04\n",
      "Epoch 451: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.8982e-04 - val_loss: 0.0058\n",
      "Epoch 452/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.8248e-04\n",
      "Epoch 452: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.5529e-04 - val_loss: 0.0065\n",
      "Epoch 453/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.7043e-04\n",
      "Epoch 453: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.6841e-04 - val_loss: 0.0044\n",
      "Epoch 454/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.8978e-04\n",
      "Epoch 454: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.7932e-04 - val_loss: 0.0045\n",
      "Epoch 455/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.2531e-04\n",
      "Epoch 455: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.0381e-04 - val_loss: 0.0054\n",
      "Epoch 456/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2100e-04\n",
      "Epoch 456: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.2100e-04 - val_loss: 0.0041\n",
      "Epoch 457/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.4374e-04\n",
      "Epoch 457: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 1.4234e-04 - val_loss: 0.0062\n",
      "Epoch 458/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.8498e-04\n",
      "Epoch 458: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.8498e-04 - val_loss: 0.0062\n",
      "Epoch 459/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4578e-04\n",
      "Epoch 459: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.7046e-04 - val_loss: 0.0053\n",
      "Epoch 460/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.1807e-04\n",
      "Epoch 460: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.9511e-04 - val_loss: 0.0052\n",
      "Epoch 461/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.0898e-04\n",
      "Epoch 461: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.8296e-04 - val_loss: 0.0068\n",
      "Epoch 462/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5901e-04\n",
      "Epoch 462: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.7301e-04 - val_loss: 0.0047\n",
      "Epoch 463/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3648e-04\n",
      "Epoch 463: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.6057e-04 - val_loss: 0.0059\n",
      "Epoch 464/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.4003e-04\n",
      "Epoch 464: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 1.8439e-04 - val_loss: 0.0107\n",
      "Epoch 465/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.4823e-04\n",
      "Epoch 465: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 2.6091e-04 - val_loss: 0.0063\n",
      "Epoch 466/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.1522e-04\n",
      "Epoch 466: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.9674e-04 - val_loss: 0.0079\n",
      "Epoch 467/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.7904e-04\n",
      "Epoch 467: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.7767e-04 - val_loss: 0.0082\n",
      "Epoch 468/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.8678e-04\n",
      "Epoch 468: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.7172e-04 - val_loss: 0.0056\n",
      "Epoch 469/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.9637e-04\n",
      "Epoch 469: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.9007e-04 - val_loss: 0.0067\n",
      "Epoch 470/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.7730e-04\n",
      "Epoch 470: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.7616e-04 - val_loss: 0.0056\n",
      "Epoch 471/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.3021e-05\n",
      "Epoch 471: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.5513e-04 - val_loss: 0.0060\n",
      "Epoch 472/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5169e-04\n",
      "Epoch 472: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.6495e-04 - val_loss: 0.0050\n",
      "Epoch 473/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.5605e-04\n",
      "Epoch 473: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.2299e-04 - val_loss: 0.0080\n",
      "Epoch 474/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3344e-04\n",
      "Epoch 474: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.5134e-04 - val_loss: 0.0049\n",
      "Epoch 475/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.7570e-04\n",
      "Epoch 475: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.6437e-04 - val_loss: 0.0064\n",
      "Epoch 476/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.2646e-04\n",
      "Epoch 476: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.3751e-04 - val_loss: 0.0067\n",
      "Epoch 477/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.2972e-04\n",
      "Epoch 477: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 2.1688e-04 - val_loss: 0.0084\n",
      "Epoch 478/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4615e-04\n",
      "Epoch 478: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.7144e-04 - val_loss: 0.0054\n",
      "Epoch 479/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.6323e-04\n",
      "Epoch 479: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.5357e-04 - val_loss: 0.0077\n",
      "Epoch 480/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5965e-04\n",
      "Epoch 480: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.4415e-04 - val_loss: 0.0055\n",
      "Epoch 481/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.1515e-04\n",
      "Epoch 481: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.0085e-04 - val_loss: 0.0080\n",
      "Epoch 482/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5231e-04\n",
      "Epoch 482: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.3883e-04 - val_loss: 0.0048\n",
      "Epoch 483/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.6898e-04\n",
      "Epoch 483: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.8645e-04 - val_loss: 0.0081\n",
      "Epoch 484/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.5465e-04\n",
      "Epoch 484: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.6309e-04 - val_loss: 0.0056\n",
      "Epoch 485/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.8798e-04\n",
      "Epoch 485: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.6656e-04 - val_loss: 0.0051\n",
      "Epoch 486/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.2771e-04\n",
      "Epoch 486: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.5585e-04 - val_loss: 0.0061\n",
      "Epoch 487/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.9221e-04\n",
      "Epoch 487: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.8116e-04 - val_loss: 0.0055\n",
      "Epoch 488/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.5984e-04\n",
      "Epoch 488: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.5984e-04 - val_loss: 0.0065\n",
      "Epoch 489/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.5821e-04\n",
      "Epoch 489: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.5681e-04 - val_loss: 0.0059\n",
      "Epoch 490/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2033e-04\n",
      "Epoch 490: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.2033e-04 - val_loss: 0.0044\n",
      "Epoch 491/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3438e-04\n",
      "Epoch 491: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.5639e-04 - val_loss: 0.0063\n",
      "Epoch 492/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.4755e-04\n",
      "Epoch 492: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.7155e-04 - val_loss: 0.0059\n",
      "Epoch 493/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.8471e-04\n",
      "Epoch 493: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.7078e-04 - val_loss: 0.0066\n",
      "Epoch 494/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.7387e-04\n",
      "Epoch 494: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.6324e-04 - val_loss: 0.0046\n",
      "Epoch 495/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.6194e-04\n",
      "Epoch 495: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.5461e-04 - val_loss: 0.0052\n",
      "Epoch 496/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.7269e-04\n",
      "Epoch 496: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.7269e-04 - val_loss: 0.0049\n",
      "Epoch 497/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.5542e-04\n",
      "Epoch 497: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.5391e-04 - val_loss: 0.0043\n",
      "Epoch 498/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.7831e-04\n",
      "Epoch 498: val_loss did not improve from 0.00398\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.5571e-04 - val_loss: 0.0057\n",
      "Epoch 499/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.2881e-04\n",
      "Epoch 499: val_loss improved from 0.00398 to 0.00354, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.1612e-04 - val_loss: 0.0035\n",
      "Epoch 500/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2393e-04\n",
      "Epoch 500: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.2393e-04 - val_loss: 0.0052\n",
      "Epoch 501/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2170e-04\n",
      "Epoch 501: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.2170e-04 - val_loss: 0.0049\n",
      "Epoch 502/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3803e-04\n",
      "Epoch 502: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3687e-04 - val_loss: 0.0048\n",
      "Epoch 503/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2471e-04\n",
      "Epoch 503: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.2337e-04 - val_loss: 0.0041\n",
      "Epoch 504/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1974e-04\n",
      "Epoch 504: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1897e-04 - val_loss: 0.0046\n",
      "Epoch 505/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.1415e-04\n",
      "Epoch 505: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.1415e-04 - val_loss: 0.0041\n",
      "Epoch 506/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2886e-04\n",
      "Epoch 506: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.2787e-04 - val_loss: 0.0040\n",
      "Epoch 507/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3273e-04\n",
      "Epoch 507: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.3131e-04 - val_loss: 0.0040\n",
      "Epoch 508/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.4934e-04\n",
      "Epoch 508: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1982e-04 - val_loss: 0.0041\n",
      "Epoch 509/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5281e-04\n",
      "Epoch 509: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.4289e-04 - val_loss: 0.0041\n",
      "Epoch 510/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.6139e-04\n",
      "Epoch 510: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.6055e-04 - val_loss: 0.0039\n",
      "Epoch 511/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0691e-04\n",
      "Epoch 511: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0197e-04 - val_loss: 0.0038\n",
      "Epoch 512/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3888e-04\n",
      "Epoch 512: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.3742e-04 - val_loss: 0.0064\n",
      "Epoch 513/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.2913e-04\n",
      "Epoch 513: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.3961e-04 - val_loss: 0.0043\n",
      "Epoch 514/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.6638e-04\n",
      "Epoch 514: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.6544e-04 - val_loss: 0.0087\n",
      "Epoch 515/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.9103e-04\n",
      "Epoch 515: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.8457e-04 - val_loss: 0.0091\n",
      "Epoch 516/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1640e-04\n",
      "Epoch 516: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.4644e-04 - val_loss: 0.0063\n",
      "Epoch 517/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5061e-04\n",
      "Epoch 517: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.4221e-04 - val_loss: 0.0057\n",
      "Epoch 518/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.6079e-04\n",
      "Epoch 518: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.5936e-04 - val_loss: 0.0063\n",
      "Epoch 519/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.4589e-04\n",
      "Epoch 519: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.4400e-04 - val_loss: 0.0055\n",
      "Epoch 520/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3806e-04\n",
      "Epoch 520: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3651e-04 - val_loss: 0.0048\n",
      "Epoch 521/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.4968e-04\n",
      "Epoch 521: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.4253e-04 - val_loss: 0.0075\n",
      "Epoch 522/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.0987e-04\n",
      "Epoch 522: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.9808e-04 - val_loss: 0.0055\n",
      "Epoch 523/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.3432e-04\n",
      "Epoch 523: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1756e-04 - val_loss: 0.0053\n",
      "Epoch 524/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5142e-04\n",
      "Epoch 524: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.4361e-04 - val_loss: 0.0047\n",
      "Epoch 525/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.7226e-04\n",
      "Epoch 525: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.3549e-04 - val_loss: 0.0045\n",
      "Epoch 526/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4284e-04\n",
      "Epoch 526: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.3544e-04 - val_loss: 0.0057\n",
      "Epoch 527/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 1.7909e-04\n",
      "Epoch 527: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.6978e-04 - val_loss: 0.0072\n",
      "Epoch 528/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.4407e-04\n",
      "Epoch 528: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.5598e-04 - val_loss: 0.0045\n",
      "Epoch 529/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.9571e-04\n",
      "Epoch 529: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.9571e-04 - val_loss: 0.0060\n",
      "Epoch 530/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4597e-04\n",
      "Epoch 530: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.3852e-04 - val_loss: 0.0046\n",
      "Epoch 531/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.4715e-04\n",
      "Epoch 531: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.3661e-04 - val_loss: 0.0061\n",
      "Epoch 532/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.7631e-04\n",
      "Epoch 532: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.7429e-04 - val_loss: 0.0039\n",
      "Epoch 533/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.8605e-04\n",
      "Epoch 533: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.7712e-04 - val_loss: 0.0067\n",
      "Epoch 534/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5596e-04\n",
      "Epoch 534: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 1.4675e-04 - val_loss: 0.0061\n",
      "Epoch 535/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.1398e-04\n",
      "Epoch 535: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.6358e-04 - val_loss: 0.0057\n",
      "Epoch 536/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.5469e-04\n",
      "Epoch 536: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.5317e-04 - val_loss: 0.0048\n",
      "Epoch 537/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.0328e-04\n",
      "Epoch 537: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3446e-04 - val_loss: 0.0056\n",
      "Epoch 538/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2638e-04\n",
      "Epoch 538: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.2470e-04 - val_loss: 0.0044\n",
      "Epoch 539/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1850e-04\n",
      "Epoch 539: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.1101e-04 - val_loss: 0.0051\n",
      "Epoch 540/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.3328e-04\n",
      "Epoch 540: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.3328e-04 - val_loss: 0.0049\n",
      "Epoch 541/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.0883e-04\n",
      "Epoch 541: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.0805e-04 - val_loss: 0.0042\n",
      "Epoch 542/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0979e-04\n",
      "Epoch 542: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.3626e-04 - val_loss: 0.0044\n",
      "Epoch 543/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1916e-04\n",
      "Epoch 543: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.1240e-04 - val_loss: 0.0046\n",
      "Epoch 544/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.3624e-04\n",
      "Epoch 544: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.3624e-04 - val_loss: 0.0043\n",
      "Epoch 545/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2775e-04\n",
      "Epoch 545: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.2625e-04 - val_loss: 0.0068\n",
      "Epoch 546/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0814e-04\n",
      "Epoch 546: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.3383e-04 - val_loss: 0.0036\n",
      "Epoch 547/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3931e-04\n",
      "Epoch 547: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3765e-04 - val_loss: 0.0079\n",
      "Epoch 548/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.5124e-04\n",
      "Epoch 548: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.4892e-04 - val_loss: 0.0044\n",
      "Epoch 549/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.3046e-04\n",
      "Epoch 549: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.1635e-04 - val_loss: 0.0047\n",
      "Epoch 550/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.2125e-04\n",
      "Epoch 550: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.1760e-04 - val_loss: 0.0045\n",
      "Epoch 551/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 1.7232e-04\n",
      "Epoch 551: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.2967e-04 - val_loss: 0.0043\n",
      "Epoch 552/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1968e-04\n",
      "Epoch 552: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1915e-04 - val_loss: 0.0043\n",
      "Epoch 553/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.2656e-04\n",
      "Epoch 553: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0893e-04 - val_loss: 0.0047\n",
      "Epoch 554/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4394e-04\n",
      "Epoch 554: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3490e-04 - val_loss: 0.0041\n",
      "Epoch 555/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1134e-04\n",
      "Epoch 555: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1051e-04 - val_loss: 0.0045\n",
      "Epoch 556/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1972e-04\n",
      "Epoch 556: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.1229e-04 - val_loss: 0.0039\n",
      "Epoch 557/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0191e-04\n",
      "Epoch 557: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.0191e-04 - val_loss: 0.0037\n",
      "Epoch 558/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5370e-04\n",
      "Epoch 558: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.4356e-04 - val_loss: 0.0051\n",
      "Epoch 559/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.2060e-04\n",
      "Epoch 559: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0806e-04 - val_loss: 0.0042\n",
      "Epoch 560/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.3026e-05\n",
      "Epoch 560: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0421e-04 - val_loss: 0.0037\n",
      "Epoch 561/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.6436e-05\n",
      "Epoch 561: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1171e-04 - val_loss: 0.0049\n",
      "Epoch 562/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2725e-04\n",
      "Epoch 562: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.2725e-04 - val_loss: 0.0040\n",
      "Epoch 563/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 7.3993e-05\n",
      "Epoch 563: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0226e-04 - val_loss: 0.0036\n",
      "Epoch 564/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4177e-04\n",
      "Epoch 564: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.3450e-04 - val_loss: 0.0037\n",
      "Epoch 565/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.1571e-04\n",
      "Epoch 565: val_loss did not improve from 0.00354\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.1571e-04 - val_loss: 0.0041\n",
      "Epoch 566/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1664e-04\n",
      "Epoch 566: val_loss improved from 0.00354 to 0.00330, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0608e-04 - val_loss: 0.0033\n",
      "Epoch 567/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2198e-04\n",
      "Epoch 567: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2097e-04 - val_loss: 0.0051\n",
      "Epoch 568/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.5087e-04\n",
      "Epoch 568: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.5087e-04 - val_loss: 0.0057\n",
      "Epoch 569/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0722e-04\n",
      "Epoch 569: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.0722e-04 - val_loss: 0.0039\n",
      "Epoch 570/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.0653e-04\n",
      "Epoch 570: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0588e-04 - val_loss: 0.0039\n",
      "Epoch 571/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.4032e-04\n",
      "Epoch 571: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.5369e-04 - val_loss: 0.0053\n",
      "Epoch 572/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2560e-04\n",
      "Epoch 572: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.2560e-04 - val_loss: 0.0050\n",
      "Epoch 573/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4409e-04\n",
      "Epoch 573: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.3605e-04 - val_loss: 0.0046\n",
      "Epoch 574/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.5550e-05\n",
      "Epoch 574: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.3454e-04 - val_loss: 0.0044\n",
      "Epoch 575/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1146e-04\n",
      "Epoch 575: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.3316e-04 - val_loss: 0.0045\n",
      "Epoch 576/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.3695e-05\n",
      "Epoch 576: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3772e-04 - val_loss: 0.0043\n",
      "Epoch 577/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1635e-04\n",
      "Epoch 577: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.1095e-04 - val_loss: 0.0044\n",
      "Epoch 578/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.7930e-04\n",
      "Epoch 578: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.5720e-04 - val_loss: 0.0055\n",
      "Epoch 579/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.6699e-04\n",
      "Epoch 579: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.5333e-04 - val_loss: 0.0060\n",
      "Epoch 580/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.3688e-04\n",
      "Epoch 580: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.3688e-04 - val_loss: 0.0045\n",
      "Epoch 581/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3194e-04\n",
      "Epoch 581: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3027e-04 - val_loss: 0.0044\n",
      "Epoch 582/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.7551e-04\n",
      "Epoch 582: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.7219e-04 - val_loss: 0.0084\n",
      "Epoch 583/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5580e-04\n",
      "Epoch 583: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3591e-04 - val_loss: 0.0038\n",
      "Epoch 584/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.2449e-04\n",
      "Epoch 584: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.2267e-04 - val_loss: 0.0076\n",
      "Epoch 585/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.6730e-04\n",
      "Epoch 585: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.4129e-04 - val_loss: 0.0036\n",
      "Epoch 586/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4524e-04\n",
      "Epoch 586: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.3755e-04 - val_loss: 0.0047\n",
      "Epoch 587/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.2449e-04\n",
      "Epoch 587: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.5084e-04 - val_loss: 0.0050\n",
      "Epoch 588/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.0542e-04\n",
      "Epoch 588: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 2.0298e-04 - val_loss: 0.0049\n",
      "Epoch 589/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.0890e-04\n",
      "Epoch 589: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.2406e-04 - val_loss: 0.0038\n",
      "Epoch 590/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2858e-04\n",
      "Epoch 590: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3179e-04 - val_loss: 0.0041\n",
      "Epoch 591/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.7712e-04\n",
      "Epoch 591: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.7557e-04 - val_loss: 0.0062\n",
      "Epoch 592/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.6596e-04\n",
      "Epoch 592: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.4762e-04 - val_loss: 0.0060\n",
      "Epoch 593/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0378e-04\n",
      "Epoch 593: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.3087e-04 - val_loss: 0.0043\n",
      "Epoch 594/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.8674e-04\n",
      "Epoch 594: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.6316e-04 - val_loss: 0.0057\n",
      "Epoch 595/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.7218e-04\n",
      "Epoch 595: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.6359e-04 - val_loss: 0.0049\n",
      "Epoch 596/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3211e-04\n",
      "Epoch 596: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.3692e-04 - val_loss: 0.0048\n",
      "Epoch 597/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.7319e-04\n",
      "Epoch 597: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.7211e-04 - val_loss: 0.0079\n",
      "Epoch 598/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.1103e-04\n",
      "Epoch 598: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.2214e-04 - val_loss: 0.0056\n",
      "Epoch 599/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.8715e-04\n",
      "Epoch 599: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.8416e-04 - val_loss: 0.0081\n",
      "Epoch 600/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.1997e-04\n",
      "Epoch 600: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.6993e-04 - val_loss: 0.0036\n",
      "Epoch 601/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.1765e-04\n",
      "Epoch 601: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.1765e-04 - val_loss: 0.0067\n",
      "Epoch 602/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.9945e-04\n",
      "Epoch 602: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.9160e-04 - val_loss: 0.0068\n",
      "Epoch 603/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.9203e-04\n",
      "Epoch 603: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.5764e-04 - val_loss: 0.0043\n",
      "Epoch 604/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.9302e-04\n",
      "Epoch 604: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.2199e-04 - val_loss: 0.0058\n",
      "Epoch 605/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.6148e-04\n",
      "Epoch 605: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.4925e-04 - val_loss: 0.0073\n",
      "Epoch 606/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.5980e-04\n",
      "Epoch 606: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.6040e-04 - val_loss: 0.0127\n",
      "Epoch 607/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.9606e-04\n",
      "Epoch 607: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 2.9276e-04 - val_loss: 0.0078\n",
      "Epoch 608/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.6052e-04\n",
      "Epoch 608: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 3.4090e-04 - val_loss: 0.0068\n",
      "Epoch 609/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 2.4206e-04\n",
      "Epoch 609: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.9582e-04 - val_loss: 0.0064\n",
      "Epoch 610/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.6038e-04\n",
      "Epoch 610: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.5290e-04 - val_loss: 0.0055\n",
      "Epoch 611/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.3189e-04\n",
      "Epoch 611: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.3189e-04 - val_loss: 0.0050\n",
      "Epoch 612/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3656e-04\n",
      "Epoch 612: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.3532e-04 - val_loss: 0.0059\n",
      "Epoch 613/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4016e-04\n",
      "Epoch 613: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3295e-04 - val_loss: 0.0046\n",
      "Epoch 614/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.6003e-04\n",
      "Epoch 614: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.5311e-04 - val_loss: 0.0051\n",
      "Epoch 615/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.5923e-04\n",
      "Epoch 615: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.5674e-04 - val_loss: 0.0054\n",
      "Epoch 616/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3525e-04\n",
      "Epoch 616: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.3405e-04 - val_loss: 0.0043\n",
      "Epoch 617/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3918e-04\n",
      "Epoch 617: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.6105e-04 - val_loss: 0.0039\n",
      "Epoch 618/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.4587e-04\n",
      "Epoch 618: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.4395e-04 - val_loss: 0.0037\n",
      "Epoch 619/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5948e-04\n",
      "Epoch 619: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.5039e-04 - val_loss: 0.0071\n",
      "Epoch 620/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.5068e-04\n",
      "Epoch 620: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.5226e-04 - val_loss: 0.0038\n",
      "Epoch 621/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.6019e-04\n",
      "Epoch 621: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.5779e-04 - val_loss: 0.0049\n",
      "Epoch 622/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3818e-04\n",
      "Epoch 622: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.2498e-04 - val_loss: 0.0045\n",
      "Epoch 623/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.1714e-04\n",
      "Epoch 623: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.1714e-04 - val_loss: 0.0040\n",
      "Epoch 624/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1950e-04\n",
      "Epoch 624: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.1400e-04 - val_loss: 0.0043\n",
      "Epoch 625/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.7105e-04\n",
      "Epoch 625: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.7105e-04 - val_loss: 0.0054\n",
      "Epoch 626/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2153e-04\n",
      "Epoch 626: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.2153e-04 - val_loss: 0.0037\n",
      "Epoch 627/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0978e-04\n",
      "Epoch 627: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.4008e-04 - val_loss: 0.0037\n",
      "Epoch 628/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.5033e-05\n",
      "Epoch 628: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.2871e-04 - val_loss: 0.0055\n",
      "Epoch 629/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.4271e-04\n",
      "Epoch 629: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.2713e-04 - val_loss: 0.0055\n",
      "Epoch 630/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3918e-04\n",
      "Epoch 630: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2731e-04 - val_loss: 0.0058\n",
      "Epoch 631/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.1911e-04\n",
      "Epoch 631: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.3446e-04 - val_loss: 0.0045\n",
      "Epoch 632/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 1.8777e-04\n",
      "Epoch 632: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.4916e-04 - val_loss: 0.0104\n",
      "Epoch 633/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.4276e-04\n",
      "Epoch 633: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.2132e-04 - val_loss: 0.0092\n",
      "Epoch 634/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.9505e-04\n",
      "Epoch 634: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.9271e-04 - val_loss: 0.0074\n",
      "Epoch 635/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.6807e-04\n",
      "Epoch 635: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.5009e-04 - val_loss: 0.0042\n",
      "Epoch 636/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.8505e-04\n",
      "Epoch 636: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.5988e-04 - val_loss: 0.0058\n",
      "Epoch 637/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.7962e-04\n",
      "Epoch 637: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.6980e-04 - val_loss: 0.0051\n",
      "Epoch 638/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.3840e-04\n",
      "Epoch 638: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.3840e-04 - val_loss: 0.0048\n",
      "Epoch 639/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3658e-04\n",
      "Epoch 639: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.3539e-04 - val_loss: 0.0045\n",
      "Epoch 640/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5914e-04\n",
      "Epoch 640: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.4220e-04 - val_loss: 0.0041\n",
      "Epoch 641/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.4700e-04\n",
      "Epoch 641: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.4700e-04 - val_loss: 0.0063\n",
      "Epoch 642/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.0530e-04\n",
      "Epoch 642: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.0543e-04 - val_loss: 0.0062\n",
      "Epoch 643/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.0913e-04\n",
      "Epoch 643: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.0941e-04 - val_loss: 0.0096\n",
      "Epoch 644/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.9126e-04\n",
      "Epoch 644: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.8929e-04 - val_loss: 0.0064\n",
      "Epoch 645/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.9831e-04\n",
      "Epoch 645: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.9600e-04 - val_loss: 0.0075\n",
      "Epoch 646/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1714e-04\n",
      "Epoch 646: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.4745e-04 - val_loss: 0.0046\n",
      "Epoch 647/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 9.9423e-05\n",
      "Epoch 647: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.4982e-04 - val_loss: 0.0054\n",
      "Epoch 648/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2438e-04\n",
      "Epoch 648: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.2438e-04 - val_loss: 0.0044\n",
      "Epoch 649/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.2208e-05\n",
      "Epoch 649: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1991e-04 - val_loss: 0.0041\n",
      "Epoch 650/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.7967e-04\n",
      "Epoch 650: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.5877e-04 - val_loss: 0.0083\n",
      "Epoch 651/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.7988e-04\n",
      "Epoch 651: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.6118e-04 - val_loss: 0.0062\n",
      "Epoch 652/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3089e-04\n",
      "Epoch 652: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.2920e-04 - val_loss: 0.0046\n",
      "Epoch 653/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.7441e-04\n",
      "Epoch 653: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.9407e-04 - val_loss: 0.0088\n",
      "Epoch 654/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.6839e-04\n",
      "Epoch 654: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.6839e-04 - val_loss: 0.0045\n",
      "Epoch 655/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.6617e-04\n",
      "Epoch 655: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.5620e-04 - val_loss: 0.0045\n",
      "Epoch 656/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5940e-04\n",
      "Epoch 656: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.4275e-04 - val_loss: 0.0052\n",
      "Epoch 657/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.3168e-04\n",
      "Epoch 657: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.3153e-04 - val_loss: 0.0049\n",
      "Epoch 658/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.3555e-04\n",
      "Epoch 658: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.3027e-04 - val_loss: 0.0050\n",
      "Epoch 659/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.3675e-04\n",
      "Epoch 659: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.3150e-04 - val_loss: 0.0042\n",
      "Epoch 660/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1456e-04\n",
      "Epoch 660: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.1396e-04 - val_loss: 0.0042\n",
      "Epoch 661/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5616e-04\n",
      "Epoch 661: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.4779e-04 - val_loss: 0.0041\n",
      "Epoch 662/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.6570e-04\n",
      "Epoch 662: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.5179e-04 - val_loss: 0.0048\n",
      "Epoch 663/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5272e-04\n",
      "Epoch 663: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.4456e-04 - val_loss: 0.0037\n",
      "Epoch 664/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3004e-04\n",
      "Epoch 664: val_loss did not improve from 0.00330\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.2863e-04 - val_loss: 0.0037\n",
      "Epoch 665/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.2523e-04\n",
      "Epoch 665: val_loss improved from 0.00330 to 0.00329, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.1617e-04 - val_loss: 0.0033\n",
      "Epoch 666/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.0633e-04\n",
      "Epoch 666: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.0584e-04 - val_loss: 0.0034\n",
      "Epoch 667/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.2481e-04\n",
      "Epoch 667: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1169e-04 - val_loss: 0.0034\n",
      "Epoch 668/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.5102e-04\n",
      "Epoch 668: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.7372e-04 - val_loss: 0.0044\n",
      "Epoch 669/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.1667e-04\n",
      "Epoch 669: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.0803e-04 - val_loss: 0.0075\n",
      "Epoch 670/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.7697e-04\n",
      "Epoch 670: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.7588e-04 - val_loss: 0.0084\n",
      "Epoch 671/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.5860e-04\n",
      "Epoch 671: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.4318e-04 - val_loss: 0.0071\n",
      "Epoch 672/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.1828e-04\n",
      "Epoch 672: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.8714e-04 - val_loss: 0.0064\n",
      "Epoch 673/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.5023e-04\n",
      "Epoch 673: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.4839e-04 - val_loss: 0.0049\n",
      "Epoch 674/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.3969e-04\n",
      "Epoch 674: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3242e-04 - val_loss: 0.0047\n",
      "Epoch 675/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.2366e-04\n",
      "Epoch 675: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.5002e-04 - val_loss: 0.0045\n",
      "Epoch 676/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 9.9746e-05\n",
      "Epoch 676: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.5927e-04 - val_loss: 0.0048\n",
      "Epoch 677/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5513e-04\n",
      "Epoch 677: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.4264e-04 - val_loss: 0.0050\n",
      "Epoch 678/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.3015e-04\n",
      "Epoch 678: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.2594e-04 - val_loss: 0.0041\n",
      "Epoch 679/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0733e-04\n",
      "Epoch 679: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3558e-04 - val_loss: 0.0047\n",
      "Epoch 680/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.3322e-05\n",
      "Epoch 680: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.1779e-04 - val_loss: 0.0042\n",
      "Epoch 681/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.4993e-04\n",
      "Epoch 681: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.2756e-04 - val_loss: 0.0043\n",
      "Epoch 682/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3746e-04\n",
      "Epoch 682: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3285e-04 - val_loss: 0.0041\n",
      "Epoch 683/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1834e-04\n",
      "Epoch 683: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.1430e-04 - val_loss: 0.0059\n",
      "Epoch 684/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.4274e-04\n",
      "Epoch 684: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.2846e-04 - val_loss: 0.0042\n",
      "Epoch 685/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5242e-04\n",
      "Epoch 685: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.3767e-04 - val_loss: 0.0044\n",
      "Epoch 686/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.2931e-04\n",
      "Epoch 686: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.5541e-04 - val_loss: 0.0054\n",
      "Epoch 687/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.7679e-05\n",
      "Epoch 687: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.2190e-04 - val_loss: 0.0041\n",
      "Epoch 688/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5524e-04\n",
      "Epoch 688: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.4837e-04 - val_loss: 0.0045\n",
      "Epoch 689/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.6192e-05\n",
      "Epoch 689: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3022e-04 - val_loss: 0.0041\n",
      "Epoch 690/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 8.7756e-05\n",
      "Epoch 690: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.5221e-04 - val_loss: 0.0042\n",
      "Epoch 691/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0603e-04\n",
      "Epoch 691: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3399e-04 - val_loss: 0.0047\n",
      "Epoch 692/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1254e-04\n",
      "Epoch 692: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1080e-04 - val_loss: 0.0038\n",
      "Epoch 693/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2340e-04\n",
      "Epoch 693: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 1.2340e-04 - val_loss: 0.0040\n",
      "Epoch 694/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0614e-04\n",
      "Epoch 694: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0614e-04 - val_loss: 0.0038\n",
      "Epoch 695/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.3289e-04\n",
      "Epoch 695: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1352e-04 - val_loss: 0.0038\n",
      "Epoch 696/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1581e-04\n",
      "Epoch 696: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0558e-04 - val_loss: 0.0039\n",
      "Epoch 697/2000\n",
      " 9/16 [===============>..............] - ETA: 0s - loss: 1.5264e-04\n",
      "Epoch 697: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1051e-04 - val_loss: 0.0038\n",
      "Epoch 698/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.6245e-05\n",
      "Epoch 698: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.1851e-04 - val_loss: 0.0041\n",
      "Epoch 699/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 9.6164e-05\n",
      "Epoch 699: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.1584e-04 - val_loss: 0.0037\n",
      "Epoch 700/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0902e-04\n",
      "Epoch 700: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.0156e-04 - val_loss: 0.0035\n",
      "Epoch 701/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.2046e-04\n",
      "Epoch 701: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.1322e-04 - val_loss: 0.0035\n",
      "Epoch 702/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1226e-04\n",
      "Epoch 702: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.1190e-04 - val_loss: 0.0038\n",
      "Epoch 703/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.2434e-04\n",
      "Epoch 703: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.1160e-04 - val_loss: 0.0036\n",
      "Epoch 704/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.3058e-04\n",
      "Epoch 704: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.1047e-04 - val_loss: 0.0037\n",
      "Epoch 705/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.3408e-04\n",
      "Epoch 705: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.3408e-04 - val_loss: 0.0039\n",
      "Epoch 706/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3895e-04\n",
      "Epoch 706: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.2460e-04 - val_loss: 0.0035\n",
      "Epoch 707/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.2861e-04\n",
      "Epoch 707: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1652e-04 - val_loss: 0.0038\n",
      "Epoch 708/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.3604e-04\n",
      "Epoch 708: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.1697e-04 - val_loss: 0.0042\n",
      "Epoch 709/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1024e-04\n",
      "Epoch 709: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0972e-04 - val_loss: 0.0037\n",
      "Epoch 710/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1133e-04\n",
      "Epoch 710: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1024e-04 - val_loss: 0.0040\n",
      "Epoch 711/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 7.7097e-05\n",
      "Epoch 711: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 9.9780e-05 - val_loss: 0.0037\n",
      "Epoch 712/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.2590e-04\n",
      "Epoch 712: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.2008e-04 - val_loss: 0.0046\n",
      "Epoch 713/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2228e-04\n",
      "Epoch 713: val_loss did not improve from 0.00329\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2228e-04 - val_loss: 0.0034\n",
      "Epoch 714/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 9.9935e-05\n",
      "Epoch 714: val_loss improved from 0.00329 to 0.00303, saving model to /home/kiwi/Downloads/畢業靠二姊-20230319T142631Z-001/畢業靠二姊/saved_models/LSTM_trained_model.h5\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 9.8346e-05 - val_loss: 0.0030\n",
      "Epoch 715/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.0882e-04\n",
      "Epoch 715: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2320e-04 - val_loss: 0.0037\n",
      "Epoch 716/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2543e-04\n",
      "Epoch 716: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2439e-04 - val_loss: 0.0045\n",
      "Epoch 717/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3198e-04\n",
      "Epoch 717: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.1957e-04 - val_loss: 0.0037\n",
      "Epoch 718/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5714e-04\n",
      "Epoch 718: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.3943e-04 - val_loss: 0.0049\n",
      "Epoch 719/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.4307e-04\n",
      "Epoch 719: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.2975e-04 - val_loss: 0.0042\n",
      "Epoch 720/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.7695e-04\n",
      "Epoch 720: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.7411e-04 - val_loss: 0.0050\n",
      "Epoch 721/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.0017e-04\n",
      "Epoch 721: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.7921e-04 - val_loss: 0.0037\n",
      "Epoch 722/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.7695e-04\n",
      "Epoch 722: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.7700e-04 - val_loss: 0.0049\n",
      "Epoch 723/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4440e-04\n",
      "Epoch 723: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.3843e-04 - val_loss: 0.0056\n",
      "Epoch 724/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.8174e-04\n",
      "Epoch 724: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.8174e-04 - val_loss: 0.0063\n",
      "Epoch 725/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3577e-04\n",
      "Epoch 725: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.6125e-04 - val_loss: 0.0081\n",
      "Epoch 726/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.7631e-04\n",
      "Epoch 726: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 3.3031e-04 - val_loss: 0.0149\n",
      "Epoch 727/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.1315e-04\n",
      "Epoch 727: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.1315e-04 - val_loss: 0.0073\n",
      "Epoch 728/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.7057e-04\n",
      "Epoch 728: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 2.7164e-04 - val_loss: 0.0181\n",
      "Epoch 729/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.5928e-04\n",
      "Epoch 729: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.5704e-04 - val_loss: 0.0051\n",
      "Epoch 730/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 2.9480e-04\n",
      "Epoch 730: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.7782e-04 - val_loss: 0.0064\n",
      "Epoch 731/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.3157e-04\n",
      "Epoch 731: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.2801e-04 - val_loss: 0.0064\n",
      "Epoch 732/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4287e-04\n",
      "Epoch 732: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3846e-04 - val_loss: 0.0056\n",
      "Epoch 733/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.3654e-04\n",
      "Epoch 733: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3003e-04 - val_loss: 0.0045\n",
      "Epoch 734/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.8019e-04\n",
      "Epoch 734: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.8033e-04 - val_loss: 0.0053\n",
      "Epoch 735/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.8297e-04\n",
      "Epoch 735: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.8083e-04 - val_loss: 0.0044\n",
      "Epoch 736/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.6841e-04\n",
      "Epoch 736: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.5299e-04 - val_loss: 0.0044\n",
      "Epoch 737/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5587e-04\n",
      "Epoch 737: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.4705e-04 - val_loss: 0.0041\n",
      "Epoch 738/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2932e-04\n",
      "Epoch 738: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2764e-04 - val_loss: 0.0045\n",
      "Epoch 739/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1884e-04\n",
      "Epoch 739: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.5304e-04 - val_loss: 0.0044\n",
      "Epoch 740/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2261e-04\n",
      "Epoch 740: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2261e-04 - val_loss: 0.0051\n",
      "Epoch 741/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5500e-04\n",
      "Epoch 741: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.4568e-04 - val_loss: 0.0047\n",
      "Epoch 742/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.6134e-04\n",
      "Epoch 742: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.6134e-04 - val_loss: 0.0048\n",
      "Epoch 743/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.3647e-04\n",
      "Epoch 743: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.3004e-04 - val_loss: 0.0045\n",
      "Epoch 744/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.1895e-04\n",
      "Epoch 744: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.3595e-04 - val_loss: 0.0041\n",
      "Epoch 745/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2628e-04\n",
      "Epoch 745: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.2553e-04 - val_loss: 0.0043\n",
      "Epoch 746/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.1000e-04\n",
      "Epoch 746: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.1000e-04 - val_loss: 0.0039\n",
      "Epoch 747/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.0134e-04\n",
      "Epoch 747: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0019e-04 - val_loss: 0.0039\n",
      "Epoch 748/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3936e-04\n",
      "Epoch 748: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.3770e-04 - val_loss: 0.0042\n",
      "Epoch 749/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3674e-04\n",
      "Epoch 749: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2322e-04 - val_loss: 0.0040\n",
      "Epoch 750/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.1619e-04\n",
      "Epoch 750: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0037e-04 - val_loss: 0.0036\n",
      "Epoch 751/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.2943e-04\n",
      "Epoch 751: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.2198e-04 - val_loss: 0.0039\n",
      "Epoch 752/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0952e-04\n",
      "Epoch 752: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.0405e-04 - val_loss: 0.0039\n",
      "Epoch 753/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.7645e-04\n",
      "Epoch 753: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.5811e-04 - val_loss: 0.0059\n",
      "Epoch 754/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.6500e-04\n",
      "Epoch 754: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.5440e-04 - val_loss: 0.0040\n",
      "Epoch 755/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0865e-04\n",
      "Epoch 755: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.1193e-04 - val_loss: 0.0036\n",
      "Epoch 756/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1828e-04\n",
      "Epoch 756: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.1759e-04 - val_loss: 0.0046\n",
      "Epoch 757/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.2112e-04\n",
      "Epoch 757: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.2514e-04 - val_loss: 0.0040\n",
      "Epoch 758/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.2295e-05\n",
      "Epoch 758: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0387e-04 - val_loss: 0.0039\n",
      "Epoch 759/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2620e-04\n",
      "Epoch 759: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.2440e-04 - val_loss: 0.0040\n",
      "Epoch 760/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.2896e-04\n",
      "Epoch 760: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0994e-04 - val_loss: 0.0044\n",
      "Epoch 761/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 7.5752e-05\n",
      "Epoch 761: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0374e-04 - val_loss: 0.0041\n",
      "Epoch 762/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.2914e-04\n",
      "Epoch 762: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.1569e-04 - val_loss: 0.0040\n",
      "Epoch 763/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1548e-04\n",
      "Epoch 763: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0859e-04 - val_loss: 0.0039\n",
      "Epoch 764/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2735e-04\n",
      "Epoch 764: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2702e-04 - val_loss: 0.0091\n",
      "Epoch 765/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.8620e-04\n",
      "Epoch 765: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.6784e-04 - val_loss: 0.0061\n",
      "Epoch 766/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.3143e-04\n",
      "Epoch 766: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1383e-04 - val_loss: 0.0048\n",
      "Epoch 767/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.0666e-04\n",
      "Epoch 767: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.0607e-04 - val_loss: 0.0042\n",
      "Epoch 768/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.3308e-05\n",
      "Epoch 768: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 9.0011e-05 - val_loss: 0.0053\n",
      "Epoch 769/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3307e-04\n",
      "Epoch 769: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.2253e-04 - val_loss: 0.0050\n",
      "Epoch 770/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 9.1178e-05\n",
      "Epoch 770: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.3904e-04 - val_loss: 0.0050\n",
      "Epoch 771/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1882e-04\n",
      "Epoch 771: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.1368e-04 - val_loss: 0.0055\n",
      "Epoch 772/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.3456e-04\n",
      "Epoch 772: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.1727e-04 - val_loss: 0.0047\n",
      "Epoch 773/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 8.6566e-05\n",
      "Epoch 773: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.1688e-04 - val_loss: 0.0053\n",
      "Epoch 774/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.4905e-04\n",
      "Epoch 774: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.4913e-04 - val_loss: 0.0100\n",
      "Epoch 775/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5012e-04\n",
      "Epoch 775: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.4123e-04 - val_loss: 0.0043\n",
      "Epoch 776/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2366e-04\n",
      "Epoch 776: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.2366e-04 - val_loss: 0.0081\n",
      "Epoch 777/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3662e-04\n",
      "Epoch 777: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.3586e-04 - val_loss: 0.0050\n",
      "Epoch 778/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1074e-04\n",
      "Epoch 778: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2379e-04 - val_loss: 0.0053\n",
      "Epoch 779/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1026e-04\n",
      "Epoch 779: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.0971e-04 - val_loss: 0.0049\n",
      "Epoch 780/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 6.1807e-05\n",
      "Epoch 780: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.1763e-05 - val_loss: 0.0044\n",
      "Epoch 781/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.4872e-05\n",
      "Epoch 781: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 7.4872e-05 - val_loss: 0.0035\n",
      "Epoch 782/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.0446e-04\n",
      "Epoch 782: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.0464e-04 - val_loss: 0.0043\n",
      "Epoch 783/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.2530e-04\n",
      "Epoch 783: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0676e-04 - val_loss: 0.0045\n",
      "Epoch 784/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 7.8611e-05\n",
      "Epoch 784: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0738e-04 - val_loss: 0.0039\n",
      "Epoch 785/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 8.9606e-05\n",
      "Epoch 785: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 8.5035e-05 - val_loss: 0.0046\n",
      "Epoch 786/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1015e-04\n",
      "Epoch 786: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0586e-04 - val_loss: 0.0046\n",
      "Epoch 787/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 7.7693e-05\n",
      "Epoch 787: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 7.7668e-05 - val_loss: 0.0040\n",
      "Epoch 788/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.1098e-05\n",
      "Epoch 788: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.0556e-05 - val_loss: 0.0045\n",
      "Epoch 789/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 8.9295e-05\n",
      "Epoch 789: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 8.7638e-05 - val_loss: 0.0041\n",
      "Epoch 790/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 8.8368e-05\n",
      "Epoch 790: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 8.6169e-05 - val_loss: 0.0045\n",
      "Epoch 791/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 6.7724e-05\n",
      "Epoch 791: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 9.6815e-05 - val_loss: 0.0043\n",
      "Epoch 792/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 6.5099e-05\n",
      "Epoch 792: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 6.7846e-05 - val_loss: 0.0036\n",
      "Epoch 793/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.2987e-05\n",
      "Epoch 793: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 8.2913e-05 - val_loss: 0.0040\n",
      "Epoch 794/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1941e-04\n",
      "Epoch 794: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.1600e-04 - val_loss: 0.0074\n",
      "Epoch 795/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.8418e-04\n",
      "Epoch 795: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.5649e-04 - val_loss: 0.0051\n",
      "Epoch 796/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.5528e-04\n",
      "Epoch 796: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.5448e-04 - val_loss: 0.0048\n",
      "Epoch 797/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.4851e-05\n",
      "Epoch 797: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.3500e-05 - val_loss: 0.0044\n",
      "Epoch 798/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.2374e-04\n",
      "Epoch 798: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.1332e-04 - val_loss: 0.0055\n",
      "Epoch 799/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 9.1829e-05\n",
      "Epoch 799: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 9.8051e-05 - val_loss: 0.0043\n",
      "Epoch 800/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1745e-04\n",
      "Epoch 800: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0981e-04 - val_loss: 0.0043\n",
      "Epoch 801/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4384e-04\n",
      "Epoch 801: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.5079e-04 - val_loss: 0.0040\n",
      "Epoch 802/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0208e-04\n",
      "Epoch 802: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.1980e-04 - val_loss: 0.0058\n",
      "Epoch 803/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.4030e-04\n",
      "Epoch 803: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2538e-04 - val_loss: 0.0044\n",
      "Epoch 804/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.0450e-04\n",
      "Epoch 804: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.6611e-04 - val_loss: 0.0051\n",
      "Epoch 805/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.2069e-04\n",
      "Epoch 805: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.1669e-04 - val_loss: 0.0054\n",
      "Epoch 806/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0308e-04\n",
      "Epoch 806: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 9.9354e-05 - val_loss: 0.0048\n",
      "Epoch 807/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.4392e-04\n",
      "Epoch 807: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.4302e-04 - val_loss: 0.0055\n",
      "Epoch 808/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 1.1147e-04\n",
      "Epoch 808: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.2058e-05 - val_loss: 0.0043\n",
      "Epoch 809/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 9.1713e-05\n",
      "Epoch 809: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 9.9076e-05 - val_loss: 0.0044\n",
      "Epoch 810/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.2274e-05\n",
      "Epoch 810: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.4500e-05 - val_loss: 0.0041\n",
      "Epoch 811/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 7.2503e-05\n",
      "Epoch 811: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 6.8447e-05 - val_loss: 0.0039\n",
      "Epoch 812/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.0334e-04\n",
      "Epoch 812: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0275e-04 - val_loss: 0.0043\n",
      "Epoch 813/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.3598e-05\n",
      "Epoch 813: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 7.3598e-05 - val_loss: 0.0038\n",
      "Epoch 814/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0829e-04\n",
      "Epoch 814: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 9.9322e-05 - val_loss: 0.0049\n",
      "Epoch 815/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0761e-04\n",
      "Epoch 815: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 9.7940e-05 - val_loss: 0.0045\n",
      "Epoch 816/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 8.3801e-05\n",
      "Epoch 816: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 7.2616e-05 - val_loss: 0.0039\n",
      "Epoch 817/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.0062e-04\n",
      "Epoch 817: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.7665e-05 - val_loss: 0.0043\n",
      "Epoch 818/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.1236e-05\n",
      "Epoch 818: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.1236e-05 - val_loss: 0.0043\n",
      "Epoch 819/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.3394e-05\n",
      "Epoch 819: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.7334e-05 - val_loss: 0.0066\n",
      "Epoch 820/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0515e-04\n",
      "Epoch 820: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 9.9339e-05 - val_loss: 0.0041\n",
      "Epoch 821/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.6351e-04\n",
      "Epoch 821: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.6152e-04 - val_loss: 0.0050\n",
      "Epoch 822/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 9.0709e-05\n",
      "Epoch 822: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 7.9745e-05 - val_loss: 0.0037\n",
      "Epoch 823/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.5188e-05\n",
      "Epoch 823: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.0969e-04 - val_loss: 0.0042\n",
      "Epoch 824/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 9.6926e-05\n",
      "Epoch 824: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 9.2442e-05 - val_loss: 0.0041\n",
      "Epoch 825/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 9.2663e-05\n",
      "Epoch 825: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 8.1286e-05 - val_loss: 0.0045\n",
      "Epoch 826/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.2223e-04\n",
      "Epoch 826: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0101e-04 - val_loss: 0.0036\n",
      "Epoch 827/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.3891e-05\n",
      "Epoch 827: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.3371e-05 - val_loss: 0.0048\n",
      "Epoch 828/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 9.1653e-05\n",
      "Epoch 828: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 8.7203e-05 - val_loss: 0.0042\n",
      "Epoch 829/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.0204e-04\n",
      "Epoch 829: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.7611e-05 - val_loss: 0.0038\n",
      "Epoch 830/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 9.0445e-05\n",
      "Epoch 830: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.9299e-05 - val_loss: 0.0042\n",
      "Epoch 831/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 8.1237e-05\n",
      "Epoch 831: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 7.3679e-05 - val_loss: 0.0039\n",
      "Epoch 832/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 8.3717e-05\n",
      "Epoch 832: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.8060e-05 - val_loss: 0.0036\n",
      "Epoch 833/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0190e-04\n",
      "Epoch 833: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.5913e-05 - val_loss: 0.0035\n",
      "Epoch 834/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.7871e-05\n",
      "Epoch 834: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.6748e-05 - val_loss: 0.0042\n",
      "Epoch 835/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.8084e-04\n",
      "Epoch 835: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.7407e-04 - val_loss: 0.0061\n",
      "Epoch 836/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.1659e-04\n",
      "Epoch 836: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.8742e-04 - val_loss: 0.0054\n",
      "Epoch 837/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.0074e-04\n",
      "Epoch 837: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.7218e-04 - val_loss: 0.0052\n",
      "Epoch 838/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0917e-04\n",
      "Epoch 838: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.2273e-04 - val_loss: 0.0058\n",
      "Epoch 839/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3585e-04\n",
      "Epoch 839: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.2340e-04 - val_loss: 0.0055\n",
      "Epoch 840/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.4297e-05\n",
      "Epoch 840: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1715e-04 - val_loss: 0.0040\n",
      "Epoch 841/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.2187e-04\n",
      "Epoch 841: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0306e-04 - val_loss: 0.0038\n",
      "Epoch 842/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.4838e-04\n",
      "Epoch 842: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.4676e-04 - val_loss: 0.0044\n",
      "Epoch 843/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3417e-04\n",
      "Epoch 843: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2495e-04 - val_loss: 0.0053\n",
      "Epoch 844/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.4671e-04\n",
      "Epoch 844: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3393e-04 - val_loss: 0.0053\n",
      "Epoch 845/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.0698e-04\n",
      "Epoch 845: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.8291e-04 - val_loss: 0.0047\n",
      "Epoch 846/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.2136e-04\n",
      "Epoch 846: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.1078e-04 - val_loss: 0.0050\n",
      "Epoch 847/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1029e-04\n",
      "Epoch 847: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0928e-04 - val_loss: 0.0063\n",
      "Epoch 848/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3606e-04\n",
      "Epoch 848: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3472e-04 - val_loss: 0.0055\n",
      "Epoch 849/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0447e-04\n",
      "Epoch 849: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0441e-04 - val_loss: 0.0045\n",
      "Epoch 850/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.2421e-05\n",
      "Epoch 850: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 8.2421e-05 - val_loss: 0.0041\n",
      "Epoch 851/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.1844e-04\n",
      "Epoch 851: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.0084e-04 - val_loss: 0.0043\n",
      "Epoch 852/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.2774e-05\n",
      "Epoch 852: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 8.1939e-05 - val_loss: 0.0040\n",
      "Epoch 853/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.2223e-04\n",
      "Epoch 853: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.2023e-04 - val_loss: 0.0050\n",
      "Epoch 854/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.0306e-04\n",
      "Epoch 854: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.8954e-05 - val_loss: 0.0049\n",
      "Epoch 855/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1624e-04\n",
      "Epoch 855: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.1080e-04 - val_loss: 0.0042\n",
      "Epoch 856/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.4255e-04\n",
      "Epoch 856: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.2593e-04 - val_loss: 0.0047\n",
      "Epoch 857/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 4.1614e-05\n",
      "Epoch 857: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 7.3123e-05 - val_loss: 0.0039\n",
      "Epoch 858/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.0309e-04\n",
      "Epoch 858: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 8.7567e-05 - val_loss: 0.0038\n",
      "Epoch 859/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.3624e-05\n",
      "Epoch 859: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 7.4356e-05 - val_loss: 0.0038\n",
      "Epoch 860/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 6.3378e-05\n",
      "Epoch 860: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.3933e-05 - val_loss: 0.0037\n",
      "Epoch 861/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1043e-04\n",
      "Epoch 861: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0060e-04 - val_loss: 0.0051\n",
      "Epoch 862/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 6.5541e-05\n",
      "Epoch 862: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 9.5705e-05 - val_loss: 0.0059\n",
      "Epoch 863/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.2905e-04\n",
      "Epoch 863: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.1186e-04 - val_loss: 0.0043\n",
      "Epoch 864/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.7378e-05\n",
      "Epoch 864: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.2780e-05 - val_loss: 0.0041\n",
      "Epoch 865/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.1938e-05\n",
      "Epoch 865: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 8.5943e-05 - val_loss: 0.0042\n",
      "Epoch 866/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.7732e-05\n",
      "Epoch 866: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 8.8581e-05 - val_loss: 0.0046\n",
      "Epoch 867/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1217e-04\n",
      "Epoch 867: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0236e-04 - val_loss: 0.0046\n",
      "Epoch 868/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 9.1840e-05\n",
      "Epoch 868: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.1896e-05 - val_loss: 0.0046\n",
      "Epoch 869/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 8.1981e-05\n",
      "Epoch 869: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0275e-04 - val_loss: 0.0046\n",
      "Epoch 870/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 7.1256e-05\n",
      "Epoch 870: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.8107e-05 - val_loss: 0.0050\n",
      "Epoch 871/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.7352e-05\n",
      "Epoch 871: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.7842e-05 - val_loss: 0.0049\n",
      "Epoch 872/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 8.5460e-05\n",
      "Epoch 872: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 7.3246e-05 - val_loss: 0.0045\n",
      "Epoch 873/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4196e-04\n",
      "Epoch 873: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3393e-04 - val_loss: 0.0045\n",
      "Epoch 874/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.4529e-04\n",
      "Epoch 874: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2435e-04 - val_loss: 0.0047\n",
      "Epoch 875/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.1394e-04\n",
      "Epoch 875: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.8123e-05 - val_loss: 0.0039\n",
      "Epoch 876/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.6929e-05\n",
      "Epoch 876: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 8.9455e-05 - val_loss: 0.0043\n",
      "Epoch 877/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.0206e-05\n",
      "Epoch 877: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 9.5982e-05 - val_loss: 0.0043\n",
      "Epoch 878/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1096e-04\n",
      "Epoch 878: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0862e-04 - val_loss: 0.0053\n",
      "Epoch 879/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 4.6212e-05\n",
      "Epoch 879: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 7.6196e-05 - val_loss: 0.0043\n",
      "Epoch 880/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 9.9913e-05\n",
      "Epoch 880: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.8614e-05 - val_loss: 0.0038\n",
      "Epoch 881/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 3.0951e-05\n",
      "Epoch 881: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.3317e-05 - val_loss: 0.0038\n",
      "Epoch 882/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 6.8868e-05\n",
      "Epoch 882: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 6.5525e-05 - val_loss: 0.0044\n",
      "Epoch 883/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 4.9584e-05\n",
      "Epoch 883: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.4361e-05 - val_loss: 0.0044\n",
      "Epoch 884/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 7.6186e-05\n",
      "Epoch 884: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.2559e-05 - val_loss: 0.0047\n",
      "Epoch 885/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.2238e-05\n",
      "Epoch 885: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.2238e-05 - val_loss: 0.0044\n",
      "Epoch 886/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 6.9728e-05\n",
      "Epoch 886: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 6.2808e-05 - val_loss: 0.0039\n",
      "Epoch 887/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.1032e-04\n",
      "Epoch 887: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.6236e-05 - val_loss: 0.0039\n",
      "Epoch 888/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 7.7395e-05\n",
      "Epoch 888: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.5162e-05 - val_loss: 0.0045\n",
      "Epoch 889/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 9.3939e-05\n",
      "Epoch 889: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 9.0502e-05 - val_loss: 0.0041\n",
      "Epoch 890/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 9.8073e-05\n",
      "Epoch 890: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 9.6496e-05 - val_loss: 0.0036\n",
      "Epoch 891/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 7.3028e-05\n",
      "Epoch 891: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 7.1385e-05 - val_loss: 0.0039\n",
      "Epoch 892/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.8268e-05\n",
      "Epoch 892: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 8.7671e-05 - val_loss: 0.0038\n",
      "Epoch 893/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.0042e-05\n",
      "Epoch 893: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.7321e-05 - val_loss: 0.0069\n",
      "Epoch 894/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0631e-04\n",
      "Epoch 894: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0631e-04 - val_loss: 0.0066\n",
      "Epoch 895/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2079e-04\n",
      "Epoch 895: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.2079e-04 - val_loss: 0.0063\n",
      "Epoch 896/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0595e-04\n",
      "Epoch 896: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.0595e-04 - val_loss: 0.0047\n",
      "Epoch 897/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.6787e-05\n",
      "Epoch 897: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 9.1665e-05 - val_loss: 0.0073\n",
      "Epoch 898/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.3009e-04\n",
      "Epoch 898: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.3009e-04 - val_loss: 0.0081\n",
      "Epoch 899/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.5185e-04\n",
      "Epoch 899: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.4503e-04 - val_loss: 0.0066\n",
      "Epoch 900/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.5929e-04\n",
      "Epoch 900: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.4990e-04 - val_loss: 0.0112\n",
      "Epoch 901/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.1429e-04\n",
      "Epoch 901: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0804e-04 - val_loss: 0.0039\n",
      "Epoch 902/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 1.1281e-04\n",
      "Epoch 902: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.6610e-04 - val_loss: 0.0053\n",
      "Epoch 903/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 9.9636e-05\n",
      "Epoch 903: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 9.7216e-05 - val_loss: 0.0065\n",
      "Epoch 904/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.0622e-04\n",
      "Epoch 904: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.4343e-05 - val_loss: 0.0043\n",
      "Epoch 905/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.9909e-05\n",
      "Epoch 905: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 8.8460e-05 - val_loss: 0.0038\n",
      "Epoch 906/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 6.6400e-05\n",
      "Epoch 906: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.5352e-05 - val_loss: 0.0037\n",
      "Epoch 907/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.3080e-04\n",
      "Epoch 907: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.1041e-04 - val_loss: 0.0037\n",
      "Epoch 908/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 7.2923e-05\n",
      "Epoch 908: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 7.1634e-05 - val_loss: 0.0036\n",
      "Epoch 909/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1753e-04\n",
      "Epoch 909: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0704e-04 - val_loss: 0.0040\n",
      "Epoch 910/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.6115e-05\n",
      "Epoch 910: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 7.4535e-05 - val_loss: 0.0037\n",
      "Epoch 911/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.0241e-05\n",
      "Epoch 911: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 7.5270e-05 - val_loss: 0.0039\n",
      "Epoch 912/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 4.7406e-05\n",
      "Epoch 912: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.2626e-04 - val_loss: 0.0254\n",
      "Epoch 913/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.7137e-04\n",
      "Epoch 913: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.3440e-04 - val_loss: 0.0069\n",
      "Epoch 914/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.2686e-04\n",
      "Epoch 914: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.2235e-04 - val_loss: 0.0061\n",
      "Epoch 915/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.3262e-05\n",
      "Epoch 915: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 8.8267e-05 - val_loss: 0.0045\n",
      "Epoch 916/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.4034e-04\n",
      "Epoch 916: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2811e-04 - val_loss: 0.0046\n",
      "Epoch 917/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 8.6425e-05\n",
      "Epoch 917: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2740e-04 - val_loss: 0.0091\n",
      "Epoch 918/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.8534e-04\n",
      "Epoch 918: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.1873e-04 - val_loss: 0.0053\n",
      "Epoch 919/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.3630e-04\n",
      "Epoch 919: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.3710e-04 - val_loss: 0.0144\n",
      "Epoch 920/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 2.4986e-04\n",
      "Epoch 920: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.2181e-04 - val_loss: 0.0093\n",
      "Epoch 921/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.1549e-04\n",
      "Epoch 921: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.1378e-04 - val_loss: 0.0078\n",
      "Epoch 922/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0166e-04\n",
      "Epoch 922: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.0166e-04 - val_loss: 0.0059\n",
      "Epoch 923/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.5256e-05\n",
      "Epoch 923: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 8.5352e-05 - val_loss: 0.0051\n",
      "Epoch 924/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 8.3436e-05\n",
      "Epoch 924: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 8.0949e-05 - val_loss: 0.0047\n",
      "Epoch 925/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.0888e-04\n",
      "Epoch 925: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.6543e-05 - val_loss: 0.0044\n",
      "Epoch 926/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.0283e-05\n",
      "Epoch 926: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.3475e-05 - val_loss: 0.0045\n",
      "Epoch 927/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0778e-04\n",
      "Epoch 927: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0022e-04 - val_loss: 0.0043\n",
      "Epoch 928/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 7.5177e-05\n",
      "Epoch 928: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.5543e-05 - val_loss: 0.0042\n",
      "Epoch 929/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0748e-04\n",
      "Epoch 929: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0286e-04 - val_loss: 0.0051\n",
      "Epoch 930/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.2743e-04\n",
      "Epoch 930: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.2654e-04 - val_loss: 0.0082\n",
      "Epoch 931/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.3455e-04\n",
      "Epoch 931: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2258e-04 - val_loss: 0.0048\n",
      "Epoch 932/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 3.3697e-05\n",
      "Epoch 932: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.4192e-05 - val_loss: 0.0041\n",
      "Epoch 933/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.2452e-04\n",
      "Epoch 933: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.1905e-04 - val_loss: 0.0051\n",
      "Epoch 934/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.1570e-04\n",
      "Epoch 934: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.0235e-04 - val_loss: 0.0045\n",
      "Epoch 935/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.2415e-04\n",
      "Epoch 935: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0416e-04 - val_loss: 0.0043\n",
      "Epoch 936/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.8860e-05\n",
      "Epoch 936: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 6.8823e-05 - val_loss: 0.0041\n",
      "Epoch 937/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 6.8077e-05\n",
      "Epoch 937: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.1990e-05 - val_loss: 0.0040\n",
      "Epoch 938/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2336e-04\n",
      "Epoch 938: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.2336e-04 - val_loss: 0.0048\n",
      "Epoch 939/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.1965e-05\n",
      "Epoch 939: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 8.7101e-05 - val_loss: 0.0054\n",
      "Epoch 940/2000\n",
      "10/16 [=================>............] - ETA: 0s - loss: 4.0838e-05\n",
      "Epoch 940: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.9298e-05 - val_loss: 0.0042\n",
      "Epoch 941/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1914e-04\n",
      "Epoch 941: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.1454e-04 - val_loss: 0.0064\n",
      "Epoch 942/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1454e-04\n",
      "Epoch 942: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.1270e-04 - val_loss: 0.0044\n",
      "Epoch 943/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 6.7518e-05\n",
      "Epoch 943: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 6.6437e-05 - val_loss: 0.0039\n",
      "Epoch 944/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.0227e-05\n",
      "Epoch 944: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 6.3473e-05 - val_loss: 0.0038\n",
      "Epoch 945/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 3.0797e-05\n",
      "Epoch 945: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.0785e-05 - val_loss: 0.0043\n",
      "Epoch 946/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.3557e-05\n",
      "Epoch 946: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 8.2219e-05 - val_loss: 0.0046\n",
      "Epoch 947/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.7504e-05\n",
      "Epoch 947: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 7.1636e-05 - val_loss: 0.0040\n",
      "Epoch 948/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.5266e-05\n",
      "Epoch 948: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.5266e-05 - val_loss: 0.0044\n",
      "Epoch 949/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 7.5616e-05\n",
      "Epoch 949: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 7.5147e-05 - val_loss: 0.0042\n",
      "Epoch 950/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.0644e-05\n",
      "Epoch 950: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.4668e-05 - val_loss: 0.0038\n",
      "Epoch 951/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 7.7465e-05\n",
      "Epoch 951: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 6.8878e-05 - val_loss: 0.0043\n",
      "Epoch 952/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.9286e-05\n",
      "Epoch 952: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 7.9286e-05 - val_loss: 0.0043\n",
      "Epoch 953/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0012e-04\n",
      "Epoch 953: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 9.4760e-05 - val_loss: 0.0045\n",
      "Epoch 954/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 6.1940e-05\n",
      "Epoch 954: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 9.3507e-05 - val_loss: 0.0052\n",
      "Epoch 955/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 8.3652e-05\n",
      "Epoch 955: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.5394e-05 - val_loss: 0.0041\n",
      "Epoch 956/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 8.4918e-05\n",
      "Epoch 956: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 9.7326e-05 - val_loss: 0.0054\n",
      "Epoch 957/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.0494e-04\n",
      "Epoch 957: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 8.9107e-05 - val_loss: 0.0046\n",
      "Epoch 958/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 6.5855e-05\n",
      "Epoch 958: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 6.4804e-05 - val_loss: 0.0038\n",
      "Epoch 959/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0277e-04\n",
      "Epoch 959: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0180e-04 - val_loss: 0.0049\n",
      "Epoch 960/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 8.6102e-05\n",
      "Epoch 960: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 7.0475e-05 - val_loss: 0.0035\n",
      "Epoch 961/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.8776e-05\n",
      "Epoch 961: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.8776e-05 - val_loss: 0.0033\n",
      "Epoch 962/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.4006e-04\n",
      "Epoch 962: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.3256e-04 - val_loss: 0.0053\n",
      "Epoch 963/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0907e-04\n",
      "Epoch 963: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0496e-04 - val_loss: 0.0058\n",
      "Epoch 964/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 9.5105e-05\n",
      "Epoch 964: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 8.8772e-05 - val_loss: 0.0069\n",
      "Epoch 965/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.1814e-04\n",
      "Epoch 965: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.1692e-04 - val_loss: 0.0044\n",
      "Epoch 966/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.1065e-04\n",
      "Epoch 966: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0158e-04 - val_loss: 0.0049\n",
      "Epoch 967/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 7.5973e-05\n",
      "Epoch 967: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 7.5436e-05 - val_loss: 0.0047\n",
      "Epoch 968/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 8.2285e-05\n",
      "Epoch 968: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.3338e-05 - val_loss: 0.0046\n",
      "Epoch 969/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 8.3961e-05\n",
      "Epoch 969: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 9.4698e-05 - val_loss: 0.0048\n",
      "Epoch 970/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 2.0704e-04\n",
      "Epoch 970: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.8472e-04 - val_loss: 0.0054\n",
      "Epoch 971/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.1305e-04\n",
      "Epoch 971: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.2555e-04 - val_loss: 0.0037\n",
      "Epoch 972/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 5.6033e-05\n",
      "Epoch 972: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.9978e-05 - val_loss: 0.0055\n",
      "Epoch 973/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.3457e-04\n",
      "Epoch 973: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1595e-04 - val_loss: 0.0049\n",
      "Epoch 974/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 6.7580e-05\n",
      "Epoch 974: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0838e-04 - val_loss: 0.0048\n",
      "Epoch 975/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 9.8493e-05\n",
      "Epoch 975: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 9.6895e-05 - val_loss: 0.0045\n",
      "Epoch 976/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0952e-04\n",
      "Epoch 976: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0354e-04 - val_loss: 0.0045\n",
      "Epoch 977/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 7.7894e-05\n",
      "Epoch 977: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.6319e-05 - val_loss: 0.0038\n",
      "Epoch 978/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.2071e-04\n",
      "Epoch 978: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0933e-04 - val_loss: 0.0066\n",
      "Epoch 979/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 1.0601e-04\n",
      "Epoch 979: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 8.8776e-05 - val_loss: 0.0040\n",
      "Epoch 980/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0011e-04\n",
      "Epoch 980: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 9.9848e-05 - val_loss: 0.0044\n",
      "Epoch 981/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.7749e-04\n",
      "Epoch 981: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.6377e-04 - val_loss: 0.0059\n",
      "Epoch 982/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.8612e-04\n",
      "Epoch 982: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.0929e-04 - val_loss: 0.0363\n",
      "Epoch 983/2000\n",
      "11/16 [===================>..........] - ETA: 0s - loss: 2.9522e-04\n",
      "Epoch 983: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.3284e-04 - val_loss: 0.0065\n",
      "Epoch 984/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.9088e-04\n",
      "Epoch 984: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.8719e-04 - val_loss: 0.0094\n",
      "Epoch 985/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.0930e-04\n",
      "Epoch 985: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3281e-04 - val_loss: 0.0048\n",
      "Epoch 986/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.0361e-04\n",
      "Epoch 986: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.3607e-04 - val_loss: 0.0087\n",
      "Epoch 987/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 1.3132e-04\n",
      "Epoch 987: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2489e-04 - val_loss: 0.0049\n",
      "Epoch 988/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.8515e-05\n",
      "Epoch 988: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.0270e-04 - val_loss: 0.0049\n",
      "Epoch 989/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.5790e-05\n",
      "Epoch 989: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 9.5790e-05 - val_loss: 0.0046\n",
      "Epoch 990/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.9332e-05\n",
      "Epoch 990: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 8.0700e-05 - val_loss: 0.0043\n",
      "Epoch 991/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.4963e-05\n",
      "Epoch 991: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 7.4963e-05 - val_loss: 0.0041\n",
      "Epoch 992/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 6.9763e-05\n",
      "Epoch 992: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 6.2513e-05 - val_loss: 0.0042\n",
      "Epoch 993/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5445e-05\n",
      "Epoch 993: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.5445e-05 - val_loss: 0.0041\n",
      "Epoch 994/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 7.5935e-05\n",
      "Epoch 994: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 7.1605e-05 - val_loss: 0.0042\n",
      "Epoch 995/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.0314e-04\n",
      "Epoch 995: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 9.2334e-05 - val_loss: 0.0041\n",
      "Epoch 996/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 7.3729e-05\n",
      "Epoch 996: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.3167e-05 - val_loss: 0.0040\n",
      "Epoch 997/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 6.4349e-05\n",
      "Epoch 997: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 6.1404e-05 - val_loss: 0.0038\n",
      "Epoch 998/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.3044e-04\n",
      "Epoch 998: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2939e-04 - val_loss: 0.0039\n",
      "Epoch 999/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.3169e-05\n",
      "Epoch 999: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 8.2370e-05 - val_loss: 0.0044\n",
      "Epoch 1000/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.7007e-05\n",
      "Epoch 1000: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 7.7341e-05 - val_loss: 0.0040\n",
      "Epoch 1001/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 8.7916e-05\n",
      "Epoch 1001: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 8.0023e-05 - val_loss: 0.0045\n",
      "Epoch 1002/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 9.1304e-05\n",
      "Epoch 1002: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 8.2599e-05 - val_loss: 0.0048\n",
      "Epoch 1003/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 7.8464e-05\n",
      "Epoch 1003: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.9277e-05 - val_loss: 0.0048\n",
      "Epoch 1004/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 7.6752e-05\n",
      "Epoch 1004: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 7.5622e-05 - val_loss: 0.0045\n",
      "Epoch 1005/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 8.0000e-05\n",
      "Epoch 1005: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 7.9729e-05 - val_loss: 0.0054\n",
      "Epoch 1006/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 7.6305e-05\n",
      "Epoch 1006: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 7.6253e-05 - val_loss: 0.0047\n",
      "Epoch 1007/2000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 1.7448e-04\n",
      "Epoch 1007: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.4333e-04 - val_loss: 0.0043\n",
      "Epoch 1008/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.6957e-05\n",
      "Epoch 1008: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 9.6957e-05 - val_loss: 0.0049\n",
      "Epoch 1009/2000\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.9640e-05\n",
      "Epoch 1009: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.9640e-05 - val_loss: 0.0043\n",
      "Epoch 1010/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 8.1273e-05\n",
      "Epoch 1010: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 7.4547e-05 - val_loss: 0.0044\n",
      "Epoch 1011/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 7.0224e-05\n",
      "Epoch 1011: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 6.9470e-05 - val_loss: 0.0042\n",
      "Epoch 1012/2000\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 7.0561e-05\n",
      "Epoch 1012: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 7.0161e-05 - val_loss: 0.0045\n",
      "Epoch 1013/2000\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 8.1228e-05\n",
      "Epoch 1013: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.7566e-05 - val_loss: 0.0044\n",
      "Epoch 1014/2000\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 7.2089e-05\n",
      "Epoch 1014: val_loss did not improve from 0.00303\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 6.4996e-05 - val_loss: 0.0041\n",
      "Epoch 1014: early stopping\n",
      "Loading trained model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 2000\n",
    "epochs = 2000\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'LSTM_trained_model.h5'\n",
    "\n",
    "# Use ModelCheckpoint to save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# earlystop\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=300, verbose=1)\n",
    "\n",
    "\n",
    "cw = {0:0.01 , 1:1}\n",
    "history = model.fit(x_train, y_train, epochs = epochs, batch_size = batch_size, validation_data= (x_test, y_test),\n",
    "                    class_weight=cw,callbacks=[earlystop, checkpoint])\n",
    "\n",
    "\n",
    "# loading our save model\n",
    "\n",
    "print(\"Loading trained model\")\n",
    "model = load_model(model_path)\n",
    "\n",
    "# score, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954/954 [==============================] - 3s 3ms/step\n",
      "精準度指標：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     30250\n",
      "           1       0.82      0.99      0.90       250\n",
      "\n",
      "    accuracy                           1.00     30500\n",
      "   macro avg       0.91      1.00      0.95     30500\n",
      "weighted avg       1.00      1.00      1.00     30500\n",
      "\n",
      "混淆矩陣：\n",
      "[[30194    56]\n",
      " [    2   248]]\n"
     ]
    }
   ],
   "source": [
    "#在訓練集訓練結果\n",
    "import math\n",
    "y_train_predict_0=model.predict(x_train)\n",
    "y_train_predict_1=y_train_predict_0[:,0]\n",
    "y_train_predict_2=[np.round(i) for i in y_train_predict_1] \n",
    "y_train_predict_3=np.array(y_train_predict_2)\n",
    "from sklearn import metrics\n",
    "print(\"精準度指標：\")\n",
    "print(metrics.classification_report(y_train,y_train_predict_3))\n",
    "print(\"混淆矩陣：\")\n",
    "print(metrics.confusion_matrix(y_train,y_train_predict_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_predict_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 1s 2ms/step\n",
      "精準度指標：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7554\n",
      "           1       0.82      0.86      0.84        70\n",
      "\n",
      "    accuracy                           1.00      7624\n",
      "   macro avg       0.91      0.93      0.92      7624\n",
      "weighted avg       1.00      1.00      1.00      7624\n",
      "\n",
      "混淆矩陣：\n",
      "[[7541   13]\n",
      " [  10   60]]\n"
     ]
    }
   ],
   "source": [
    "#測試集結果\n",
    "y_test_predict=model.predict(x_test)\n",
    "y_test_predict=y_test_predict[:,0]\n",
    "y_test_predict=[np.round(i) for i in y_test_predict]\n",
    "y_test_predict=np.array(y_test_predict)\n",
    "from sklearn import metrics\n",
    "print(\"精準度指標：\")\n",
    "print(metrics.classification_report(y_test,y_test_predict))\n",
    "print(\"混淆矩陣：\")\n",
    "print(metrics.confusion_matrix(y_test,y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821917808219178\n",
      "0.8571428571428571\n",
      "0.8391608391608392\n",
      "0.9277109572979312\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "precision = precision_score(y_test,y_test_predict)\n",
    "recall = recall_score(y_test,y_test_predict)\n",
    "f1 = f1_score(y_test,y_test_predict)\n",
    "roc_auc_score = roc_auc_score(y_test,y_test_predict)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n",
    "print(roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541/541 [==============================] - 1s 1ms/step\n",
      "精準度指標：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17236\n",
      "           1       0.38      0.73      0.50        67\n",
      "\n",
      "    accuracy                           0.99     17303\n",
      "   macro avg       0.69      0.86      0.75     17303\n",
      "weighted avg       1.00      0.99      1.00     17303\n",
      "\n",
      "混淆矩陣：\n",
      "[[17155    81]\n",
      " [   18    49]]\n"
     ]
    }
   ],
   "source": [
    "#驗證集結果\n",
    "Y_va_predict=model.predict(X_va)\n",
    "Y_va_predict=Y_va_predict[:,0]\n",
    "Y_va_predict=[np.round(i) for i in Y_va_predict]\n",
    "Y_va_predict=np.array(Y_va_predict\n",
    "                     )\n",
    "from sklearn import metrics\n",
    "print(\"精準度指標：\")\n",
    "print(metrics.classification_report(Y_va,Y_va_predict))\n",
    "print(\"混淆矩陣：\")\n",
    "print(metrics.confusion_matrix(Y_va,Y_va_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9942784488239034\n",
      "0.3769230769230769\n",
      "0.7313432835820896\n",
      "0.4974619289340101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "accuracy = accuracy_score(Y_va,Y_va_predict)\n",
    "precision = precision_score(Y_va,Y_va_predict)\n",
    "recall = recall_score(Y_va,Y_va_predict)\n",
    "f1 = f1_score(Y_va,Y_va_predict)\n",
    "roc_auc_score = roc_auc_score(Y_va,Y_va_predict)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541/541 [==============================] - 1s 2ms/step\n",
      "精準度指標：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17236\n",
      "           1       0.00      0.00      0.00        67\n",
      "\n",
      "    accuracy                           1.00     17303\n",
      "   macro avg       0.50      0.50      0.50     17303\n",
      "weighted avg       0.99      1.00      0.99     17303\n",
      "\n",
      "混淆矩陣：\n",
      "[[17236     0]\n",
      " [   67     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiwi/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kiwi/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kiwi/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_va_predict=model.predict(X_va)\n",
    "y_va_predict=y_va_predict[:,0]\n",
    "y_va_predict=[int(i) for i in y_va_predict]\n",
    "y_va_predict=np.array(y_va_predict)\n",
    "from sklearn import metrics\n",
    "print(\"精準度指標：\")\n",
    "print(metrics.classification_report(Y_va,y_va_predict))\n",
    "print(\"混淆矩陣：\")\n",
    "print(metrics.confusion_matrix(Y_va,y_va_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.004480474628508091, 0.004426789004355669, 0.004352746065706015, 0.0042292270809412, 0.004050383344292641, 0.003771084127947688, 0.003466013353317976, 0.003122918074950576, 0.002814633073285222, 0.002504516625776887, 0.0022562851663678885, 0.0020403957460075617, 0.0018344525014981627, 0.0016734044766053557, 0.0015785015420988202, 0.0014705804642289877, 0.0013882291968911886, 0.0012990589020773768, 0.0012673338642343879, 0.0011628966312855482, 0.0011541127460077405, 0.0010895556770265102, 0.0010606771102175117, 0.0010461871279403567, 0.0009990704711526632, 0.0009855028474703431, 0.0009248716523870826, 0.0008990474743768573, 0.0008745299419388175, 0.0008572021615691483, 0.0008536005625501275, 0.000825336785055697, 0.0007993665640242398, 0.0008028130978345871, 0.000773648323956877, 0.000775390537455678, 0.0007673485670238733, 0.0007642138516530395, 0.0008136603282764554, 0.0007427817326970398, 0.0007993559702299535, 0.0007555034244433045, 0.0007137591019272804, 0.000754610518924892, 0.0007018284522928298, 0.0007471130811609328, 0.0007006293162703514, 0.0006771887419745326, 0.0006652368465438485, 0.0006705641862936318, 0.0006932718097232282, 0.0006721141980960965, 0.0006231973529793322, 0.0006808758480474353, 0.0006386290770024061, 0.0006208695122040808, 0.0006690687732771039, 0.0006546759395860136, 0.0006505984347313643, 0.0006433683447539806, 0.0006598646868951619, 0.0006194972665980458, 0.000605677894782275, 0.0005964052979834378, 0.0006053529214113951, 0.0005988223128952086, 0.000600941595621407, 0.0006255203043110669, 0.000571129028685391, 0.0005559709388762712, 0.0005669090314768255, 0.0005434412159956992, 0.0005570093635469675, 0.0005823955871164799, 0.0005632619140669703, 0.000630981579888612, 0.0006006706971675158, 0.0005400552763603628, 0.000588730676099658, 0.0005604359903372824, 0.0005756433238275349, 0.0005277188611216843, 0.0005795193137601018, 0.000528929871506989, 0.000512808037456125, 0.0005239240126684308, 0.0005213963449932635, 0.0005287821986712515, 0.00047805209760554135, 0.0005475302459672093, 0.000494327221531421, 0.0004978058859705925, 0.00047361970064230263, 0.0004952185554429889, 0.0004548011929728091, 0.0004878136969637126, 0.00047917518531903625, 0.0005082623101770878, 0.00046049291267991066, 0.0004992532194592059, 0.0004420720797497779, 0.0004706787585746497, 0.00047237309627234936, 0.0005008176667615771, 0.0005165324546396732, 0.0004857491876464337, 0.00047516298945993185, 0.0005648909136652946, 0.0005906015867367387, 0.0005751642165705562, 0.0005306677194312215, 0.0004953907919116318, 0.0004753419489134103, 0.0005057880189269781, 0.0005514688673429191, 0.0005374170141294599, 0.00046626670518890023, 0.0005287965177558362, 0.0004395201103761792, 0.0004891858552582562, 0.0004408153472468257, 0.0004911588039249182, 0.00042355904588475823, 0.00045106917968951166, 0.00044005276868119836, 0.00046371715143322945, 0.00047953453031368554, 0.0004492150037549436, 0.00047889217967167497, 0.00046781726996414363, 0.0004954299656674266, 0.0004640506813302636, 0.000423555844463408, 0.0004600060055963695, 0.0004158286319579929, 0.00046240424853749573, 0.0005186438211239874, 0.00044706850894726813, 0.0004777431895490736, 0.0004851838748436421, 0.00047150274622254074, 0.0004415176808834076, 0.0004583776753861457, 0.00042764097452163696, 0.00044166104635223746, 0.0004228112811688334, 0.00041661475552245975, 0.0004272731312084943, 0.0004062454681843519, 0.0004447618848644197, 0.000462833238998428, 0.00043518314487300813, 0.0004124879778828472, 0.00045441807014867663, 0.0004278338165022433, 0.0004334404948167503, 0.00047178121167235076, 0.0004723919846583158, 0.00045834676711820066, 0.0004134704067837447, 0.00044274365063756704, 0.0004039485938847065, 0.00044080332736484706, 0.00043454859405755997, 0.0004097386554349214, 0.0004105945990886539, 0.0004476795729715377, 0.0004351252573542297, 0.0004327335918787867, 0.0004273403319530189, 0.00041319456067867577, 0.0004229499027132988, 0.00038501614471897483, 0.00041177746606990695, 0.00043045866186730564, 0.0004747523053083569, 0.00042315933387726545, 0.0004283614980522543, 0.0004040765343233943, 0.00041271696682088077, 0.00038672436494380236, 0.00038850316195748746, 0.0004144796694163233, 0.000398182135540992, 0.00037911286926828325, 0.0004165958089288324, 0.00038028700510039926, 0.000402730714995414, 0.00045848306035622954, 0.00042314521851949394, 0.00039501028368249536, 0.00041349558159708977, 0.00043988338438794017, 0.000443534750957042, 0.0004462744982447475, 0.00045062365825288, 0.0004200833791401237, 0.0004322825698181987, 0.0004328613867983222, 0.00043756995000876486, 0.0004017748578917235, 0.0004087335546500981, 0.0004427261301316321, 0.00043017431744374335, 0.00045663033961318433, 0.00046855397522449493, 0.0004628011374734342, 0.0004316743870731443, 0.0004801476316060871, 0.00039328259299509227, 0.00041429049451835454, 0.0004293268721085042, 0.00038116745417937636, 0.00040272908518090844, 0.00039324062527157366, 0.0004180152900516987, 0.00036913843359798193, 0.0004121871024835855, 0.00040403721504844725, 0.00039983904571272433, 0.00042417299118824303, 0.00039047191967256367, 0.0003980061155743897, 0.00038460124051198363, 0.0003906968340743333, 0.00043531935079954565, 0.00042969599599018693, 0.0004172041954007, 0.00038142711855471134, 0.0003871089138556272, 0.0004138761723879725, 0.0003793136856984347, 0.0003833485534414649, 0.00040535570587962866, 0.00038171926280483603, 0.0003594229056034237, 0.00039903903962112963, 0.0003773031057789922, 0.00041164938011206686, 0.0004033995501231402, 0.00044958546641282737, 0.00045978446723893285, 0.0004106768756173551, 0.0004494538006838411, 0.00038919923827052116, 0.00037705281283706427, 0.0003457682323642075, 0.00037551450077444315, 0.00040594450547359884, 0.0003784438013099134, 0.00034885527566075325, 0.00040035779238678515, 0.0004020974738523364, 0.00044023041846230626, 0.0003900286683347076, 0.00038196248351596296, 0.00035200006095692515, 0.00035312274121679366, 0.0003717694489751011, 0.00037223470280878246, 0.0003558735188562423, 0.00034305211738683283, 0.000314532924676314, 0.0004026688984595239, 0.0003476591664366424, 0.0003735936479642987, 0.00034884081105701625, 0.0003373379586264491, 0.00033208567765541375, 0.0003443322202656418, 0.00033723795786499977, 0.0003594644949771464, 0.0003662055532913655, 0.0003278874501120299, 0.0003332648775540292, 0.00029029586585238576, 0.00030218687606975436, 0.00033801450626924634, 0.0003618054906837642, 0.0003641875518951565, 0.0003396291867829859, 0.0003227963170502335, 0.0003373912477400154, 0.0002978770062327385, 0.0003150185220874846, 0.0003306106082163751, 0.0002994168025907129, 0.0003447275375947356, 0.000303369335597381, 0.0002832553000189364, 0.0002717858587857336, 0.0002824787807185203, 0.0003245420230086893, 0.0003167464747093618, 0.00034462325857020915, 0.0003796606615651399, 0.0003113265847787261, 0.00033970363438129425, 0.00031741755083203316, 0.0003459842118900269, 0.0003330841427668929, 0.0002920504775829613, 0.00027285193209536374, 0.00028876509168185294, 0.00037560611963272095, 0.0002898252569139004, 0.00036841616383753717, 0.00029419505153782666, 0.00029257748974487185, 0.0002821090165525675, 0.0002576650585979223, 0.0003001808072440326, 0.00028911014669574797, 0.00024120244779624045, 0.00029783896752633154, 0.00028305829619057477, 0.00032447310513816774, 0.00030402393895201385, 0.0002642152539920062, 0.00033169626840390265, 0.0003208496782463044, 0.00025614575133658946, 0.00029121662373654544, 0.00032430386636406183, 0.00026550679467618465, 0.00022023363271728158, 0.0002621032181195915, 0.0003579584008548409, 0.0003101149632129818, 0.0003157524042762816, 0.00036425108555704355, 0.0003515377757139504, 0.0002865971182473004, 0.0003441562585067004, 0.00026023489772342145, 0.00023565306037198752, 0.0002427282161079347, 0.00023743521887809038, 0.0002475925430189818, 0.0002085218147840351, 0.00022874788555782288, 0.0002682743943296373, 0.0002244425704702735, 0.00023815303575247526, 0.00022715743398293853, 0.00023648822389077395, 0.00021560779714491218, 0.00020815510652028024, 0.0002608596405480057, 0.0002623558393679559, 0.00022144371178001165, 0.00031718434183858335, 0.0002524353039916605, 0.0002850274322554469, 0.0002791498554870486, 0.0001919586502481252, 0.000195406362763606, 0.00022782161249779165, 0.00023023165704216808, 0.00023361670901067555, 0.00019008410163223743, 0.00020173504890408367, 0.00021196548186708242, 0.00022568881104234606, 0.00020174685050733387, 0.00019326859910506755, 0.00017369730630889535, 0.00030341336969286203, 0.00025109347188845277, 0.00021549680968746543, 0.00015157384041231126, 0.00017170971841551363, 0.0002116696268785745, 0.0002636391727719456, 0.00021920265862718225, 0.00021108976216055453, 0.00021871078934054822, 0.0002000461536226794, 0.00023020169464871287, 0.00022988306591287255, 0.0002097172400681302, 0.0003027065540663898, 0.00024084751203190535, 0.00018263851234223694, 0.00020646605116780847, 0.0002458712551742792, 0.00021926176850683987, 0.00017925328575074673, 0.00018237074255011976, 0.00022090341371949762, 0.0002304691879544407, 0.00023395969765260816, 0.00018684970564208925, 0.00017093039059545845, 0.0002093122311634943, 0.00021227530669420958, 0.00021907215705141425, 0.0001780854945536703, 0.000231704514590092, 0.00021958274010103196, 0.00018420338165014982, 0.0001988651347346604, 0.00017183176532853395, 0.00016515601600985974, 0.0001933468010975048, 0.00019216960936319083, 0.00021605408983305097, 0.00022277429525274783, 0.00015310515300370753, 0.00019784703908953816, 0.00021353643387556076, 0.0002312815049663186, 0.00028123302035965025, 0.0002531379577703774, 0.00019776741100940853, 0.0001737032289383933, 0.0002203798503614962, 0.00017464443226344883, 0.00019151406013406813, 0.00015363749116659164, 0.0002146603656001389, 0.00017630714864935726, 0.00017012613534461707, 0.00019662319391500205, 0.00014930717588867992, 0.00017231809033546597, 0.00019876383885275573, 0.00015055175754241645, 0.00014903393457643688, 0.00018034303502645344, 0.0001630698243388906, 0.00021587680384982377, 0.00021033178200013936, 0.000171522973687388, 0.00019256070663686842, 0.0001969528675545007, 0.000183754920726642, 0.00020741185289807618, 0.00022121072106529027, 0.00025213274057023227, 0.00017155912064481527, 0.00019873651035595685, 0.00026831921422854066, 0.00025879417080432177, 0.0001628919126233086, 0.00025020731845870614, 0.00020334670261945575, 0.00019511058053467423, 0.00015225724200718105, 0.00015891851217020303, 0.00018982216715812683, 0.00015529429947491735, 0.00016840860189404339, 0.00017932130140252411, 0.00020381425565574318, 0.0001209999609272927, 0.00014234177069738507, 0.00018498308782000095, 0.00017046363791450858, 0.00019511372374836355, 0.00018296253983862698, 0.0001730140356812626, 0.00016056951426435262, 0.0001843860518420115, 0.0002609053917694837, 0.00019673882343340665, 0.000177674344740808, 0.00017171584477182478, 0.0001900714123621583, 0.00017616443801671267, 0.00015513099788222462, 0.00016494853480253369, 0.00022298513795249164, 0.00015134099521674216, 0.00016437200247310102, 0.0002375108888372779, 0.00021687825210392475, 0.00017144354933407158, 0.00015357347729150206, 0.000144149104016833, 0.00020084940479137003, 0.00013883193605579436, 0.0001864489895524457, 0.0001630902843317017, 0.00016655601211823523, 0.0001558473741170019, 0.0001811605179682374, 0.00015984267520252615, 0.00015681050717830658, 0.00012032844097120687, 0.0001563906407682225, 0.00017155475507024676, 0.0001707849296508357, 0.00016324497119057924, 0.00015460519352927804, 0.00017268647206947207, 0.0001539122313261032, 0.000155714777065441, 0.00011612175148911774, 0.00012392576900310814, 0.00012170366971986368, 0.000136874383315444, 0.00012337267980910838, 0.0001189715985674411, 0.00011415277549531311, 0.00012786647130269557, 0.00013131496962159872, 0.00011981913121417165, 0.00014289023238234222, 0.00016054911247920245, 0.00010196711809840053, 0.00013741674774792045, 0.00013961481454316527, 0.00016544271784368902, 0.00018457115220371634, 0.00014644008479081094, 0.00014221186575014144, 0.00015935595729388297, 0.0001439965417375788, 0.00013650755863636732, 0.00014252937398850918, 0.00019808454089798033, 0.00011756391177186742, 0.00014360854402184486, 0.00013549467257689685, 0.00013543602835852653, 0.00016977579798549414, 0.00015597775927744806, 0.00019570511358324438, 0.00013851509720552713, 0.000236607258557342, 0.00017428694991394877, 0.00017711894179228693, 0.00014675103011541069, 0.00016358397260773927, 0.00015317312499973923, 0.00013446449884213507, 0.00012470012006815523, 0.00011100649135187268, 0.0001332830433966592, 0.00010804744670167565, 0.00013626355212181807, 0.0001123986003221944, 0.00013623775157611817, 0.00012625147064682096, 0.00013382549514062703, 0.00013764682807959616, 0.0001489231362938881, 0.0001163482666015625, 0.00011759992776205763, 0.00012966587382834405, 0.00011914947390323505, 0.00010892900900216773, 0.00013490248238667846, 0.0001105134142562747, 0.00011228583753108978, 0.00010191076580667868, 0.0001435632148059085, 0.00010805508645717055, 0.00010421231854707003, 0.00011171296500833705, 0.0001272463850909844, 0.00010226103768218309, 0.0001345002674497664, 0.00011570967035368085, 0.0001060761496773921, 0.00012097476428607479, 0.00015086852363310754, 0.00010721788567025214, 0.00010587741417111829, 0.0001536857889732346, 0.00012559576134663075, 0.00013604665582533926, 0.00013454328291118145, 0.00013316074910108, 0.00013771610974799842, 0.00011095151421613991, 0.00015719640941824764, 0.0001533260365249589, 0.00013687857426702976, 0.0001302702003158629, 0.00017218859284184873, 0.00013590900925919414, 0.00022266920132096857, 0.00014128978364169598, 0.000137554612592794, 0.00015084032202139497, 0.00020298187155276537, 0.00012405746383592486, 0.000131789522129111, 0.0001755700504872948, 0.00014761873171664774, 0.0001308691134909168, 0.00016315930406562984, 0.00016359407163690776, 0.00013691883941646665, 0.00017211487283930182, 0.00022213840566109866, 0.00018415898375678807, 0.0001699296699371189, 0.000217647451790981, 0.00019159549265168607, 0.0001576358190504834, 0.00022198830265551805, 0.00024924983154051006, 0.0003604038502089679, 0.0002927571185864508, 0.00034089741529896855, 0.00019581579545047134, 0.00015290419105440378, 0.00013188844604883343, 0.00013532457523979247, 0.00013295466487761587, 0.00015311053721234202, 0.00015673752932343632, 0.00013405259232968092, 0.00016104844689834863, 0.00014395275502465665, 0.0001503918319940567, 0.00015225709648802876, 0.0001577896036906168, 0.0001249848719453439, 0.00011713659478118643, 0.00011399840877857059, 0.0001710489741526544, 0.00012153369607403874, 0.00014008305151946843, 0.00012870747013948858, 0.00012712784518953413, 0.0001273148227483034, 0.0001344615448033437, 0.0001491631119279191, 0.00022132419690024108, 0.00019270570192020386, 0.00015009369235485792, 0.00015988068480510265, 0.00016980025975499302, 0.00013839603343512863, 0.0001353875413769856, 0.0001421984488843009, 0.00014699673920404166, 0.00020542713173199445, 0.00020941285765729845, 0.0001892920845421031, 0.00019600348605308682, 0.00014744745567440987, 0.00014981682761572301, 0.00012437942496035248, 0.00011990610073553398, 0.00015877488476689905, 0.0001611832994967699, 0.00012919896107632667, 0.00019407151557970792, 0.00016838900046423078, 0.0001561971876071766, 0.00014275391004048288, 0.00013153479085303843, 0.00013027383829466999, 0.0001315029221586883, 0.0001139616288128309, 0.0001477876940043643, 0.0001517947530373931, 0.00014455625205300748, 0.00012862849689554423, 0.00011617178097367287, 0.0001058429988916032, 0.00011168501805514097, 0.00017371530702803284, 0.00020803172083105892, 0.00017588140326552093, 0.0002431846660329029, 0.0001871386484708637, 0.00014839314098935574, 0.00013241588021628559, 0.00015001511201262474, 0.00015927248750813305, 0.00014264308265410364, 0.0001259401615243405, 0.00013557773490902036, 0.00011778927728300914, 0.00012755862553603947, 0.00013285016757436097, 0.00011429909500293434, 0.0001284613972529769, 0.00013767035852652043, 0.0001554125628899783, 0.00012190188863314688, 0.00014837396156508476, 0.00013022494385950267, 0.00015220782370306551, 0.00013398515875451267, 0.00011080369586125016, 0.00012340035755187273, 0.00010613624908728525, 0.00011351616558386013, 0.00010558073699939996, 0.00011050682951463386, 0.00011850974260596558, 0.00011584006279008463, 0.00010155724885407835, 0.00011321539932396263, 0.00011190461373189464, 0.00011159966379636899, 0.00011046887084376067, 0.00013407552614808083, 0.00012459937715902925, 0.00011651629029074684, 0.00011697423178702593, 0.00010971662413794547, 0.00011024060222553089, 9.977973968489096e-05, 0.00012007861369056627, 0.00012228277046233416, 9.834556840360165e-05, 0.00012319567031227052, 0.00012439109559636563, 0.00011956952221225947, 0.00013942967052571476, 0.00012974573473911732, 0.00017410743748769164, 0.00017920926620718092, 0.00017700239550322294, 0.00013843181659467518, 0.000181742143468, 0.00016125205729622394, 0.000330308306729421, 0.00031315002706833184, 0.0002716399321798235, 0.0002570426440797746, 0.0002778180059976876, 0.0002280070912092924, 0.0001384577335556969, 0.0001300339208682999, 0.00018033041851595044, 0.0001808320957934484, 0.0001529930450487882, 0.00014704721979796886, 0.00012763819540850818, 0.00015303946565836668, 0.00012261261872481555, 0.00014568193000741303, 0.00016133619647007436, 0.00013004090578760952, 0.00013594943447969854, 0.00012552543194033206, 0.00011000222730217502, 0.00010018765897257254, 0.0001377018925268203, 0.00012321765825618058, 0.00010036934691015631, 0.00012197841715533286, 0.00010404688509879634, 0.00015810898912604898, 0.00015439713024534285, 0.00011192942474735901, 0.0001175914949271828, 0.00012514129048213363, 0.000103872807812877, 0.0001243959995917976, 0.00010994156036758795, 0.0001037368638208136, 0.00011569309572223574, 0.00010859478788916022, 0.00012701605737674981, 0.0001678375992923975, 0.0001138285078923218, 0.00010607046715449542, 9.001108264783397e-05, 0.00012252638407517225, 0.0001390391553286463, 0.0001136824139393866, 0.00011726831871783361, 0.00011688248196151108, 0.00014913313498254865, 0.00014123457367531955, 0.000123662524856627, 0.00013586122076958418, 0.00012378809333313257, 0.00010970523726427928, 9.176251478493214e-05, 7.487242692150176e-05, 0.00010464160732226446, 0.00010675722296582535, 0.00010737773845903575, 8.503454591846094e-05, 0.00010586257121758536, 7.766768976580352e-05, 8.055629587033764e-05, 8.763772348174825e-05, 8.616888953838497e-05, 9.681507799541578e-05, 6.784626020817086e-05, 8.291334961540997e-05, 0.00011599683057283983, 0.00015648991393391043, 0.0001544800616102293, 8.349978452315554e-05, 0.0001133209589170292, 9.805068839341402e-05, 0.00010981383093167096, 0.00015079115109983832, 0.00011979784903815016, 0.0001253822265425697, 0.00016611367755103856, 0.00011669241939671338, 9.935445996234193e-05, 0.00014302121417131275, 9.205783862853423e-05, 9.907608909998089e-05, 7.449957774952054e-05, 6.844669405836612e-05, 0.00010275003296555951, 7.359846495091915e-05, 9.932198008755222e-05, 9.794044308364391e-05, 7.261646533152089e-05, 8.766477549215779e-05, 8.123642328428105e-05, 7.73335195844993e-05, 9.933907858794555e-05, 0.0001615180226508528, 7.974540494615212e-05, 0.00010968574497383088, 9.244201646652073e-05, 8.128578338073567e-05, 0.00010100851068273187, 8.337119652424008e-05, 8.720310870558023e-05, 8.76110207173042e-05, 7.929903222247958e-05, 7.367919170064852e-05, 9.806032903725281e-05, 9.591262642061338e-05, 8.674780838191509e-05, 0.00017407331324648112, 0.000187424331670627, 0.0001721840671962127, 0.00012273488391656429, 0.00012339693785179406, 0.0001171512994915247, 0.00010305824253009632, 0.00014675520651508123, 0.00012495317787397653, 0.00013393386325333267, 0.00018291303422302008, 0.00011078456736868247, 0.0001092836755560711, 0.00013471508282236755, 0.00010441157064633444, 8.24212038423866e-05, 0.00010084360837936401, 8.193876419682056e-05, 0.00012023431190755218, 8.89536677277647e-05, 0.00011080421973019838, 0.000125929233036004, 7.312274829018861e-05, 8.756696479395032e-05, 7.435593579430133e-05, 6.393301737261936e-05, 0.00010059760097647086, 9.57054435275495e-05, 0.00011185541370650753, 7.27804726921022e-05, 8.594348037149757e-05, 8.858099317876622e-05, 0.00010235832451144233, 7.189619645942003e-05, 0.00010274775559082627, 9.810731717152521e-05, 8.784158853814006e-05, 7.324646139750257e-05, 0.00013393102562986314, 0.00012435225653462112, 9.812325151870027e-05, 8.945510489866138e-05, 9.598232281859964e-05, 0.00010862109047593549, 7.619582902407274e-05, 9.861413127509877e-05, 6.33167292107828e-05, 6.55253097647801e-05, 9.436064283363521e-05, 7.255938544403762e-05, 8.223844633903354e-05, 6.280752131715417e-05, 8.623614121461287e-05, 8.516212983522564e-05, 9.050160588230938e-05, 9.649593266658485e-05, 7.138521323213354e-05, 8.76713456818834e-05, 7.73206411395222e-05, 0.00010631310578901321, 0.000120785269245971, 0.00010594687046250328, 9.166548261418939e-05, 0.00013009323447477072, 0.00014503189595416188, 0.00014989722694735974, 0.00010804002522490919, 0.00016610065358690917, 9.72156849456951e-05, 9.434333333047107e-05, 8.846021955832839e-05, 6.535249849548563e-05, 0.00011040500976378098, 7.163403643062338e-05, 0.00010703652515076101, 7.45351571822539e-05, 7.527023262809962e-05, 0.0001262626756215468, 0.00023440367658622563, 0.00012234675523359329, 8.826746488921344e-05, 0.00012810913904104382, 0.00012739647354464978, 0.0004187260346952826, 0.00023710426467005163, 0.00022181074018590152, 0.0002137845876859501, 0.00010165577987208962, 8.53516103234142e-05, 8.094875374808908e-05, 9.654335008235648e-05, 7.347546488745138e-05, 0.00010021602793131024, 6.554274295922369e-05, 0.00010286153701599687, 0.00012653933663386852, 0.0001225822779815644, 8.41919390950352e-05, 0.00011904537677764893, 0.00010235496301902458, 0.00010416118311695755, 6.882262096041813e-05, 6.199007475515828e-05, 0.000123361503938213, 8.710138354217634e-05, 8.929828618420288e-05, 0.00011453992919996381, 0.00011270113463979214, 6.643666711170226e-05, 6.347250018734485e-05, 8.078530663624406e-05, 8.221869939006865e-05, 7.16357899364084e-05, 8.526622696081176e-05, 7.514651952078566e-05, 6.466772902058437e-05, 6.887834751978517e-05, 7.928610284579918e-05, 9.475991828367114e-05, 9.350719483336434e-05, 7.539399666711688e-05, 9.732649778015912e-05, 8.910681935958564e-05, 6.480429146904498e-05, 0.00010180490789934993, 7.047451072139665e-05, 6.877609121147543e-05, 0.00013255630619823933, 0.00010496360482648015, 8.877190703060478e-05, 0.00011691739928210154, 0.00010157869837712497, 7.543645187979564e-05, 7.333792746067047e-05, 9.46975706028752e-05, 0.00018471572548151016, 0.00012554666318465024, 8.997760596685112e-05, 0.00011594995885388926, 0.00010837744048330933, 9.689454600447789e-05, 0.00010353726975154132, 7.63189309509471e-05, 0.00010932803706964478, 8.877590153133497e-05, 9.984774806071073e-05, 0.0001637744571780786, 0.00020928539743181318, 0.00023284288181457669, 0.0002871942997444421, 0.0001328092475887388, 0.00013606947322841734, 0.00012489091022871435, 0.00010269868653267622, 9.578963363310322e-05, 8.069970499491319e-05, 7.496254693251103e-05, 6.251325976336375e-05, 6.54454343020916e-05, 7.160485984059051e-05, 9.23336046980694e-05, 6.316692451946437e-05, 6.140398181742057e-05, 0.00012939395674038678, 8.237012661993504e-05, 7.734143582638353e-05, 8.002349932212383e-05, 8.259886817540973e-05, 6.927712820470333e-05, 7.562207611044869e-05, 7.972936145961285e-05, 7.625308353453875e-05, 0.0001433330908184871, 9.69567263382487e-05, 6.96403076290153e-05, 7.45465949876234e-05, 6.947042857063934e-05, 7.016052404651418e-05, 7.756563718430698e-05, 6.499594746856019e-05], 'val_loss': [0.18510915338993073, 0.18804821372032166, 0.18553277850151062, 0.1812698394060135, 0.17425614595413208, 0.15828785300254822, 0.1423022598028183, 0.12319749593734741, 0.10765330493450165, 0.09233157336711884, 0.07840127497911453, 0.0672476664185524, 0.06009112298488617, 0.05383098125457764, 0.049709685146808624, 0.043066803365945816, 0.03833121061325073, 0.03724813461303711, 0.031027428805828094, 0.034594085067510605, 0.031086882576346397, 0.026452939957380295, 0.029558364301919937, 0.025945641100406647, 0.025507671758532524, 0.028196021914482117, 0.02552318200469017, 0.02275031805038452, 0.02373403310775757, 0.026858847588300705, 0.021233858540654182, 0.020770065486431122, 0.020283428952097893, 0.01875380612909794, 0.018980704247951508, 0.01906300149857998, 0.02145307883620262, 0.017912693321704865, 0.020186491310596466, 0.017156852409243584, 0.017889482900500298, 0.019206097349524498, 0.015950970351696014, 0.02082798257470131, 0.017908919602632523, 0.017054090276360512, 0.018495751544833183, 0.015271340496838093, 0.02013808861374855, 0.01401466317474842, 0.01745603047311306, 0.014807097613811493, 0.011503947898745537, 0.01389484666287899, 0.012219147756695747, 0.016431035473942757, 0.012812583707273006, 0.014303059317171574, 0.015416988171637058, 0.01393388956785202, 0.017855850979685783, 0.012339850887656212, 0.014652988873422146, 0.017354587092995644, 0.015183000825345516, 0.014702586457133293, 0.014317997731268406, 0.015621899627149105, 0.013393959030508995, 0.012882312759757042, 0.01179403718560934, 0.01396972592920065, 0.011943094432353973, 0.011485773138701916, 0.01497070211917162, 0.015724798664450645, 0.01521153375506401, 0.011706097982823849, 0.01191721297800541, 0.011963803321123123, 0.011357029899954796, 0.011850611306726933, 0.012306008487939835, 0.011149656027555466, 0.011384577490389347, 0.01360645517706871, 0.01044971588999033, 0.01184907741844654, 0.010312615893781185, 0.011560741811990738, 0.011675170622766018, 0.010798481293022633, 0.00995698757469654, 0.00990883819758892, 0.009333916939795017, 0.014387576840817928, 0.008944839239120483, 0.010422172024846077, 0.010334847494959831, 0.010392770171165466, 0.009225678630173206, 0.009574195370078087, 0.011695320717990398, 0.009366095066070557, 0.008747226558625698, 0.010160648263990879, 0.010202229022979736, 0.017769062891602516, 0.015255426988005638, 0.015372379682958126, 0.013625371269881725, 0.00921731349080801, 0.009674583561718464, 0.00887355487793684, 0.014318054541945457, 0.00948922336101532, 0.009111973457038403, 0.01039899792522192, 0.008680676110088825, 0.008350773714482784, 0.008978946134448051, 0.008841375820338726, 0.00853245984762907, 0.012111841700971127, 0.00872873142361641, 0.010256534442305565, 0.009424727410078049, 0.010651063174009323, 0.009386216290295124, 0.008868098258972168, 0.01001039706170559, 0.007421179208904505, 0.008279010653495789, 0.008806137368083, 0.00727334339171648, 0.01414223201572895, 0.008421147242188454, 0.008863760158419609, 0.012550724670290947, 0.009352081455290318, 0.007040014956146479, 0.008352180942893028, 0.007056189235299826, 0.0074962736107409, 0.007469606585800648, 0.007552908267825842, 0.006579299923032522, 0.007223655469715595, 0.006827302277088165, 0.00849878042936325, 0.007570165675133467, 0.009103120304644108, 0.006928401067852974, 0.007911087945103645, 0.0075431144796311855, 0.006962052546441555, 0.010342362336814404, 0.009224744513630867, 0.007466898765414953, 0.006984743755310774, 0.007355259265750647, 0.006947888061404228, 0.00766705721616745, 0.007050748448818922, 0.008334930054843426, 0.00707930326461792, 0.00749850133433938, 0.009134478867053986, 0.006790295708924532, 0.007264766842126846, 0.007174220401793718, 0.006381540093570948, 0.007893907837569714, 0.00835374090820551, 0.007550475653260946, 0.007819239981472492, 0.008093034848570824, 0.008613652549684048, 0.0069478945806622505, 0.007999824360013008, 0.006624738685786724, 0.006797154899686575, 0.007903228513896465, 0.0070639001205563545, 0.006257330998778343, 0.00780433090403676, 0.007526407018303871, 0.007766736205667257, 0.007885158061981201, 0.00815256405621767, 0.007811176124960184, 0.009333503432571888, 0.009933107532560825, 0.006679358892142773, 0.008343037217855453, 0.008667915128171444, 0.00863351859152317, 0.011537493206560612, 0.008038355968892574, 0.00663192942738533, 0.00664873979985714, 0.008259272202849388, 0.007794281467795372, 0.00740599911659956, 0.00965804886072874, 0.008904116228222847, 0.010652896016836166, 0.007899352349340916, 0.009313437156379223, 0.007534456439316273, 0.0066327061504125595, 0.006750080268830061, 0.007645199541002512, 0.007053544279187918, 0.006765944883227348, 0.007988505065441132, 0.006353222299367189, 0.007287733256816864, 0.007509457878768444, 0.007412008009850979, 0.006771421059966087, 0.006573024671524763, 0.006401231978088617, 0.006865210365504026, 0.0062566762790083885, 0.008588033728301525, 0.008019563741981983, 0.0077535230666399, 0.007064160890877247, 0.00642632320523262, 0.007122954819351435, 0.005999528802931309, 0.00682080490514636, 0.007269986439496279, 0.007010109256953001, 0.0067356363870203495, 0.006839577108621597, 0.007505216170102358, 0.007556690834462643, 0.009763109497725964, 0.01005510799586773, 0.009367482736706734, 0.007080205716192722, 0.011373378336429596, 0.006466533523052931, 0.005951293278485537, 0.005854027345776558, 0.0054864417761564255, 0.006824587471783161, 0.00904807448387146, 0.00672267796471715, 0.007597669493407011, 0.009090477600693703, 0.009560632519423962, 0.007905527949333191, 0.007779974490404129, 0.007510320283472538, 0.007741465698927641, 0.008551841601729393, 0.005780540872365236, 0.006574180442839861, 0.007898218929767609, 0.0054491558112204075, 0.0066496278159320354, 0.005440892186015844, 0.005922303535044193, 0.005406707059592009, 0.006884136702865362, 0.007596056908369064, 0.005703086499124765, 0.009634916670620441, 0.006971881724894047, 0.005549294874072075, 0.006202367600053549, 0.006052121054381132, 0.00523746432736516, 0.00647527864202857, 0.006423653103411198, 0.008364183828234673, 0.007731943391263485, 0.00554755749180913, 0.007109078578650951, 0.007639633491635323, 0.006692185997962952, 0.005070200189948082, 0.005589467007666826, 0.00463401572778821, 0.005676298402249813, 0.004983860999345779, 0.005183492787182331, 0.004749536048620939, 0.0047811842523515224, 0.005447795148938894, 0.00404491787776351, 0.005052733700722456, 0.008779723197221756, 0.005284086801111698, 0.0058650244027376175, 0.004906071815639734, 0.005033987108618021, 0.006475461646914482, 0.005376415327191353, 0.005399486515671015, 0.007096132263541222, 0.008147994987666607, 0.006056550424546003, 0.006814795546233654, 0.005813585594296455, 0.005924062337726355, 0.005326340440660715, 0.004885340109467506, 0.004657759331166744, 0.005762964487075806, 0.0049022794701159, 0.011444405652582645, 0.005489930976182222, 0.008465206250548363, 0.0066678766161203384, 0.006405769847333431, 0.006078806705772877, 0.005490044131875038, 0.005380549002438784, 0.005947882309556007, 0.007512830197811127, 0.009116191416978836, 0.005332144908607006, 0.00957412552088499, 0.006225752644240856, 0.00648724939674139, 0.022325977683067322, 0.005501913372427225, 0.008212100714445114, 0.0066504632122814655, 0.007710391655564308, 0.006222151219844818, 0.00535572599619627, 0.006485414691269398, 0.005301596596837044, 0.0058177560567855835, 0.005485260393470526, 0.005863480735570192, 0.005923530552536249, 0.006982374005019665, 0.007568530272692442, 0.004699659999459982, 0.00536338659003377, 0.005095599684864283, 0.007155924569815397, 0.006411851849406958, 0.005043999757617712, 0.005414489656686783, 0.00607462739571929, 0.005621710792183876, 0.009562716819345951, 0.00738097820430994, 0.004525348078459501, 0.005582170095294714, 0.004444607067853212, 0.008425368927419186, 0.0047540245577692986, 0.005172124132514, 0.005051901564002037, 0.0049792747013270855, 0.005629353225231171, 0.005760046653449535, 0.005553655792027712, 0.005686406046152115, 0.008571594022214413, 0.006367849186062813, 0.004809568170458078, 0.005064718890935183, 0.00600407226011157, 0.004669128451496363, 0.0071827108040452, 0.004904401488602161, 0.0055834571830928326, 0.005629315972328186, 0.006659526843577623, 0.005792124662548304, 0.008480417542159557, 0.007301385048776865, 0.005184980574995279, 0.007618440315127373, 0.0049733049236238, 0.005340285133570433, 0.006325486581772566, 0.00635086465626955, 0.004887616727501154, 0.004298321437090635, 0.005445920862257481, 0.004988450091332197, 0.005455649923533201, 0.004502974916249514, 0.0052447570487856865, 0.005330690182745457, 0.005582880228757858, 0.006094314623624086, 0.005151455756276846, 0.00786177720874548, 0.0059802536852657795, 0.004646481014788151, 0.0045508588664233685, 0.004494220018386841, 0.005111030302941799, 0.005730684846639633, 0.005232690367847681, 0.004811715334653854, 0.0051203626208007336, 0.004128505475819111, 0.006622738670557737, 0.003978523891419172, 0.005094725638628006, 0.00831659883260727, 0.005352269858121872, 0.004924160894006491, 0.004510928876698017, 0.0047972542233765125, 0.0043722945265471935, 0.004846765194088221, 0.004403546452522278, 0.005004857666790485, 0.006043712142854929, 0.004493515007197857, 0.006856087129563093, 0.004899843595921993, 0.004061010200530291, 0.004721973091363907, 0.004229365382343531, 0.00423411512747407, 0.005192799028009176, 0.006080057937651873, 0.004607354756444693, 0.004533004015684128, 0.00750668253749609, 0.005062620155513287, 0.004389028064906597, 0.009400821290910244, 0.005045218393206596, 0.0052839620038867, 0.010835211724042892, 0.0054309070110321045, 0.0052002957090735435, 0.01250706147402525, 0.007091373670846224, 0.00837678276002407, 0.0074304500594735146, 0.005080359056591988, 0.004691447596997023, 0.005141210742294788, 0.0048020994290709496, 0.005759828258305788, 0.0065331184305250645, 0.004446011036634445, 0.004471528809517622, 0.0053854468278586864, 0.004143535625189543, 0.0061871446669101715, 0.006234284024685621, 0.005264489911496639, 0.00521745253354311, 0.006788820959627628, 0.004652733914554119, 0.00591612933203578, 0.010705431923270226, 0.006253021769225597, 0.007882088422775269, 0.00815294124186039, 0.0055937375873327255, 0.006681560538709164, 0.0056493328884243965, 0.005981605499982834, 0.005013379734009504, 0.007967344485223293, 0.0048604863695800304, 0.006447280291467905, 0.006693518720567226, 0.008389001712203026, 0.0054458254016935825, 0.007712428458034992, 0.005549022927880287, 0.008025651797652245, 0.004818724002689123, 0.008140111342072487, 0.0056319087743759155, 0.00507110171020031, 0.006092492491006851, 0.005520297214388847, 0.006475900299847126, 0.005944022908806801, 0.004395439755171537, 0.0062504420056939125, 0.005940186325460672, 0.0065947300754487514, 0.004591932520270348, 0.005150476936250925, 0.004921331536024809, 0.004268959164619446, 0.005690101534128189, 0.003538512159138918, 0.005229628179222345, 0.004935953766107559, 0.004800279624760151, 0.004078302066773176, 0.00461605004966259, 0.004084319341927767, 0.003999251406639814, 0.00402818201109767, 0.004070771858096123, 0.004069076851010323, 0.003872423665598035, 0.003754890989512205, 0.006367103662341833, 0.004326960537582636, 0.008735394105315208, 0.009081822820007801, 0.006332028191536665, 0.005709598306566477, 0.006307798437774181, 0.005473040044307709, 0.004802091978490353, 0.007474387995898724, 0.005469861440360546, 0.005324508994817734, 0.004739286378026009, 0.004499369766563177, 0.00570424972102046, 0.007160212378948927, 0.004476737696677446, 0.005981690715998411, 0.004584777634590864, 0.00606042193248868, 0.003895604982972145, 0.00668077589944005, 0.006102577317506075, 0.005654357839375734, 0.004812321625649929, 0.005614456254988909, 0.004383671097457409, 0.005084942560642958, 0.0048525030724704266, 0.004233009181916714, 0.004362470004707575, 0.00461308378726244, 0.004273567348718643, 0.006821810733526945, 0.0036463718861341476, 0.007860717363655567, 0.004446515813469887, 0.004731620196253061, 0.004469173960387707, 0.004349773749709129, 0.004275531508028507, 0.004693681374192238, 0.004147641360759735, 0.004456658381968737, 0.0038675288669764996, 0.00372672057710588, 0.005061618518084288, 0.004249786492437124, 0.0037128529511392117, 0.004926613997668028, 0.004047073889523745, 0.0035687407944351435, 0.00372039875946939, 0.0040907058864831924, 0.0032992102205753326, 0.005124301183968782, 0.005724741145968437, 0.003858824959024787, 0.003901854855939746, 0.0052509792149066925, 0.004975209943950176, 0.004645951557904482, 0.004447758663445711, 0.004547896794974804, 0.004339471459388733, 0.00441192090511322, 0.00551685830578208, 0.005992546211928129, 0.004482320975512266, 0.00443131010979414, 0.008423036895692348, 0.0038434150628745556, 0.0075699565932154655, 0.003603193210437894, 0.004704611375927925, 0.005011710338294506, 0.004902863875031471, 0.0037787607870996, 0.004144015721976757, 0.006227357778698206, 0.005953525193035603, 0.004338253755122423, 0.005650865379720926, 0.004862093832343817, 0.004763705190271139, 0.007924390025436878, 0.005603828467428684, 0.00814878661185503, 0.0036309114657342434, 0.00667266221717, 0.006757291033864021, 0.004282460082322359, 0.0058340891264379025, 0.007291879504919052, 0.012692011892795563, 0.007797672878950834, 0.00684405816718936, 0.006350905168801546, 0.005498070735484362, 0.005046788137406111, 0.005880751647055149, 0.004567570053040981, 0.0050829690881073475, 0.00538443960249424, 0.004306949209421873, 0.0039022420533001423, 0.003699101973325014, 0.0071000270545482635, 0.0037804655730724335, 0.004885176662355661, 0.0045372010208666325, 0.003996892366558313, 0.0042838528752326965, 0.005419726017862558, 0.0037488695234060287, 0.003665842581540346, 0.005499440710991621, 0.00549197057262063, 0.005788272712379694, 0.004470609128475189, 0.010396760888397694, 0.009205415844917297, 0.007351952139288187, 0.004228429403156042, 0.005822630133479834, 0.00508943060413003, 0.004751933738589287, 0.004520562943071127, 0.004111586604267359, 0.006349026691168547, 0.006216229405254126, 0.009562963619828224, 0.006439412012696266, 0.007498505525290966, 0.004638452548533678, 0.005404962692409754, 0.00444024009630084, 0.004072698764503002, 0.008316379971802235, 0.006200358737260103, 0.004636959172785282, 0.008803403936326504, 0.00454645324498415, 0.0044968086294829845, 0.005228884518146515, 0.00487162172794342, 0.0050074453465640545, 0.004225441254675388, 0.004202431533485651, 0.004139650613069534, 0.004750024527311325, 0.0036787691060453653, 0.003670132253319025, 0.0032940958626568317, 0.0033607373479753733, 0.0034088960383087397, 0.004442182835191488, 0.00749824708327651, 0.008355474099516869, 0.007054840214550495, 0.00635722279548645, 0.004937507212162018, 0.004732032306492329, 0.0044660321436822414, 0.004822127521038055, 0.0049970210529863834, 0.004070849157869816, 0.0047011892311275005, 0.004161929711699486, 0.004304193425923586, 0.0041394480504095554, 0.005860267672687769, 0.004163158126175404, 0.0044484431855380535, 0.005413509439677, 0.004051171708852053, 0.004467190243303776, 0.004089654423296452, 0.004205360542982817, 0.0046768453903496265, 0.0037604609970003366, 0.004001039545983076, 0.0038398653268814087, 0.003796245902776718, 0.0039110635407269, 0.0038265541661530733, 0.004117984790354967, 0.0037308300379663706, 0.0034707491286098957, 0.0035336515866219997, 0.003793350886553526, 0.0035996364895254374, 0.003678489476442337, 0.0039162361063063145, 0.00353591819293797, 0.0037536057643592358, 0.004211269784718752, 0.0037393157836049795, 0.00401307875290513, 0.0036559724248945713, 0.004610640462487936, 0.003436051541939378, 0.0030307723209261894, 0.0036548583302646875, 0.00453635398298502, 0.003721680259332061, 0.0048604197800159454, 0.004158850759267807, 0.0050115883350372314, 0.003717002458870411, 0.004947638604789972, 0.0056176381185650826, 0.0063191684894263744, 0.008072891272604465, 0.014894265681505203, 0.007313293870538473, 0.018121926113963127, 0.005060190800577402, 0.006413638591766357, 0.006420711055397987, 0.005564527120441198, 0.00451767398044467, 0.005323834717273712, 0.0044281259179115295, 0.004358028527349234, 0.004135121591389179, 0.004534114617854357, 0.004424995742738247, 0.005058625712990761, 0.004675869829952717, 0.004779068753123283, 0.004460208583623171, 0.004116684664040804, 0.004285601899027824, 0.0038691486697643995, 0.003930427599698305, 0.0041974736377596855, 0.004044674802571535, 0.003612113418057561, 0.0038733328692615032, 0.0038641721475869417, 0.005910373758524656, 0.003990276716649532, 0.0036147257778793573, 0.004602687433362007, 0.003991969395428896, 0.003864580998197198, 0.0039884177967906, 0.004355655983090401, 0.004096807446330786, 0.003992199897766113, 0.0038611770141869783, 0.009131654165685177, 0.0061306944116950035, 0.004835948348045349, 0.004182614851742983, 0.005269937217235565, 0.004952666349709034, 0.005044312682002783, 0.005493079777806997, 0.004732295870780945, 0.005313795525580645, 0.010022426955401897, 0.004251813516020775, 0.008117841556668282, 0.005010736174881458, 0.005288721062242985, 0.004879026673734188, 0.004356870427727699, 0.0035022965166717768, 0.004328637849539518, 0.004463066812604666, 0.003886243561282754, 0.004595070146024227, 0.004617547616362572, 0.003962022252380848, 0.004533278290182352, 0.004126005340367556, 0.004543139133602381, 0.004319682717323303, 0.0036068877670913935, 0.003966435790061951, 0.007363181095570326, 0.005131208337843418, 0.0048066903837025166, 0.004361131228506565, 0.0054533216170966625, 0.004287792835384607, 0.0043297382071614265, 0.003960258327424526, 0.005763249006122351, 0.004397953860461712, 0.005124369636178017, 0.0053570629097521305, 0.004805384669452906, 0.005465751048177481, 0.004273983184248209, 0.004373165778815746, 0.004087751265615225, 0.003880698001012206, 0.00434560002759099, 0.00384872336871922, 0.004874046426266432, 0.00450498191639781, 0.003925076220184565, 0.004315654281526804, 0.004287899471819401, 0.0066359275951981544, 0.00405981345102191, 0.004959346726536751, 0.0036602860782295465, 0.004185748752206564, 0.004095145501196384, 0.0044866385869681835, 0.003585902974009514, 0.004774549510329962, 0.0041848281398415565, 0.0037691781762987375, 0.004237933084368706, 0.003856348106637597, 0.003565995255485177, 0.003487990703433752, 0.004191496409475803, 0.006096043158322573, 0.005440132692456245, 0.005220327526330948, 0.005817221011966467, 0.005503375083208084, 0.003986357245594263, 0.003754027420654893, 0.004433650989085436, 0.005332011729478836, 0.005290772765874863, 0.004724435042589903, 0.005008625332266092, 0.006316540762782097, 0.005527152214199305, 0.004533413331955671, 0.004052378237247467, 0.004298088606446981, 0.003985745832324028, 0.00496253278106451, 0.004869918804615736, 0.004178743343800306, 0.004737119656056166, 0.0038740374147892, 0.003764160443097353, 0.0038292622193694115, 0.0037342594005167484, 0.0050715976394712925, 0.005879140459001064, 0.004312411416321993, 0.004132317379117012, 0.004182461183518171, 0.004575200844556093, 0.004641988314688206, 0.004624790046364069, 0.004639321472495794, 0.005010557826608419, 0.004889555275440216, 0.004491014871746302, 0.00450810045003891, 0.004713208414614201, 0.003919553942978382, 0.0043162754736840725, 0.0042815725319087505, 0.005342989694327116, 0.004313835874199867, 0.0038449945859611034, 0.0037938228342682123, 0.004384482279419899, 0.004350386559963226, 0.004734195303171873, 0.0043877167627215385, 0.003926610574126244, 0.003931792452931404, 0.004461511969566345, 0.004057356156408787, 0.0036149064544588327, 0.0038554901257157326, 0.003785554552450776, 0.006924215704202652, 0.006553830113261938, 0.006296142935752869, 0.004728144034743309, 0.007267378736287355, 0.008112656883895397, 0.006599592976272106, 0.01115692500025034, 0.003949418198317289, 0.005258770193904638, 0.0064864992164075375, 0.004254771862179041, 0.003763539483770728, 0.003656303510069847, 0.003661355935037136, 0.0035991144832223654, 0.003955251071602106, 0.0037004011683166027, 0.0038548121228814125, 0.025399206206202507, 0.00685559306293726, 0.006121212616562843, 0.0045224945060908794, 0.0046472991816699505, 0.00914958119392395, 0.005326256621629, 0.014381003566086292, 0.009263523854315281, 0.0077832844108343124, 0.00587081303820014, 0.005105850286781788, 0.0047136940993368626, 0.004404077306389809, 0.004457293543964624, 0.004260727670043707, 0.00418501440435648, 0.0051020607352256775, 0.008189484477043152, 0.004817570559680462, 0.004053235519677401, 0.005136264022439718, 0.004481643904000521, 0.004266044124960899, 0.004128798842430115, 0.003971468191593885, 0.004822785034775734, 0.005405763629823923, 0.004157458432018757, 0.006441773846745491, 0.004395846277475357, 0.0038836852181702852, 0.0038202686700969934, 0.0043427725322544575, 0.0046148584224283695, 0.003951008431613445, 0.004446835722774267, 0.004238766618072987, 0.003773928852751851, 0.004254856612533331, 0.004318648017942905, 0.004463748075067997, 0.005171490833163261, 0.004059751518070698, 0.005426317453384399, 0.004620762076228857, 0.0038229627534747124, 0.004872499033808708, 0.003537054406479001, 0.003329650731757283, 0.005342626012861729, 0.005798949394375086, 0.006939283572137356, 0.004448022227734327, 0.004872620105743408, 0.004704232793301344, 0.004557209089398384, 0.004800861235707998, 0.005410860758274794, 0.003716544946655631, 0.005545805208384991, 0.004867289215326309, 0.004837322048842907, 0.004527703858911991, 0.004491633735597134, 0.0038428157567977905, 0.0066155497916042805, 0.003998330794274807, 0.004400447476655245, 0.005924600176513195, 0.03629785776138306, 0.0064721968956291676, 0.009385486133396626, 0.004798312671482563, 0.00868811272084713, 0.004900584928691387, 0.004920471925288439, 0.004592318087816238, 0.004262400791049004, 0.004059125203639269, 0.004165994469076395, 0.004075965844094753, 0.004155813250690699, 0.004113649483770132, 0.00396775035187602, 0.0038498840294778347, 0.003931763116270304, 0.0044457209296524525, 0.004002666100859642, 0.004518711473792791, 0.004764215555042028, 0.0047907899133861065, 0.004481585696339607, 0.005442727357149124, 0.004703070968389511, 0.0042796870693564415, 0.004861253313720226, 0.004286980256438255, 0.004386174492537975, 0.004160033073276281, 0.004481809679418802, 0.004388804081827402, 0.004120953846722841]}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABcMAAAIQCAYAAABNHXZAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACaC0lEQVR4nOzdeXxTVf7/8Xfa0pYKLbK1gIUKIouyyVIRBR071g0HxRHREUSEUQGVuiAqm37HoiAyjihuyOiIIo7gAqKA4gIoCjKIAgoCRaBlkxYKdMv9/XF+aZI2XQLtvaV9PR+P+0hy77n3ntwkJbzvyee6LMuyBAAAAAAAAABANRbidAcAAAAAAAAAAKhshOEAAAAAAAAAgGqPMBwAAAAAAAAAUO0RhgMAAAAAAAAAqj3CcAAAAAAAAABAtUcYDgAAAAAAAACo9gjDAQAAAAAAAADVHmE4AAAAAAAAAKDaIwwHAAAAAAAAAFR7hOEAAAAAAAAAgGqPMBwASjF79my5XC59//33TncFAAAAAAAAJ4EwHAAAAAAAAABQ7RGGAwDK5Ha7dfz4cae7AQAAAAAAcMIIwwGgAvzwww+64oorFB0drTp16ujSSy/VN99849cmLy9PkyZNUuvWrRUZGakGDRrowgsv1JIlSwrbpKena8iQITrjjDMUERGhJk2a6C9/+Yu2b99eZh82bdqkG264QY0aNVLt2rXVpk0bPfLII4XLb731ViUkJBRbb+LEiXK5XH7zXC6XRo4cqTfffFPnnHOOIiIi9OGHH6p+/foaMmRIsW1kZWUpMjJS999/f+G8nJwcTZgwQWeddZYiIiIUHx+vBx98UDk5OX7rLlmyRBdeeKHq1aunOnXqqE2bNnr44YfLfL4AAAAAAADBCHO6AwBwqvvpp5900UUXKTo6Wg8++KBq1aqlF198URdffLG++OILJSYmSjKhc2pqqm6//Xb16NFDWVlZ+v7777V27Vr9+c9/liT1799fP/30k0aNGqWEhATt3btXS5YsUVpaWsAg22P9+vW66KKLVKtWLQ0fPlwJCQnaunWrPvzwQ/3jH/84oef12Wef6Z133tHIkSPVsGFDtW7dWtdee63ee+89vfjiiwoPDy9su2DBAuXk5OjGG2+UZEaSX3PNNfr66681fPhwtWvXTj/++KOeeeYZ/fLLL1qwYEHhsbv66qvVsWNHPfbYY4qIiNCWLVu0YsWKE+ozAAAAAABASQjDAeAkPfroo8rLy9PXX3+tli1bSpIGDRqkNm3a6MEHH9QXX3whSVq4cKGuvPJKvfTSSwG3c+jQIa1cuVJTpkzxG2E9duzYMvswatQoWZaltWvXqnnz5oXzJ0+efMLPa/Pmzfrxxx/Vvn37wnkDBgzQrFmz9Omnn+rqq68unD937ly1bNlS3bp1kyTNmTNHS5cu1RdffKELL7ywsN25556rO+64QytXrtQFF1ygJUuWKDc3Vx9//LEaNmx4wn0FAAAAAAAoC2VSAOAkFBQU6NNPP1W/fv0Kg3BJatKkiW666SZ9/fXXysrKkiTVq1dPP/30k3799deA26pdu7bCw8O1fPly/fHHH+Xuw759+/Tll1/qtttu8wvCJRUrfxKMPn36+AXhkvSnP/1JDRs21Ny5cwvn/fHHH1qyZIkGDBhQOG/evHlq166d2rZtq/379xdOf/rTnyRJn3/+uSRzTCTp/fffl9vtPuG+AgAAAAAAlIUwHABOwr59+3T06FG1adOm2LJ27drJ7XZr586dkqTHHntMhw4d0tlnn60OHTrogQce0Pr16wvbR0RE6Mknn9THH3+s2NhY9e7dW0899ZTS09NL7cNvv/0myYy6rkhnnnlmsXlhYWHq37+/3n///cLa3++9957y8vL8wvBff/1VP/30kxo1auQ3nX322ZKkvXv3SjIjzXv16qXbb79dsbGxuvHGG/XOO+8QjAMAAAAAgApHGA4ANundu7e2bt2qWbNm6dxzz9Urr7yi8847T6+88kphm3vvvVe//PKLUlNTFRkZqXHjxqldu3b64YcfTnr/JY0SLygoCDi/du3aAeffeOONOnz4sD7++GNJ0jvvvKO2bduqU6dOhW3cbrc6dOigJUuWBJzuuuuuwn18+eWXWrp0qW655RatX79eAwYM0J///OcS+wUAAAAAAHAiCMMB4CQ0atRIUVFR2rx5c7FlmzZtUkhIiOLj4wvn1a9fX0OGDNFbb72lnTt3qmPHjpo4caLfeq1atdJ9992nTz/9VBs2bFBubq6efvrpEvvgKc+yYcOGUvt6+umn69ChQ8Xm79ixo9T1iurdu7eaNGmiuXPnav/+/frss8/8RoV7nsPBgwd16aWXKikpqdjkO5I+JCREl156qaZNm6aff/5Z//jHP/TZZ58VllIBAAAAAACoCIThAHASQkNDddlll+n999/X9u3bC+dnZGRozpw5uvDCCxUdHS1JOnDggN+6derU0VlnnVVYbuTo0aM6fvy4X5tWrVqpbt26hW0CadSokXr37q1Zs2YpLS3Nb5llWX7byszM9CvNsmfPHs2fPz+o5xwSEqLrr79eH374od544w3l5+cXC8NvuOEG7dq1Sy+//HKx9Y8dO6bs7GxJ0sGDB4st79y5sySV+pwBAAAAAACC5bJ8kxIAgJ/Zs2dryJAhuvPOO9W0adNiy++55x6lpaUpMTFR9erV01133aWwsDC9+OKL2rVrl7744gslJiZKkmJjY3XxxRera9euql+/vr7//nu99NJLGjlypJ599lmtW7dOl156qW644Qa1b99eYWFhmj9/vpYsWaJ3331X/fv3L7Gf//vf/3ThhRcqIiJCw4cP15lnnqnt27dr4cKFWrdunSQTxrdo0UKxsbG6++67dfToUb3wwgtq1KiR1q5d6xecu1wujRgxQs8991zA/a1YsUIXXnih6tatq4SEBL+AXTJlUvr27auPP/64sC54QUGBNm3apHfeeUeffPKJunXrpnvvvVdffvmlrrrqKrVo0UJ79+7V888/L5fLpQ0bNigmJibYlwwAAAAAACCgMKc7AACnghdeeCHg/FtvvVXnnHOOvvrqK40dO1apqalyu91KTEzUf/7zn8IgXJLuvvtuffDBB/r000+Vk5OjFi1a6P/+7//0wAMPSJLi4+M1cOBALVu2TG+88YbCwsLUtm1bvfPOO6UG4ZLUqVMnffPNNxo3bpxeeOEFHT9+XC1atNANN9xQ2KZBgwaaP3++UlJS9OCDD+rMM89Uamqqfv31V61duzao43HBBRcoPj5eO3fuLDYqXDKjxxcsWKBnnnlGr7/+uubPn6+oqCi1bNlS99xzT+GFNK+55hpt375ds2bN0v79+9WwYUP16dNHkyZNIggHAAAAAAAVipHhAAAAAAAAAIBqj5rhAAAAAAAAAIBqjzAcAAAAAAAAAFDtEYYDAAAAAAAAAKo9wnAAAAAAJ+3LL79U37591bRpU7lcLi1YsKDMdZYvX67zzjtPEREROuusszR79uxK7ycAAABqLsJwAAAAACctOztbnTp10owZM8rVftu2bbrqqqt0ySWXaN26dbr33nt1++2365NPPqnkngIAAKCmclmWZTndCQAAAADVh8vl0vz589WvX78S24wZM0YLFy7Uhg0bCufdeOONOnTokBYvXmxDLwEAAFDThDndAbu43W7t3r1bdevWlcvlcro7AAAAqCCWZenw4cNq2rSpQkL44eOpYtWqVUpKSvKbl5ycrHvvvbfEdXJycpSTk1P42O126+DBg2rQoAHf8QEAAKqRyvqOX2PC8N27dys+Pt7pbgAAAKCS7Ny5U2eccYbT3UA5paenKzY21m9ebGyssrKydOzYMdWuXbvYOqmpqZo0aZJdXQQAAIDDKvo7fo0Jw+vWrSvJHMDo6GiHewMAAICKkpWVpfj4+MLve6i+xo4dq5SUlMLHmZmZat68Od/xAQAAqpnK+o5fY8Jwz88mo6Oj+aIMAABQDVEm49QSFxenjIwMv3kZGRmKjo4OOCpckiIiIhQREVFsPt/xAQAAqqeK/o5PUUUAAAAAtuvZs6eWLVvmN2/JkiXq2bOnQz0CAABAdUcYDgAAAOCkHTlyROvWrdO6deskSdu2bdO6deuUlpYmyZQ4GTRoUGH7O+64Q7/99psefPBBbdq0Sc8//7zeeecdjR492onuAwAAoAYgDAcAAABw0r7//nt16dJFXbp0kSSlpKSoS5cuGj9+vCRpz549hcG4JJ155plauHChlixZok6dOunpp5/WK6+8ouTkZEf6DwAAgOrPZVmW5XQn7JCVlaWYmBhlZmZSTxAAAJvk5+crNzfX6W6gGoiMjFRISOBxHHzPq7l47QEAAKqnyvqeV2MuoAkAAOxjWZbS0tK0f/9+p7uCaiIkJETt27cPePFEAAAAACgPwnAAAFDhPEF4s2bNVKdOnRJH9ALl4Xa7tW3bNm3fvl1nn312hV9RHgAAAEDNQBgOAAAqVH5+fmEQHhcX53R3UE00a9ZM27Zt0w8//KBzzjmHEeIAAAAAgsYwLQAAUKE8NcLr1KnjcE9QnXjC75UrV+qTTz5RTk6Owz0CAAAAcKohDAcAAJWC0iioSJ7SKA0aNNCPP/6oNWvWONwjAAAAAKca/pcKAACAU0Z4eLjCw8O5OCsAAACAoBGGAwAAVKKEhARNnz693O2XL18ul8ulQ4cOVVqfJGn27NmqV69epe6jsoSEhBSW4wEAAACA8iIMBwAAkCnDUdo0ceLEE9rud999p+HDh5e7/QUXXKA9e/YoJibmhPYHAAAAAAgszOkOAAAAVAV79uwpvD937lyNHz9emzdvLpzne0FQy7JUUFCgsLCyv0o1atQoqH6Eh4crLi4uqHUAAAAAAGVjZDgAAICkuLi4wikmJkYul6vw8aZNm1S3bl19/PHH6tq1qyIiIvT1119r69at+stf/qLY2FjVqVNH3bt319KlS/22W7RMisvl0iuvvKJrr71WUVFRat26tT744IPC5UXLpHjKmXzyySdq166d6tSpo8svv9wvvM/Pz9fdd9+tevXqqUGDBhozZowGDx6sfv36BXUMXnjhBbVq1Urh4eFq06aN3njjjcJllmVp4sSJat68uSIiItS0aVPdfffdhcuff/55tW7dWpGRkYqNjdX1118f1L4BAAAAoLIRhgMAgEpnWVJ2tjOTZVXc83jooYc0efJkbdy4UR07dtSRI0d05ZVXatmyZfrhhx90+eWXq2/fvkpLSyt1O5MmTdINN9yg9evX68orr9TNN9+sgwcPltj+6NGjmjp1qt544w19+eWXSktL0/3331+4/Mknn9Sbb76p1157TStWrFBWVpYWLFgQ1HObP3++7rnnHt13333asGGD/v73v2vIkCH6/PPPJUn//e9/9cwzz+jFF1/Ur7/+qgULFqhDhw6SpO+//1533323HnvsMW3evFmLFy9W7969g9o/AAAAAFQ2yqQAAIBKd/So5FNlxFZHjkinnVYx23rsscf05z//ufBx/fr11alTp8LHjz/+uObPn68PPvhAI0eOLHE7t956qwYOHChJeuKJJ/Tss89q9erVuvzyywO2z8vL08yZM9WqVStJ0siRI/XYY48VLv/Xv/6lsWPH6tprr5UkPffcc1q0aFFQz23q1Km69dZbddddd0mSUlJS9M0332jq1Km65JJLlJaWpri4OCUlJalWrVpq3ry5evToIUlKS0vTaaedpquvvlp169ZVixYt1KVLl6D2DwAAAACVjZHhAAAA5dStWze/x0eOHNH999+vdu3aqV69eqpTp442btxY5sjwjh07Ft4/7bTTFB0drb1795bYPioqqjAIl6QmTZoUts/MzFRGRkZhMC1JoaGh6tq1a1DPbePGjerVq5ffvF69emnjxo2SpL/+9a86duyYWrZsqWHDhmn+/PnKz8+XJP35z39WixYt1LJlS91yyy168803dfTo0aD2DwAAAACVjTC8Mv3+u/TXv0pff+10TwAAcFRUlBmh7cQUFVVxz+O0IkPM77//fs2fP19PPPGEvvrqK61bt04dOnRQbm5uqdupVauW32OXyyW32x1Ue6si67+UQ3x8vDZv3qznn39etWvX1l133aXevXsrLy9PdevW1dq1a/XWW2+pSZMmGj9+vDp16lRY9xwAAAAAqgLC8Mo0YoT07rvSRRc53RMAABzlcplSJU5MLlflPa8VK1bo1ltv1bXXXqsOHTooLi5O27dvr7wdBhATE6PY2Fh99913hfMKCgq0du3aoLbTrl07rVixwm/eihUr1L59+8LHtWvXVt++ffXss89q+fLlWrVqlX788UdJUlhYmJKSkvTUU09p/fr12r59uz777LOTeGYAAAAAULGoGV6Zfv7Z6R4AAIBK1Lp1a7333nvq27evXC6Xxo0bV+oI78oyatQopaam6qyzzlLbtm31r3/9S3/88YdcQZwJeOCBB3TDDTeoS5cuSkpK0ocffqj33ntPS5culSTNnj1bBQUFSkxMVFRUlP7zn/+odu3aatGihT766CP99ttv6t27t04//XQtWrRIbrdbbdq0qaynDAAAAABBIwyvTGEcXgAAqrNp06bptttu0wUXXKCGDRtqzJgxysrKsr0fY8aMUXp6ugYNGqTQ0FANHz5cycnJCg0NLfc2+vXrp3/+85+aOnWq7rnnHp155pl67bXXdPHFF0uS6tWrp8mTJyslJUUFBQXq0KGDPvzwQzVo0ED16tXTe++9p4kTJ+r48eNq3bq13nrrLZ1zzjmV9IwBAAAAIHguy+6Ckw7JyspSTEyMMjMzFR0dbc9OO3SQNmww9wsKpBCq0gAAqr+jR49q48aNateunaIqsmA3ys3tdqtdu3a64YYb9PjjjzvdnQrheV9t375d27dvV4sWLXT99ddLcuh7HqoEXnsAAIDqqbK+5zF0uTL5/jR5/36pcWPn+gIAAKqtHTt26NNPP1WfPn2Uk5Oj5557Ttu2bdNNN93kdNcAAAAAoMpgqHJl+uMP7/1du5zrBwAAqNZCQkI0e/Zsde/eXb169dKPP/6opUuXql27dk53DQAAAACqDEaGVxbLkvbu9T4+eNC5vgAAgGotPj5eK1ascLobAAAAAFClMTK8shw7JrVs6X186JBjXQEAAAAAAACAmo4wvLJERUkbN0pXXmkeZ2Y62x8AAAAAAAAAqMEIwytbTIy5JQwHAAAAAAAAAMcQhlc2wnAAAAAAAAAAcBxheGXzhOHUDAcAAAAAAAAAxxCGV7Z69cwtI8MBAAAAAAAAwDGE4ZWNMikAANQoF198se69997CxwkJCZo+fXqp67hcLi1YsOCk911R2ynNxIkT1blz50rdBwAAAABUBsLwykaZFAAATgl9+/bV5ZdfHnDZV199JZfLpfXr1we93e+++07Dhw8/2e75KSmQ3rNnj6644ooK3RcAAAAAVBeE4ZXNE4ZnZTnbDwAAUKqhQ4dqyZIl+v3334ste+2119StWzd17Ngx6O02atRIUVFRFdHFMsXFxSkiIsKWfQEAAADAqYYwvLJ5/vN77Jiz/QAAAKW6+uqr1ahRI82ePdtv/pEjRzRv3jwNHTpUBw4c0MCBA9WsWTNFRUWpQ4cOeuutt0rdbtEyKb/++qt69+6tyMhItW/fXkuWLCm2zpgxY3T22WcrKipKLVu21Lhx45SXlydJmj17tiZNmqT//e9/crlccrlchX0uWiblxx9/1J/+9CfVrl1bDRo00PDhw3XkyJHC5bfeeqv69eunqVOnqkmTJmrQoIFGjBhRuK/ycLvdeuyxx3TGGWcoIiJCnTt31uLFiwuX5+bmauTIkWrSpIkiIyPVokULpaamSpIsy9LEiRPVvHlzRUREqGnTprr77rvLvW8AAAAACEaY0x2o9mrXNrdHjzrbDwAAnGRZzv1bGBUluVxlNgsLC9OgQYM0e/ZsPfLII3L9/3XmzZungoICDRw4UEeOHFHXrl01ZswYRUdHa+HChbrlllvUqlUr9ejRo8x9uN1uXXfddYqNjdW3336rzMxMv/riHnXr1tXs2bPVtGlT/fjjjxo2bJjq1q2rBx98UAMGDNCGDRu0ePFiLV26VJIU4/klmo/s7GwlJyerZ8+e+u6777R3717dfvvtGjlypF/g//nnn6tJkyb6/PPPtWXLFg0YMECdO3fWsGHDynw+kvTPf/5TTz/9tF588UV16dJFs2bN0jXXXKOffvpJrVu31rPPPqsPPvhA77zzjpo3b66dO3dq586dkqT//ve/euaZZ/T222/rnHPOUXp6uv73v/+Va78AAAAAECzC8MrmCcMZGQ4AqMmOHpXq1HFm30eOSKedVq6mt912m6ZMmaIvvvhCF198sSRTIqV///6KiYlRTEyM7r///sL2o0aN0ieffKJ33nmnXGH40qVLtWnTJn3yySdq2rSpJOmJJ54oVuf70UcfLbyfkJCg+++/X2+//bYefPBB1a5dW3Xq1FFYWJji4uJK3NecOXN0/Phxvf766zrt/z//5557Tn379tWTTz6p2NhYSdLpp5+u5557TqGhoWrbtq2uuuoqLVu2rNxh+NSpUzVmzBjdeOONkqQnn3xSn3/+uaZPn64ZM2YoLS1NrVu31oUXXiiXy6UWLVoUrpuWlqa4uDglJSWpVq1aat68ebmOIwAAAACcCMqkVDbKpAAAcMpo27atLrjgAs2aNUuStGXLFn311VcaOnSoJKmgoECPP/64OnTooPr166tOnTr65JNPlJaWVq7tb9y4UfHx8YVBuCT17NmzWLu5c+eqV69eiouLU506dfToo4+Wex++++rUqVNhEC5JvXr1ktvt1ubNmwvnnXPOOQoNDS183KRJE+3du7dc+8jKytLu3bvVq1cvv/m9evXSxo0bJZlSLOvWrVObNm10991369NPPy1s99e//lXHjh1Ty5YtNWzYMM2fP1/5+flBPU8AAAAAKC/C8MpGmRQAAMzJ4SNHnJmCvHjl0KFD9d///leHDx/Wa6+9platWqlPnz6SpClTpuif//ynxowZo88//1zr1q1TcnKycnNzK+xQrVq1SjfffLOuvPJKffTRR/rhhx/0yCOPVOg+fNWqVcvvscvlktvtrrDtn3feedq2bZsef/xxHTt2TDfccIOuv/56SVJ8fLw2b96s559/XrVr19Zdd92l3r17B1WzHAAAAADKizIplc0ThufnmymMQw4AqIFcrnKXKnHaDTfcoHvuuUdz5szR66+/rjvvvLOwfviKFSv0l7/8RX/7298kmRrgv/zyi9q3b1+ubbdr1047d+7Unj171KRJE0nSN99849dm5cqVatGihR555JHCeTt27PBrEx4eroKCgjL3NXv2bGVnZxeODl+xYoVCQkLUpk2bcvW3LNHR0WratKlWrFhReMLAsx/fcifR0dEaMGCABgwYoOuvv16XX365Dh48qPr166t27drq27ev+vbtqxEjRqht27b68ccfdd5551VIHwEAAADAg2S2svmORjt2TKpb17m+AACAMtWpU0cDBgzQ2LFjlZWVpVtvvbVwWevWrfXuu+9q5cqVOv300zVt2jRlZGSUOwxPSkrS2WefrcGDB2vKlCnKysryC709+0hLS9Pbb7+t7t27a+HChZo/f75fm4SEBG3btk3r1q3TGWecobp16yoiIsKvzc0336wJEyZo8ODBmjhxovbt26dRo0bplltuKawXXhEeeOABTZgwQa1atVLnzp312muvad26dXrzzTclSdOmTVOTJk3UpUsXhYSEaN68eYqLi1O9evU0e/ZsFRQUKDExUVFRUfrPf/6j2rVr+9UVBwAAAICKQpmUyhYZ6b1P3XAAAE4JQ4cO1R9//KHk5GS/+t6PPvqozjvvPCUnJ+viiy9WXFyc+vXrV+7thoSEaP78+Tp27Jh69Oih22+/Xf/4xz/82lxzzTUaPXq0Ro4cqc6dO2vlypUaN26cX5v+/fvr8ssv1yWXXKJGjRrprbfeKravqKgoffLJJzp48KC6d++u66+/Xpdeeqmee+654A5GGe6++26lpKTovvvuU4cOHbR48WJ98MEHat26tSSpbt26euqpp9StWzd1795d27dv16JFixQSEqJ69erp5ZdfVq9evdSxY0ctXbpUH374oRo0aFChfQQAAAAASXJZlmU53Qk7ZGVlKSYmRpmZmYqOjrZ357VrS8ePS9u2SQkJ9u4bAACbHT16VBs3blS7du0UFWS9bqAknvfV9u3btX37drVo0aKw9rij3/PgKF57AACA6qmyvued0MjwGTNmKCEhQZGRkUpMTNTq1atLbPvTTz+pf//+SkhIkMvl0vTp04u18SwrOo0YMaKwzcUXX1xs+R133HEi3befJwhgZDgAAAAAAAAAOCLoMHzu3LlKSUnRhAkTtHbtWnXq1EnJycnau3dvwPZHjx5Vy5YtNXnyZMXFxQVs891332nPnj2F05IlSyRJf/3rX/3aDRs2zK/dU089FWz3neG5iCZhOAAAAAAAAAA4IugwfNq0aRo2bJiGDBmi9u3ba+bMmYqKitKsWbMCtu/evbumTJmiG2+8sdiFnTwaNWqkuLi4wumjjz5Sq1at1KdPH792UVFRfu1OmZ9CesLwo0ed7QcAAAAAAAAA1FBBheG5ublas2aNkpKSvBsICVFSUpJWrVpVIR3Kzc3Vf/7zH912221yuVx+y9588001bNhQ5557rsaOHaujpYTLOTk5ysrK8pscQ5kUAAAAAAAAAHBUWDCN9+/fr4KCAsXGxvrNj42N1aZNmyqkQwsWLNChQ4d06623+s2/6aab1KJFCzVt2lTr16/XmDFjtHnzZr333nsBt5OamqpJkyZVSJ9OGmVSAAAAAAAAAMBRQYXhdnj11Vd1xRVXqGnTpn7zhw8fXni/Q4cOatKkiS699FJt3bpVrVq1KradsWPHKiUlpfBxVlaW4uPjK6/jpaFMCgCgBnK73U53AdUI7ycAAAAAJyuoMLxhw4YKDQ1VRkaG3/yMjIwSL44ZjB07dmjp0qUljvb2lZiYKEnasmVLwDA8IiKixBrltouMNLc5Oc72AwAAG0RGRiokJETbtm1Ts2bNFBERUaz0GRAMt9ut3bt3y7Is5eXlOd0dAAAAAKeooMLw8PBwde3aVcuWLVO/fv0kmf+cLFu2TCNHjjzpzrz22mtq3LixrrrqqjLbrlu3TpLUpEmTk95vpQv7/4e5oMDZfgAAYIOQkBC1b99e27dv17Zt25zuDqoJy7L0+++/y+12y+12q1atWk53CQAAAMApJugyKSkpKRo8eLC6deumHj16aPr06crOztaQIUMkSYMGDVKzZs2UmpoqyVwQ8+effy68v2vXLq1bt0516tTRWWedVbhdt9ut1157TYMHD1ZYmH+3tm7dqjlz5ujKK69UgwYNtH79eo0ePVq9e/dWx44dT/jJ28bzfPLzne0HAAA2iYiI0Nlnn609e/bo448/liTVq1fP2U7hlJaXlye3262cnBzl5eWpQYMGTncJAAAAwCkm6DB8wIAB2rdvn8aPH6/09HR17txZixcvLryoZlpamkJCQgrb7969W126dCl8PHXqVE2dOlV9+vTR8uXLC+cvXbpUaWlpuu2224rtMzw8XEuXLi0M3uPj49W/f389+uijwXbfGZ4wnJ/1AgBqEJfLpSZNmqhr16767LPPtHfvXr/vCECwLMuS2+1W27Ztdd555zndHQAAAACnmBO6gObIkSNLLIviG3BLUkJCgizLKnObl112WYnt4uPj9cUXXwTdzyqDkeEAgBrK5XKpU6dOql27tjIyMnTs2DGnu4RTWFhYmKKjo3XOOecoKirK6e4AAAAAOMWcUBiOIBGGAwBqMJfLpTZt2qhNmzZOdwUAAAAAUIPxW2U7EIYDAAAAAAAAgKMIw+1Qq5a5JQwHAAAAAAAAAEcQhtuBkeEAAAAAAAAA4CjCcDsQhgMAAAAAAACAowjD7UAYDgAAAAAAAACOIgy3A2E4AAAAAAAAADiKMNwOhOEAAAAAAAAA4CjCcDsQhgMAAAAAAACAowjD7UAYDgAAAAAAAACOIgy3A2E4AAAAAAAAADiKMNwOnjA8L8/ZfgAAAAAAAABADUUYbgdGhgMAAAAAAACAowjD7UAYDgAAAAAAAACOIgy3A2E4AAAAAAAAADiKMNwOtWqZW8JwAAAAAAAAAHAEYbgdGBkOAAAAAAAAAI4iDLcDYTgAAAAAAAAAOIow3A6E4QAAAAAAAADgKMJwOxCGAwAAAAAAAICjCMPtQBgOAAAAAAAAAI4iDLcDYTgAAAAAAAAAOIow3A6E4QAAAAAAAADgKMJwOxCGAwAAAAAAAICjCMPtQBgOAAAAAAAAAI4iDLeDJwzPy3O2HwAAAAAAAABQQxGG24GR4QAAAAAAAADgKMJwOxCGAwAAAAAAAICjCMPtUKuWuSUMBwAAAAAAAABHEIbbgZHhAAAAAAAAAOAownA7EIYDAAAAAAAAgKMIw+1AGA4AAIAaYMaMGUpISFBkZKQSExO1evXqUttPnz5dbdq0Ue3atRUfH6/Ro0fr+PHjNvUWAAAANQ1huB0IwwEAAFDNzZ07VykpKZowYYLWrl2rTp06KTk5WXv37g3Yfs6cOXrooYc0YcIEbdy4Ua+++qrmzp2rhx9+2OaeAwAAoKYgDLcDYTgAAACquWnTpmnYsGEaMmSI2rdvr5kzZyoqKkqzZs0K2H7lypXq1auXbrrpJiUkJOiyyy7TwIEDyxxNDgAAAJwownA7EIYDAACgGsvNzdWaNWuUlJRUOC8kJERJSUlatWpVwHUuuOACrVmzpjD8/u2337Ro0SJdeeWVtvQZAAAANU+Y0x2oETxhuNttphDOQQAAAKD62L9/vwoKChQbG+s3PzY2Vps2bQq4zk033aT9+/frwgsvlGVZys/P1x133FFqmZScnBzl5OQUPs7KyqqYJwAAAIAagVTWDmE+5xwKCpzrBwAAAFBFLF++XE888YSef/55rV27Vu+9954WLlyoxx9/vMR1UlNTFRMTUzjFx8fb2GMAAACc6hgZbgffMDw/X6pVy7m+AAAAABWsYcOGCg0NVUZGht/8jIwMxcXFBVxn3LhxuuWWW3T77bdLkjp06KDs7GwNHz5cjzzyiEIC/Jpy7NixSklJKXyclZVFIA4AAIByY2S4HXzD8Lw85/oBAAAAVILw8HB17dpVy5YtK5zndru1bNky9ezZM+A6R48eLRZ4h4aGSpIsywq4TkREhKKjo/0mAAAAoLwYGW6HoiPDAQAAgGomJSVFgwcPVrdu3dSjRw9Nnz5d2dnZGjJkiCRp0KBBatasmVJTUyVJffv21bRp09SlSxclJiZqy5YtGjdunPr27VsYigMAAAAViTDcDr5f5gnDAQAAUA0NGDBA+/bt0/jx45Wenq7OnTtr8eLFhRfVTEtL8xsJ/uijj8rlcunRRx/Vrl271KhRI/Xt21f/+Mc/nHoKAAAAqOZcVkm/QaxmsrKyFBMTo8zMTGd+TlmrlgnCd+2Smja1f/8AAADVlOPf8+AYXnsAAIDqqbK+51Ez3C6eUimMDAcAAAAAAAAA2xGG24UwHAAAAAAAAAAcQxhuF8JwAAAAAAAAAHDMCYXhM2bMUEJCgiIjI5WYmKjVq1eX2Pann35S//79lZCQIJfLpenTpxdrM3HiRLlcLr+pbdu2fm2OHz+uESNGqEGDBqpTp4769++vjIyME+m+MwjDAQAAAAAAAMAxQYfhc+fOVUpKiiZMmKC1a9eqU6dOSk5O1t69ewO2P3r0qFq2bKnJkycrLi6uxO2ec8452rNnT+H09ddf+y0fPXq0PvzwQ82bN09ffPGFdu/ereuuuy7Y7juHMBwAAAAAAAAAHBN0GD5t2jQNGzZMQ4YMUfv27TVz5kxFRUVp1qxZAdt3795dU6ZM0Y033qiIiIgStxsWFqa4uLjCqWHDhoXLMjMz9eqrr2ratGn605/+pK5du+q1117TypUr9c033wT7FJxBGA4AAAAAAAAAjgkqDM/NzdWaNWuUlJTk3UBIiJKSkrRq1aqT6sivv/6qpk2bqmXLlrr55puVlpZWuGzNmjXKy8vz22/btm3VvHnzEvebk5OjrKwsv8lRhOEAAAAAAAAA4JigwvD9+/eroKBAsbGxfvNjY2OVnp5+wp1ITEzU7NmztXjxYr3wwgvatm2bLrroIh0+fFiSlJ6ervDwcNWrV6/c+01NTVVMTEzhFB8ff8L9qxCE4QAAAAAAAADgmBO6gGZFu+KKK/TXv/5VHTt2VHJyshYtWqRDhw7pnXfeOeFtjh07VpmZmYXTzp07K7DHJ4AwHAAAAAAAAAAcExZM44YNGyo0NFQZGRl+8zMyMkq9OGaw6tWrp7PPPltbtmyRJMXFxSk3N1eHDh3yGx1e2n4jIiJKrVFuO08YnpfnbD8AAAAAAAAAoAYKamR4eHi4unbtqmXLlhXOc7vdWrZsmXr27FlhnTpy5Ii2bt2qJk2aSJK6du2qWrVq+e138+bNSktLq9D9VipGhgMAAAAAAACAY4IaGS5JKSkpGjx4sLp166YePXpo+vTpys7O1pAhQyRJgwYNUrNmzZSamirJXHTz559/Lry/a9curVu3TnXq1NFZZ50lSbr//vvVt29ftWjRQrt379aECRMUGhqqgQMHSpJiYmI0dOhQpaSkqH79+oqOjtaoUaPUs2dPnX/++RVyICodYTgAAAAAAAAAOCboMHzAgAHat2+fxo8fr/T0dHXu3FmLFy8uvKhmWlqaQkK8A853796tLl26FD6eOnWqpk6dqj59+mj58uWSpN9//10DBw7UgQMH1KhRI1144YX65ptv1KhRo8L1nnnmGYWEhKh///7KyclRcnKynn/++RN93varVcvcEoYDAAAAAAAAgO1clmVZTnfCDllZWYqJiVFmZqaio6Pt70Dv3tJXX0nvviv172///gEAAKopx7/nwTG89gAAANVTZX3PC6pmOE4CZVIAAAAAAAAAwDGE4XYhDAcAAAAAAAAAxxCG24UwHAAAAAAAAAAcQxhuF8JwAAAAAAAAAHAMYbhdCMMBAAAAAAAAwDGE4XYhDAcAAAAAAAAAxxCG24UwHAAAAAAAAAAcQxhuF8JwAAAAAAAAAHAMYbhdPGF4Xp6z/QAAAAAAAACAGogw3C6MDAcAAAAAAAAAxxCG24UwHAAAAAAAAAAcQxhuF08YXlDgbD8AAAAAAAAAoAYiDLcLI8MBAAAAAAAAwDGE4XYhDAcAAAAAAAAAxxCG2yU01NwShgMAAAAAAACA7QjD7cLIcAAAAAAAAABwDGG4XbiAJgAAAAAAAAA4hjDcLowMBwAAAAAAAADHEIbbhZrhAAAAAAAAAOAYwnC7MDIcAAAAAAAAABxDGG4XaoYDAAAAAAAAgGMIw+3CyHAAAAAAAAAAcAxhuF2oGQ4AAAAAAAAAjiEMtwsjwwEAAAAAAADAMYThdqFmOAAAAAAAAAA4hjDcLowMBwAAAAAAAADHEIbbhTAcAAAAAAAAABxDGG4XLqAJAAAAAAAAAI4hDLcLNcMBAAAAAAAAwDGE4XahTAoAAAAAAAAAOIYw3C6E4QAAAAAAAADgGMJwu1AzHAAAAAAAAAAcQxhuF2qGAwAAAAAAAIBjCMPtQpkUAAAAAAAAAHAMYbhdCMMBAAAAAAAAwDGE4XahZjgAAAAAAAAAOIYw3C7UDAcAAAAAAAAAxxCG24UyKQAAAAAAAADgGMJwuxCGAwAAAAAAAIBjCMPtQs1wAAAAAAAAAHAMYbhdGBkOAAAAAAAAAI4hDLcLF9AEAAAAAAAAAMcQhtuFkeEAAAAAAAAA4BjCcLtQMxwAAAAAAAAAHEMYbhffMimW5WxfAAAAAAAAAKCGOaEwfMaMGUpISFBkZKQSExO1evXqEtv+9NNP6t+/vxISEuRyuTR9+vRibVJTU9W9e3fVrVtXjRs3Vr9+/bR582a/NhdffLFcLpffdMcdd5xI953hCcMlye12rh8AAAAAAAAAUAMFHYbPnTtXKSkpmjBhgtauXatOnTopOTlZe/fuDdj+6NGjatmypSZPnqy4uLiAbb744guNGDFC33zzjZYsWaK8vDxddtllys7O9ms3bNgw7dmzp3B66qmngu2+c3zDcEqlAAAAAAAAAICtwspu4m/atGkaNmyYhgwZIkmaOXOmFi5cqFmzZumhhx4q1r579+7q3r27JAVcLkmLFy/2ezx79mw1btxYa9asUe/evQvnR0VFlRioV3memuGSCcMjIpzrCwAAAAAAAADUMEGNDM/NzdWaNWuUlJTk3UBIiJKSkrRq1aoK61RmZqYkqX79+n7z33zzTTVs2FDnnnuuxo4dq6NHj5a4jZycHGVlZflNjmJkOAAAAAAAAAA4JqiR4fv371dBQYFiY2P95sfGxmrTpk0V0iG32617771XvXr10rnnnls4/6abblKLFi3UtGlTrV+/XmPGjNHmzZv13nvvBdxOamqqJk2aVCF9qhC+YXhBgXP9AAAAAAAAAIAaKOgyKZVtxIgR2rBhg77++mu/+cOHDy+836FDBzVp0kSXXnqptm7dqlatWhXbztixY5WSklL4OCsrS/Hx8ZXX8bKE+AzCZ2Q4AAAAAAAAANgqqDC8YcOGCg0NVUZGht/8jIyMCqnlPXLkSH300Uf68ssvdcYZZ5TaNjExUZK0ZcuWgGF4RESEIqpSXW6Xy9QNLyggDAcAAAAAAAAAmwVVMzw8PFxdu3bVsmXLCue53W4tW7ZMPXv2POFOWJalkSNHav78+frss8905plnlrnOunXrJElNmjQ54f3azlMqhTAcAAAAAAAAAGwVdJmUlJQUDR48WN26dVOPHj00ffp0ZWdna8iQIZKkQYMGqVmzZkpNTZVkLrr5888/F97ftWuX1q1bpzp16uiss86SZEqjzJkzR++//77q1q2r9PR0SVJMTIxq166trVu3as6cObryyivVoEEDrV+/XqNHj1bv3r3VsWPHCjkQtggLk3JyqBkOAAAAAAAAADYLOgwfMGCA9u3bp/Hjxys9PV2dO3fW4sWLCy+qmZaWphCf+ti7d+9Wly5dCh9PnTpVU6dOVZ8+fbR8+XJJ0gsvvCBJuvjii/329dprr+nWW29VeHi4li5dWhi8x8fHq3///nr00UeD7b6zGBkOAAAAAAAAAI5wWZZlOd0JO2RlZSkmJkaZmZmKjo52phMNG0oHDkg//yy1a+dMHwAAAKqZKvE9D47gtQcAAKieKut7XlA1w3GSQkPNLSPDAQAAAAAAAMBWhOF28pRJoWY4AAAAAAAAANiKMNxO1AwHAAAAAAAAAEcQhtuJMBwAAAAAAAAAHEEYbidqhgMAAAAAAACAIwjD7UTNcAAAAAAAAABwBGG4nSiTAgAAAAAAAACOIAy3E2E4AAAAAAAAADiCMNxO1AwHAAAAAAAAAEcQhtuJmuEAAAAAAAAA4AjCcDtRJgUAAAAAAAAAHEEYbifCcAAAAAAAAABwBGG4nagZDgAAgGpsxowZSkhIUGRkpBITE7V69epS2x86dEgjRoxQkyZNFBERobPPPluLFi2yqbcAAACoacKc7kCNwshwAAAAVFNz585VSkqKZs6cqcTERE2fPl3JycnavHmzGjduXKx9bm6u/vznP6tx48Z699131axZM+3YsUP16tWzv/MAAACoEQjD7cQFNAEAAFBNTZs2TcOGDdOQIUMkSTNnztTChQs1a9YsPfTQQ8Xaz5o1SwcPHtTKlStVq1YtSVJCQoKdXQYAAEANQ5kUOzEyHAAAANVQbm6u1qxZo6SkpMJ5ISEhSkpK0qpVqwKu88EHH6hnz54aMWKEYmNjde655+qJJ55QQSkDR3JycpSVleU3AQAAAOVFGG4naoYDAACgGtq/f78KCgoUGxvrNz82Nlbp6ekB1/ntt9/07rvvqqCgQIsWLdK4ceP09NNP6//+7/9K3E9qaqpiYmIKp/j4+Ap9HgAAAKjeCMPtxMhwAAAAQJLkdrvVuHFjvfTSS+ratasGDBigRx55RDNnzixxnbFjxyozM7Nw2rlzp409BgAAwKmOmuF2omY4AAAAqqGGDRsqNDRUGRkZfvMzMjIUFxcXcJ0mTZqoVq1aCvX8elJSu3btlJ6ertzcXIWHhxdbJyIiQhERERXbeQAAANQYjAy3EyPDAQAAUA2Fh4era9euWrZsWeE8t9utZcuWqWfPngHX6dWrl7Zs2SK3210475dfflGTJk0CBuEAAADAySIMtxM1wwEAAFBNpaSk6OWXX9a///1vbdy4UXfeeaeys7M1ZMgQSdKgQYM0duzYwvZ33nmnDh48qHvuuUe//PKLFi5cqCeeeEIjRoxw6ikAAACgmqNMip0YGQ4AAIBqasCAAdq3b5/Gjx+v9PR0de7cWYsXLy68qGZaWppCQrxjceLj4/XJJ59o9OjR6tixo5o1a6Z77rlHY8aMceopAAAAoJojDLcTNcMBAABQjY0cOVIjR44MuGz58uXF5vXs2VPffPNNJfcKAAAAMCiTYidGhgMAAAAAAACAIwjD7UQYDgAAAAAAAACOIAy3ExfQBAAAAAAAAABHEIbbiZrhAAAAAAAAAOAIwnA7USYFAAAAAAAAABxBGG4nwnAAAAAAAAAAcARhuJ2oGQ4AAAAAAAAAjiAMtxM1wwEAAAAAAADAEYThdvKE4Xl5zvYDAAAAAAAAAGoYwnA7UTMcAAAAAAAAABxBGG4nwnAAAAAAAAAAcARhuJ1q1TK3hOEAAAAAAAAAYCvCcDtRMxwAAAAAAAAAHEEYbidGhgMAAAAAAACAIwjD7cTIcAAAAAAAAABwBGG4nRgZDgAAAAAAAACOIAy3EyPDAQAAAAAAAMARhOF2YmQ4AAAAAAAAADiCMNxOjAwHAAAAAAAAAEcQhtuJkeEAAAAAAAAA4AjCcDsxMhwAAAAAAAAAHEEYbidPGM7IcAAAAAAAAACw1QmF4TNmzFBCQoIiIyOVmJio1atXl9j2p59+Uv/+/ZWQkCCXy6Xp06ef0DaPHz+uESNGqEGDBqpTp4769++vjIyME+m+czxlUhgZDgAAAAAAAAC2CjoMnzt3rlJSUjRhwgStXbtWnTp1UnJysvbu3Ruw/dGjR9WyZUtNnjxZcXFxJ7zN0aNH68MPP9S8efP0xRdfaPfu3bruuuuC7b6zGBkOAAAAAAAAAI5wWZZlBbNCYmKiunfvrueee06S5Ha7FR8fr1GjRumhhx4qdd2EhATde++9uvfee4PaZmZmpho1aqQ5c+bo+uuvlyRt2rRJ7dq106pVq3T++eeX2e+srCzFxMQoMzNT0dHRwTzlivPjj1LHjlLjxtKpNqodAACgiqoS3/PgCF57AACA6qmyvucFNTI8NzdXa9asUVJSkncDISFKSkrSqlWrTqgD5dnmmjVrlJeX59embdu2at68eYn7zcnJUVZWlt/kOEaGAwAAAAAAAIAjggrD9+/fr4KCAsXGxvrNj42NVXp6+gl1oDzbTE9PV3h4uOrVq1fu/aampiomJqZwio+PP6H+VShPzXDCcAAAAAAAAACw1QldQPNUMHbsWGVmZhZOO3fudLpL3pHhXEATAAAAAAAAAGwVFkzjhg0bKjQ0VBlF6l1nZGSUeHHMithmXFyccnNzdejQIb/R4aXtNyIiQhERESfUp0rDyHAAAAAAAAAAcERQI8PDw8PVtWtXLVu2rHCe2+3WsmXL1LNnzxPqQHm22bVrV9WqVcuvzebNm5WWlnbC+3WE78jw4K5bCgAAAAAAAAA4CUGNDJeklJQUDR48WN26dVOPHj00ffp0ZWdna8iQIZKkQYMGqVmzZkpNTZVkLpD5888/F97ftWuX1q1bpzp16uiss84q1zZjYmI0dOhQpaSkqH79+oqOjtaoUaPUs2dPnX/++RVyIGzhGRkuSW63FBrqXF8AAAAAAAAAoAYJOgwfMGCA9u3bp/Hjxys9PV2dO3fW4sWLCy+AmZaWppAQ74Dz3bt3q0uXLoWPp06dqqlTp6pPnz5avnx5ubYpSc8884xCQkLUv39/5eTkKDk5Wc8///yJPm9nhPkc7rw8wnAAAAAAAAAAsInLsmpGvY6srCzFxMQoMzNT0dHRznTi2DEpKsrcP3xYqlPHmX4AAABUI1Xiex4cwWsPAABQPVXW97ygaobjJBUdGQ4AAAAAAAAAsAVhuJ18w/D8fOf6AQAAAAAAAAA1DGG4nVwub51wRoYDAAAAAAAAgG0Iw+1Wq5a5ZWQ4AAAAAAAAANiGMNxunlIpjAwHAAAAAAAAANsQhtvNE4YzMhwAAAAAAAAAbEMYbjdPmRRGhgMAAAAAAACAbQjD7cbIcAAAAAAAAACwHWG43biAJgAAAAAAAADYjjDcblxAEwAAAAAAAABsRxhuN0aGAwAAAAAAAIDtCMPtxshwAAAAAAAAALAdYbjdGBkOAAAAAAAAALYjDLcbI8MBAAAAAAAAwHaE4XZjZDgAAAAAAAAA2I4w3G6MDAcAAAAAAAAA2xGG242R4QAAAAAAAABgO8JwuzEyHAAAAAAAAABsRxhuN0aGAwAAAAAAAIDtCMPtxshwAAAAAAAAALAdYbjdPGE4I8MBAAAAAAAAwDaE4XbzlElhZDgAAAAAAAAA2IYw3G6MDAcAAAAAAAAA2xGG242R4QAAAAAAAABgO8JwuzEyHAAAAAAAAABsRxhuN8/IcMJwAAAAAAAAALANYbjdPCPDKZMCAAAAAAAAALYhDLcbI8MBAAAAAAAAwHaE4XZjZDgAAAAAAAAA2I4w3G6MDAcAAAAAAAAA2xGG242R4QAAAAAAAABgO8JwuzEyHAAAAAAAAABsRxhuN0aGAwAAAAAAAIDtCMPtxshwAAAAAAAAALAdYbjdGBkOAAAAAAAAALYjDLebJwxnZDgAAAAAAAAA2IYw3G6eMimMDAcAAAAAAAAA2xCG242R4QAAAAAAAABgO8JwuzEyHAAAAAAAAABsRxhuN0aGAwAAAAAAAIDtCMPt5hkZThgOAAAAAAAAALYhDLebJwzPzXW2HwAAAAAAAABQgxCG2y083NxSMxwAAAAAAAAAbEMYbjdGhgMAAAAAAACA7U4oDJ8xY4YSEhIUGRmpxMRErV69utT28+bNU9u2bRUZGakOHTpo0aJFfstdLlfAacqUKYVtEhISii2fPHnyiXTfWZ6R4YThAAAAAAAAAGCboMPwuXPnKiUlRRMmTNDatWvVqVMnJScna+/evQHbr1y5UgMHDtTQoUP1ww8/qF+/furXr582bNhQ2GbPnj1+06xZs+RyudS/f3+/bT322GN+7UaNGhVs951HmRQAAAAAAAAAsF3QYfi0adM0bNgwDRkyRO3bt9fMmTMVFRWlWbNmBWz/z3/+U5dffrkeeOABtWvXTo8//rjOO+88Pffcc4Vt4uLi/Kb3339fl1xyiVq2bOm3rbp16/q1O+2004LtvvMYGQ4AAAAAAAAAtgsqDM/NzdWaNWuUlJTk3UBIiJKSkrRq1aqA66xatcqvvSQlJyeX2D4jI0MLFy7U0KFDiy2bPHmyGjRooC5dumjKlCnKz88PpvtVA2E4AAAAAAAAANguqDB8//79KigoUGxsrN/82NhYpaenB1wnPT09qPb//ve/VbduXV133XV+8++++269/fbb+vzzz/X3v/9dTzzxhB588MES+5qTk6OsrCy/qUrgApoAAACopoK9tpDH22+/LZfLpX79+lVuBwEAAFCjhTndgaJmzZqlm2++WZGRkX7zU1JSCu937NhR4eHh+vvf/67U1FRFREQU205qaqomTZpU6f0NGiPDAQAAUA15ri00c+ZMJSYmavr06UpOTtbmzZvVuHHjEtfbvn277r//fl100UU29hYAAAA1UVAjwxs2bKjQ0FBlZGT4zc/IyFBcXFzAdeLi4srd/quvvtLmzZt1++23l9mXxMRE5efna/v27QGXjx07VpmZmYXTzp07y9ymLQjDAQAAUA0Fe20hSSooKNDNN9+sSZMmFbteEAAAAFDRggrDw8PD1bVrVy1btqxwntvt1rJly9SzZ8+A6/Ts2dOvvSQtWbIkYPtXX31VXbt2VadOncrsy7p16xQSElLiKJOIiAhFR0f7TVWCJwx3u6WCAmf7AgAAAFSAE7m2kCQ99thjaty4ccDrBQVSZUshAgAA4JQQdJmUlJQUDR48WN26dVOPHj00ffp0ZWdna8iQIZKkQYMGqVmzZkpNTZUk3XPPPerTp4+efvppXXXVVXr77bf1/fff66WXXvLbblZWlubNm6enn3662D5XrVqlb7/9Vpdcconq1q2rVatWafTo0frb3/6m008//USet3M8Ybgk5eVJoaHO9QUAAACoAKVdW2jTpk0B1/n666/16quvat26deXeT5UthQgAAIBTQtBh+IABA7Rv3z6NHz9e6enp6ty5sxYvXlz4xTctLU0hId4B5xdccIHmzJmjRx99VA8//LBat26tBQsW6Nxzz/Xb7ttvvy3LsjRw4MBi+4yIiNDbb7+tiRMnKicnR2eeeaZGjx7tV0f8lOEbhufmSkVqowMAAADV3eHDh3XLLbfo5ZdfVsOGDcu93tixY/3+D5CVlaX4+PjK6CIAAACqIZdlWZbTnbBDVlaWYmJilJmZ6WzJFLfbOxp83z4piC//AAAAKK7KfM+rwXJzcxUVFaV3331X/fr1K5w/ePBgHTp0SO+//75f+3Xr1qlLly4K9fmVpNvtlmTKq2zevFmtWrUqc7+89gAAANVTZX3PC6pmOCpASIg3DOcimgAAAKgGgr22UNu2bfXjjz9q3bp1hdM111yjSy65ROvWrWO0NwAAACpF0GVSUAHCw6Vjx0zNcAAAAKAaCObaQpGRkcXKJtarV0+Sis0HAAAAKgphuBM8YTgjwwEAAFBNBHttIQAAAMBu1Ax3QuPGpl74hg3SOec42xcAAIBTXJX6ngdb8doDAABUT9QMr05q1TK3jAwHAAAAAAAAAFsQhjshPNzcEoYDAAAAAAAAgC0Iw51AGA4AAAAAAAAAtiIMd4InDM/Lc7YfAAAAAAAAAFBDEIY7gZHhAAAAAAAAAGArwnAnEIYDAAAAAAAAgK0Iw51Qq5a5JQwHAAAAAAAAAFsQhjshIsLcHj/ubD8AAAAAAAAAoIYgDHdCVJS5PXbM2X4AAAAAAAAAQA1BGO4ETxh+9Kiz/QAAAAAAAACAGoIw3Am1a5tbRoYDAAAAAAAAgC0Iw53AyHAAAAAAAAAAsBVhuBMIwwEAAAAAAADAVoThTiAMBwAAAAAAAABbEYY7gZrhAAAAAAAAAGArwnAnMDIcAAAAAAAAAGxFGO4EwnAAAAAAAAAAsBVhuBMIwwEAAAAAAADAVoThTqBmOAAAAAAAAADYijDcCYwMBwAAAAAAAABbEYY7gTAcAAAAAAAAAGxFGO4EwnAAAAAAAAAAsBVhuBOoGQ4AAAAAAAAAtiIMdwIjwwEAAAAAAADAVoThTvCE4cePS263s30BAAAAAAAAgBqAMNwJnjIpEqVSAAAAAAAAAMAGhOFOIAwHAAAAAAAAAFsRhjshNFSKiDD3qRsOAAAAAAAAAJWOMNwpXEQTAAAAAAAAAGxDGO4UT6kUwnAAAAAAAAAAqHSE4U7xjAynZjgAAAAAAAAAVDrCcKdQJgUAAAAAAAAAbEMY7hTCcAAAAAAAAACwDWG4U6gZDgAAAAAAAAC2IQx3CjXDAQAAAAAAAMA2hOFOoUwKAAAAAAAAANiGMNwphOEAAAAAAAAAYBvCcKecdpq5PXzY2X4AAAAAAAAAQA1AGO6UevXMbWamo90AAAAAAAAAgJqAMNwpnjD80CEnewEAAAAAAAAANQJhuFMYGQ4AAAAAAAAAtjmhMHzGjBlKSEhQZGSkEhMTtXr16lLbz5s3T23btlVkZKQ6dOigRYsW+S2/9dZb5XK5/KbLL7/cr83Bgwd18803Kzo6WvXq1dPQoUN15MiRE+l+1cDIcAAAAAAAAACwTdBh+Ny5c5WSkqIJEyZo7dq16tSpk5KTk7V3796A7VeuXKmBAwdq6NCh+uGHH9SvXz/169dPGzZs8Gt3+eWXa8+ePYXTW2+95bf85ptv1k8//aQlS5boo48+0pdffqnhw4cH2/2qIybG3BKGAwAAAAAAAEClc1mWZQWzQmJiorp3767nnntOkuR2uxUfH69Ro0bpoYceKtZ+wIABys7O1kcffVQ47/zzz1fnzp01c+ZMSWZk+KFDh7RgwYKA+9y4caPat2+v7777Tt26dZMkLV68WFdeeaV+//13NW3atMx+Z2VlKSYmRpmZmYqOjg7mKVeO1aulxESpeXNpxw6newMAAHDKqnLf82AbXnsAAIDqqbK+5wU1Mjw3N1dr1qxRUlKSdwMhIUpKStKqVasCrrNq1Sq/9pKUnJxcrP3y5cvVuHFjtWnTRnfeeacOHDjgt4169eoVBuGSlJSUpJCQEH377bfBPIWqg5rhAAAAAAAAAGCbsGAa79+/XwUFBYqNjfWbHxsbq02bNgVcJz09PWD79PT0wseXX365rrvuOp155pnaunWrHn74YV1xxRVatWqVQkNDlZ6ersaNG/t3PCxM9evX99uOr5ycHOXk5BQ+zsrKCuapVj5PGJ6VJbndUgjXMgUAAAAAAACAyhJUGF5ZbrzxxsL7HTp0UMeOHdWqVSstX75cl1566QltMzU1VZMmTaqoLlY8T81wy5IOH/Y+BgAAAAAAAABUuKCGIzds2FChoaHKyMjwm5+RkaG4uLiA68TFxQXVXpJatmyphg0basuWLYXbKHqBzvz8fB08eLDE7YwdO1aZmZmF086dO8t8frYKD/eOBj92zNm+AAAAAAAAAEA1F1QYHh4erq5du2rZsmWF89xut5YtW6aePXsGXKdnz55+7SVpyZIlJbaXpN9//10HDhxQkyZNCrdx6NAhrVmzprDNZ599JrfbrcTExIDbiIiIUHR0tN9UpbhcUlSUuX/0qLN9AQAAAAAAAIBqLuhC1SkpKXr55Zf173//Wxs3btSdd96p7OxsDRkyRJI0aNAgjR07trD9Pffco8WLF+vpp5/Wpk2bNHHiRH3//fcaOXKkJOnIkSN64IEH9M0332j79u1atmyZ/vKXv+iss85ScnKyJKldu3a6/PLLNWzYMK1evVorVqzQyJEjdeONN6pp06YVcRycQRgOAAAAAAAAALYIumb4gAEDtG/fPo0fP17p6enq3LmzFi9eXHiRzLS0NIX4XAzyggsu0Jw5c/Too4/q4YcfVuvWrbVgwQKde+65kqTQ0FCtX79e//73v3Xo0CE1bdpUl112mR5//HFFREQUbufNN9/UyJEjdemllyokJET9+/fXs88+e7LP31m1a5tbwnAAAAAAAAAAqFQuy7Ispzthh6ysLMXExCgzM7PqlExp317auFH6/HPp4oud7g0AAMApqUp+z4MteO0BAACqp8r6nhd0mRRUIMqkAAAAAAAAAIAtCMOd5AnDjx1zth8AAAAAAAAAUM0RhjuJmuEAAAAAAAAAYAvCcCdRJgUAAAAAAAAAbEEY7iTCcAAAAAAAAACwBWG4kwjDAQAAAAAAAMAWhOFOIgwHAAAAAAAAAFsQhjvJcwHNY8ec7QcAAAAAAAAAVHOE4U5iZDgAAAAAAAAA2IIw3EmnnWZujxxxth8AAAAAAAAAUM0RhjspOtrcHj7sbD8AAAAAAAAAoJojDHeSJwzPynK2HwAAAAAAAABQzRGGO6luXXNLGA4AAAAAAAAAlYow3EmUSQEAAAAAAAAAWxCGO4kyKQAAAAAAAABgC8JwJxGGAwAAAAAAAIAtCMOd5KkZfuyYlJ/vbF8AAAAAAAAAoBojDHeSJwyXqBsOAAAAAAAAAJWIMNxJ4eFSZKS5T6kUAAAAAAAAAKg0hOFO89QNP3jQ2X4AAAAAAAAAQDVGGO60tm3N7dq1zvYDAAAAAAAAAKoxwnCnXXihuf3qK2f7AQAAAAAAAADVGGG407p0Mbe//upsPwAAAAAAAACgGiMMd1rduuY2O9vZfgAAgJrt00+l4cOlI0ec7gkAAAAAVIowpztQ4512mrk9etTZfgAAgJotOdncNmwoPfGEs30BAAAAgErAyHCnRUWZW0aGAwCAqmDHDqd7AAAAAACVgjDcaYwMBwAAAAAAAIBKRxjuNE8YfuiQ9PDDjnYFAABAluV0DwAAAACgUhCGO81TJkWSUlOd6wcAAAAAAAAAVGOE4U7zjAz3KChwph8AAACS5HI53QOcwmbMmKGEhARFRkYqMTFRq1evLrHtyy+/rIsuukinn366Tj/9dCUlJZXaHgAAADhZhOFOCw/3f5yZ6Uw/AAAAgJMwd+5cpaSkaMKECVq7dq06deqk5ORk7d27N2D75cuXa+DAgfr888+1atUqxcfH67LLLtOuXbts7jkAAABqCsJwpxUdfXXggDP9AAAAAE7CtGnTNGzYMA0ZMkTt27fXzJkzFRUVpVmzZgVs/+abb+quu+5S586d1bZtW73yyityu91atmyZzT0HAABATUEYXtUQhgMAAOAUk5ubqzVr1igpKalwXkhIiJKSkrRq1apybePo0aPKy8tT/fr1K6ubAAAAqOHCnO4Aijh40OkeAACAmsyynO4BTkH79+9XQUGBYmNj/ebHxsZq06ZN5drGmDFj1LRpU79AvaicnBzl5OQUPs7KyjqxDgMAAKBGYmR4VcPIcAAAANQwkydP1ttvv6358+crMjKyxHapqamKiYkpnOLj423sJQAAAE51hOFVDWE4AAAATjENGzZUaGioMjIy/OZnZGQoLi6u1HWnTp2qyZMn69NPP1XHjh1LbTt27FhlZmYWTjt37jzpvgMAAKDmIAyvCu66y3s/M9O5fgAAABS9uDdQDuHh4eratavfxS89F8Ps2bNnies99dRTevzxx7V48WJ169atzP1EREQoOjrabwIAAADKi5rhVcFzz5nb55+XqHsIAACAU1BKSooGDx6sbt26qUePHpo+fbqys7M1ZMgQSdKgQYPUrFkzpaamSpKefPJJjR8/XnPmzFFCQoLS09MlSXXq1FGdOnUcex4AAACovgjDqwKXS2ra1NyfNk26/XapXTtn+wQAAAAEYcCAAdq3b5/Gjx+v9PR0de7cWYsXLy68qGZaWppCQrw/TH3hhReUm5ur66+/3m87EyZM0MSJE+3sOgAAAGoIwvCqwvcnnhdfLBWptwgAAGALy3K6BziFjRw5UiNHjgy4bPny5X6Pt2/fXvkdAgAAAHxQM7yq8A3D9+51rh8AAAAAAAAAUA0RhlcVRS/+M368lJvrTF8AAAAAAAAAoJohDK8qiobhjz8uzZjhTF8AAEDN5XI53QMAAAAAqBSE4VVF0TBckn74wf5+AAAAAAAAAEA1RBheVQQKw9PTpQcflLZutb8/AACgZuICmgAAAACqqRMKw2fMmKGEhARFRkYqMTFRq1evLrX9vHnz1LZtW0VGRqpDhw5atGhR4bK8vDyNGTNGHTp00GmnnaamTZtq0KBB2r17t982EhIS5HK5/KbJkyefSPerpjp1is9bskSaMkW6/HL7+wMAAAAAAAAA1UjQYfjcuXOVkpKiCRMmaO3aterUqZOSk5O1d+/egO1XrlypgQMHaujQofrhhx/Ur18/9evXTxs2bJAkHT16VGvXrtW4ceO0du1avffee9q8ebOuueaaYtt67LHHtGfPnsJp1KhRwXa/6qpfv+RlW7bY1w8AAAAAAAAAqIZclhXcb2ETExPVvXt3Pffcc5Ikt9ut+Ph4jRo1Sg899FCx9gMGDFB2drY++uijwnnnn3++OnfurJkzZwbcx3fffacePXpox44dat68uSQzMvzee+/VvffeG0x3C2VlZSkmJkaZmZmKDlSSpCrYuFFq3z7wMn6yDADAqS07W1qxQrr4Yik83OneFOe5cObAgdKcOc72JUinxPc8VApeewAAgOqpsr7nBTUyPDc3V2vWrFFSUpJ3AyEhSkpK0qpVqwKus2rVKr/2kpScnFxie0nKzMyUy+VSvXr1/OZPnjxZDRo0UJcuXTRlyhTl5+cH0/2qr107afRop3sBAAAqw8CBUnKyFGDwAAAAAACg8oUF03j//v0qKChQbGys3/zY2Fht2rQp4Drp6ekB26enpwdsf/z4cY0ZM0YDBw70S/3vvvtunXfeeapfv75WrlypsWPHas+ePZo2bVrA7eTk5CgnJ6fwcVZWVrmeo+OmTpXOOEO67z6newIAACrShx+a2+eek0r4/lIleEaIAwAAAEA1E1QYXtny8vJ0ww03yLIsvfDCC37LUlJSCu937NhR4eHh+vvf/67U1FRFREQU21ZqaqomTZpU6X2ucCEh0ogR/mE4/ykFAKD6qOqlz6p6/wAAAADgBAVVJqVhw4YKDQ1VRkaG3/yMjAzFxcUFXCcuLq5c7T1B+I4dO7RkyZIya8EkJiYqPz9f27dvD7h87NixyszMLJx27txZxrOrQiIiJN/nX7u2c30BAAAAAAAAgGogqDA8PDxcXbt21bJlywrnud1uLVu2TD179gy4Ts+ePf3aS9KSJUv82nuC8F9//VVLly5VgwYNyuzLunXrFBISosaNGwdcHhERoejoaL/plOJT4kWRkc71AwAAVH+MBgcAAABQAwRdJiUlJUWDBw9Wt27d1KNHD02fPl3Z2dkaMmSIJGnQoEFq1qyZUlNTJUn33HOP+vTpo6efflpXXXWV3n77bX3//fd66aWXJJkg/Prrr9fatWv10UcfqaCgoLCeeP369RUeHq5Vq1bp22+/1SWXXKK6detq1apVGj16tP72t7/p9NNPr6hjUbX4huG+9wEAACoaYTgAAACAGiDoMHzAgAHat2+fxo8fr/T0dHXu3FmLFy8uvEhmWlqaQkK8A84vuOACzZkzR48++qgefvhhtW7dWgsWLNC5554rSdq1a5c++OADSVLnzp399vX555/r4osvVkREhN5++21NnDhROTk5OvPMMzV69Gi/OuLVTseO0vr15n52tuR2m3riQFW0ebO0YoV06628TwGgLFUxeHa7ne4BAAAAAFQ6l2VVxf+RVbysrCzFxMQoMzPz1CiZsnWrNHOmNHWqeXzkiHTaac72CSiJ5yKvr74q3Xabs30BgKrK87cyNFTKz3e2L0Xl5pprlkjSwIHSnDnO9idIp9z3PFQYXnsAAIDqqbK+5zGEs6pq1Up68knvf5yPHJH27zfz9uxxtm9ASVatcroHAFD1VcVxCIwMBwAAAFADEIZXZSEhUlSUuX/woDR4sPTQQ9L113vb/PGHM30DAADVR0GB0z0AAAAAgEpHGF7VnXeeuZ0xQ1q0yNxfudLcvvaaVL++9NxzzvQNKMrzSwYAQMkYGQ4AAAAAjiAMr+oee8zcvvii//zly721mUeNkjZskB59VMrKsrV7AACgGiAMBwAAAFADhDndAZTh4oulnj2L12K+5BL/xx06mNtDhxgpDgBAVcbIcAAAAABwBCPDTwXdupW/7XffVV4/AABA9UQYDgAAAKAGIAw/FbRvX/62IbykAAAgSL5heFUcuQ4AAAAAFYDk9FTQuXP524aGVlo3gDJxAU3AWceOEWTixBCGAwAAAKgBCMNPBYmJ0owZ0qeflt02UBi+dq00e7Y9/7ndvFnaubPy9wMA8PfLL1JUlHT77U73BKeiggLvfUqmAAAAAKimuIDmqcDlku66q3xtv/xS+v136YwzvPO6djW327ZJN90ktWlT8X2UpIMHpbZtzX23m1HCAGCnqVPN7axZ0quvOtsXnHp8A3DCcAAAAADVFCPDTzXr15fd5tprA89/7DFvWF0Zfv/dez8rq/L2g6qLEyCAcyhtgZNBGA4AAACgBiAMP9V06CDdcUfpbb7/Xjp8WFqxIvB/aI8cqZy++V68c/fuytkHACAwwnCcDN/vC74lUwAAAACgGiEMPxW98IK0dKn3ccuWxdtccYV04YXSv/5VfFll1fQ+dsx7nzAcAOxFGI6TwchwAAAAADUAYfipqnFj7/2rry6+fMUKc/vUU8WXzZtnasp6gpOsLGnQIGnBgpPrE2F4zURoAlQNhOE4GYThAAAAAGoALqB5qjr3XGn4cCk2VsrJKbldoFB6wgRz27q1dNFF0jPPSG+8YaY//pDq1TuxPh0/7r0/aJB0441SrVonti2cOvLzvfepGQ44hzAcJ4MwHAAAAEANwMjwU5XLJb34orko5l/+cmLb2LLF3C5c6J23ZMmJ98l3ZLgk/fTTiW8Lpw7fMByAMzZulGbPdroXOJX51gknDAcAAABQTRGGVwcXXGCC8WD98Ye59YTikrRvX9nrPfusNGJE8VGIviPDJenXX6UZM6T588vfp4wM6ejR8reH8wjDAefdfLPTPcCJ+P578yuq7dud7gkjwwEAAADUCJRJqS46dSpfu/h47wU033xTev99byguSQcP+re3rOKlL+65x9wOHGgu0ulRdGT4+++bfXi2U5bdu6VmzaS4OGnPnrLbo2ogDAecV54Tmah6unc3t1u3St9952xffANw31HiAAAAAFCNMDK8uujYsXy1vhcvlhITzf21a6Uvv/RffuCA9/5rr0kNGngvxin5/wc5K8t/3aIjw7/+2nu/PIHp55+b2/T0stui6vB9balZDDiDz96pbf16p3vAyHAAAAAANQJheHVRu7a0a5e5IGZRZ57pvd+ihfTooyVvx3dk+G23mVHjQ4d652Vne+8XHTFedGS470jFw4dL3qdHiM/bkWDn1OEbhjOaEHAGfzNPbXl5TveAMBwAAABAjUAYXp1ERUnh4cXnX3+99/5pp0lJSVLTpoG34RkZ7jvKOzfXe9831C462tsThp9+urn1rf1ddBR5IL5h+B13SD//XPY6cJ5viFNTS6b88Yc0ebKUluZ0T2qOf/1LSkjwv+ZBTVY0DOfE1KmlKpzMIAwHAAAAUAMQhlc3OTn+jydOlKKj/edFRkr9+wdef+FC6dVXpfr1/dt7+IbhvqPEJW+AHhdXfLu+682cKU2ZUryNbxj+0ktSz56B+4iqhZHh0vDh0tix0sUXO92TmuPuu6UdO6T773e6J1VD0TC1Kow0xqmFMBwAAABADcAFNKsb3xHdN98sTZhgRq1+8ol0443eZWefXfI2br/d//HGjaYm+SWX+I8yP3LE3P7zn9K2bVKtWuZxbKxZx5dnZHhurnTnneb+gAFS8+beNkXDm/KMJq9MW7dKv/8u9elT+fsKdKHSqmTvXlM/PjS0+DLfMLymjgz/+GNzu22bs/2oiYqelKupAoXhvicygbL4nsysqSc2AQAAAFR7jAyvbnxD7tdfN7enny599ZU0YoR32c03S/Hx3scJCYGDTo8ff5SefVZ6/nnvvCNHzH+Y773XBOKeC2bGxhZf3zMy/I8/vPMOHfJv41tWpSo46ywz0nfDhsrdz5QpUqNGVbcszOrV5jXt1y/wcsJwKYzzio6pCuUlqoKix6GmfhargiNHpKlTpd9+c7onwWFkOAAAAIAagDC8unn6aTMC/Isv/MuOFHX66abEwC+/SNdeK82ZY0LfWbNKX+/tt733jxyR9uzxPt61y9wGCsM9o7x9L9C5ebMZUesJcYpegNNJvsHS2rXe+zt3SgsWVGwA9+CDplb7gw9W3DYr0r/+ZW4/+ijw8pochufnm19LZGYGt97+/dK0aVJGRuX0qyYhDA+MMinOefBB6YEHpMREp3sSHMJwAAAAADUAYXh1ExcnvfWW1Lt32W1dLql1a+m990x97rZtpSFDTCmT9eullJTS1x83TmrRwvt4505vH4q64QZTqsU3DL/hBunKK6U33zSPgx0ZnpdnLrQ5Z05w65WH76h13xHzHTqYkwfz51fMfnwvTlpVg+SyyrfU5DD8gw+k//43+PVuvlm6776Sa/ej/E4kDJ840ZzEqE6Bsd01wzkJUbJFi8zt/v3O9iNYhOEAAAAAagDCcBQXGmpC3yeflNq0Kb1toP8wN2okhYcXn3/55WYEdFG33GLKkQSq/Vv0gqC+Pv5YevFFEyxWVDDz3nvSX/4i/fqrd56nX8ePe0cAL1168vt6/XWpTh3v49JG5Jfm0KHKDabKCsN9Q7eaFoafaF37Tz81tytWVFxfahLf9/uJvPcnTTInMebNq7g+Oa3o3+LKDMNzc6UuXfyvIQGvko59VT+BQBgOAAAAoAYgDEfJwsLMCPFNm6RvvpHuv7986113nf+IZ1++I8N9ffGFfwDtUdrIOt+R5Js3F18ebPBw/LgZqfvBB9Lkyd75ngD//PO98xYtKj2oL0tBgTR4sH9o4huG5+VJycnSFVeUHkp8/70peeNbD76ilRaGL19uflXgEUwYnpcnde8uDRx4wl1z1MKF5oQR7Of72Qv2c+7b/rvv/Je9+qp0xhnS//534n1zSmWODC+67ZUrzTH673+rfsDrhJKOfVkBs9PH0rd/69eX/O81AAAAAJzCCMNRuvBwMzo8MdEExIFGfPsaM0aqX7/k5UOGlLwsUE3qvXv9HxcUSH/+swncfUflrlrl3+6qq6Rzzw0usPaM1pWk33/33j940IS8vgHZjh0nF4T61lr3CAkxFxi97DKpTx/Tn8WLpW3bSt7OpEnm9oUXTqwP5RnZXFIY/p//SJdc4j+voKD8+1+71oT5b799cicWnHL11eZE0aliyRKpUydp2TKne3Lyjhzx3g/mPSf5v9d++sl/2e23m2sf/P3v/vPT00v/HFYFlRWGP/ig1KyZOQaBtl3Sic+arKRjX/S9Om6c/2OnLyJdNKwfNcqZfgAAAABAJSIMR/mFhpoAtVkz87huXWnoUFP726NVK3P7/PPBb9834PL4/HPvfcuS7rrLlCiZP1/assW7bONGc5ubK61bZ0Zu//yzGd3mdktr1piLFV5xhTRzZuD9r1/vve87YvTAAe/FQX299FLpz+fJJ01N80CjpT311X2FhEhPPWVCS99w/+efpbFjTS34oiIji887dKh4yFfUwYNS06ZmKotvGP7FF977t9xSvG0wI8N9g6HffzejTH/7zTvP7ZaGD5eefbb4urm55litW1f+/ZXEsszrUVGjMoO9mGYga9b4v79PVm6uOcmyfr25yG5Vkp8vzZ0b+ARRSXz/VgQbIPqWYwr0OZS8r6HbbT77TZpILVuak1Wl2bTpxEvnnKyiQWtFheFTppjXxvd947uvQOWtarqSThAUfY3+7//8H//yS+X0p7yKhuFz5niv6QEAAAAA1QRhOIJTv74ZIfn66yZwfeUVE2RNnixddJH017+adnfeaULcceNMcPXnPxff1ptvSlu3lr6/++4zP8mXTEkO3wD6nXe89z0lVh591NSy9ThwwIxs7NbNXNhz8WLTt+++kzZsMKFFQYHZR9FReh6vvSYlJBSfHxVlbvfvN8+l6IUkH3rI1DR/9dXi66alFZ+Xnx84dL/vPnN8b7rJjE733Y9vGO4JMgYNMqPiP/kk8PORpB9+MLfZ2f7bGzfOXDjVNxj2DcMvvrjkbUrSl1+a0d7l4XuR0n/9y9Qf9pxM8Wzr5Zele+4pHlTPnm1+heD7Wpdmz56Sw+6nn5aaN5f++c/ybUsqvdzB1VeXfzuS+RVDdrYJL//7XxOEd+tmLm77zTem1v5VV5mTOSfKd1RvRYzk3bcv+BHZJfnnP6Ubb/QvQxRof+PHm0B/927/MPzw4eD25xve7tgR+H3heX3vvtuUTfEoLaz88kupXTvpttuC609FOXbM/3FF1wz3Dfl993XXXRW7n1Pd5s0ln6Ap6zOzYYP3/s6dUufO5t9YuwTq39/+Zt/+AQAAAMAGhOEIXq1aZlRwfLx33pgxJgyqV887r1076bHHpNNOk954w9Tjfv55Uw7l2DET7rZsKfXtW3wfvhdm69VLmjGjeCmUHTu893/5xYTyU6b4t/nf/wKPhO3Rw1wkNCLC1Ebv1at4m/btSzwEkkzQfued5oKhf/ubmRYuNOGyb3D50EMmwPM1Z07x7R0+HDiY862l3rmzCfwlM3r1P//xLvPUd/3wQ3N79dVmZHVZJxw8o1137jQjFZ95xv/YFg1+MzNLHw3avbv07bf+8zZskGbN8n9+vmH4668X345vyFn0wqu+I99HjixeTsfXa6+ZEfDTpgVe/sAD5nb06JK3UbRfgUbpe3z9tak/X16LFplfQEybZt733bp5l/XsaU5qLFpkTuqcKN8g0/N6b97s/zqX1/r1UuPGUr9+J94fX+++a27T0ko+kdKhg/T44+ZXEw8/7B+GBzsS2/e9e+xY4BHpv/xils2Y4T+/tJMgnpNp//2v/Rcf9JxM8VXRYbhvAO772Zw71/vLnNJU5gU9q5LExJKXBROGP/SQ+fdr2LCK6Vd5cNFMAAAAADUAYTjsERtrQq877zSjXH1HNNeq5d/29tulhg39540cKT3ySMnb//lnM7q0qIceOvE+33df8XmdOnnrZB886F9yZe5cE0Dfe69/zfFDh8z8l16SLr3U9OmDD4pvOyureImNQCPSPbXKr7nGf356un9IlZ9vRlbfcUfgfXl4guavvvLO8x25XnTE6ZYtgS9Y6uuKK7z38/JMmDl0qDRvnhnhu2+ff8mJQOUnfMPynTtN/+65xwR/tWt7l82YYd5XJfGM1C3vBWDLMnhw2aMli564Kcvu3eaEUWlOpmyK7/sqPd0c27ZtzfvL7ZZ+/NGU4ilPiRdPffpANf5PhG8A17174Da+J5cOHPAPwzMyip9sKigwvx4IdCKo6IkcT5uiYe2ECcXXLW0Uum8ffMv92CHQe6Miwmffk1e+o52LnoDw/awGMm6cudBveULzU1VurqlHX9pnqKww3PfEZ2kXjz4ReXlln6QjDAcAAABQAxCGw3lTpkgXXmhGkEvm4nWeuuQV6cUXS172xBMm+PGMqpZMaO+rRQtTp/qzz0wJi5LMmFG85MMPP5jn9dln3jD79NNNAOlx+LB/DePYWDNSNtCI5W3bzEh8X7t3B77I39KlxUece0aR+973DXD79PGG4EV/8r9lizn5UJo//vAGP76h6YABZoRv794lB2iesi2+o73T0sw6zz4rTZxY/OKlvrXMiwoNLXnZwoUlL9u714T4Rfc1f37J63h4wsk9e/zLkhw/XvJJm7LKl3gu/Og5rlu2lL/GsG9Al5HhH9ZmZEgdO5pSPI8/bt4rL73kf8FYj19+Kbnm/okqK4Aruvyjj8xJEV++F7+VzK8QhgwxF/8tqmgY7gmxfT8TUvFfmUilB52+pWhWrCi5XWXwDVE9KiIM9w1PfU+KBRuG/9//meMe7K8bjhwxv34pa/tOW73aXEPj2muLL/P9+1lWGF70mgklCfb6Bm63KSnVtm3Jf2fWrvW/VgYAAAAAVFOE4XBey5Zm1O+hQ6ZmdrduplbvrbeaciZFa26/8op05pmmlnLv3iYI8wTp9etLV15Z/OKOnpIhgcpxSKbMi+RfHzg2VjrnHHM/LEzavt3ULJbMPsqjbt3A80NCzEj5SZO8ZWIOHPAPN8eNkxo0MOUzfGtpS+aYFfXgg2bkeiBFR8j6jsT+4QcTzD/3nH+befPMbdEw/NlnvTXHS1Onjgl3li4tvmzTJv8LlvryBF++o4F9R6pPnVp8nQMHpAsuMCUqfBUU+AdQviHS0qWl1/eeMsWULfD9dUFp5Vh87dplSp80a2ZGpB85YsrPJCWZXxAU9ccfgQNNXxkZ5lcJTZua0f6tW0tdu5Z9AcmcHP8TOwUF3vex5H8CZe1a6b33zImbzp2LB2dDhvg/roiLjhYN/SzLTDNnmnAu0K8Gip6MmTbNvy/LlpnbQOFj0TB83z5zWzT8DvQZKykMP37cf7S63Rcd9Lx3/vY3769JkpLMdRZOhu9z8j1uRcNwzzEsS7D13UeNMn/Ly6rDvnVr4Iv3/vyzuYZCoGs0VKRFi8xn5eOPiy+rU8db/qc8Ybjnfez7ufC9n5dnfkHh++ubsmzaZEpL7dgR+ITp8ePmb8njj5d/mwAAAABwqrJqiMzMTEuSlZmZ6XRXcCJefNGyRo60rN9/L197t9uy1q+3rFGjTLT25ZfeZb/+aubfe69Z1rKl/7qvv25ZX39t7q9caVm3325Zixf7t9m/37IeeMCyli0z25Ysq0kTy9q2zbJ69/bEeZb14IOWNWGC97FkWR99ZFnr1nm3dfiwZdWt69/m/ffNc/DwXXYi0xtvWNbatd7tjR1bvvW2bj35fQc7bd5sWUuXnti655zj/zrdcYf/8v37zWv7ww+WNWZM8fU9li/3n5+fb45frVql73/YMHM7dKhlnXGGd/4TT1Te8dqwwfStpM/GG2+Uvv6MGSUve/ZZyzp61Lut2Fj/5X/8EXifvtxuy/r4Y8tKT/eff/iwZXXvXnyfR496P1OSZaWmlty/J56wrKgoc79zZ8v64AOz7QEDir+mHtddV3w7WVmWVb++/7xOnYq3mzIl8HNMS/NvFx5uWbm5/m1mz7as0aMtq6Cg7GMWrMsuM/udPNm/H3/9a/nWd7sta/hwy7r/fv/5v/3m3Va7dt75d97pv5+pU0vfvqdd9+7eeXPmWNZ335VvvUCvo8cnn3i3/fDDlnXwoHfZOecE/rvg4XZb1uOPm+N2Mh54oPTPWP/+pt3vv5fcxuUyt/v2mbYXX+xd5vuc1q0L7vNnWZb1yivedZYvL75827bS+3+K4HtezcVrDwAAUD1V1ve8U+d/OSeJL8o1VH6+ZWVkBF7mdlvWf/9rAt+TlZHhDTHcbvN49mzLysz07ue000zIHciTT3qDh9NOK768b9+KCU6fftqyZs2q+EC2dWvv/RYtTm5bRQPsYKcWLSyrVSvviRDf6ZVXLCs01LJq17asG28svvz4cXO8Gzb0n79jh2VdemnZ+/a8jpdcUr6+Xn31yR/7SZO897dvL/7eef750tcvGgIXnZo1s6wjR8y2Tj/df9mWLWV/Np55xrRNSjInOfLyzPzHHw+8v127LOvll8v33L/6yrL69fOfd/fd/kGi221OCNx7r2Wde27g7fTr5w0jR4woeX99+wYOIL//3ixv0sSy6tQx93/6ybvc7fZuo2tXy5o7t+zjVl6//25ZISFm24FOXr35Ztnb2LLF2/7QIe/89ev9t/X22yZwLrqPrl1L3nZurrddnTrmPf/ss955t99ugvZ33im+ru8+SlL05Mall1rWt99a1v/9n/98z/vO148/epeX9O+EZVnW559b1ltvlbz81lv99+V7MkayrEGDTLsdO0p+b515prn1/BvRpYt32aZNZl5OjjnWnvm+J1U98vPNSdyBA83r6nZbVp8+3nXmzLGsn3+2rI4dzePZsy3rm2/8++J7Ik/yPyEWyGefmcB/167S21UyvufVXLz2AAAA1RNh+EniizKqNLfbOwJ24sTiy3fvtqx//cuymjf3Dyl69zajkH0D6PbtLWv6dBOq+wYq5ZmaNDF9KSkQ/Ppr72jes87yzveEPImJJnD3XScy0ow4DaYfZU21aplRr8Gu5wkNS5oefNCy/vOf4vPfecc/8C9pmj+//H0JDbWsAwdMEFiRx8YzOtrD9/Xo1s2yrrzyxLb73XfeoNczffWVCRmPHzfvm5tuMuHkhAneUb9FtzNggAnsEhMD76dRI/PeLU+fDh4M/Hr5Tu+9F9zz9A2GS5omTTIj2z3h38cfm/mdO1tWz57m/v33m/B89mzL2rmz+Dbc7pMbJb5qlWXdcIN3exddFPh4S5b1xRcmID7rLHOyICvLf1u+v4T44Qfv/BUryn/c3ngjcD/T08u/Dcsy76fnny/+Oqxa5d3etm3e7RcNnqXAJ3hWrrSsRx81vwj59VfzHvQNzD0jpr//3gTq06aZEx9Hj3rbrF1rXretW/1/uXPVVf77KnqSyzMy3HekfdHpnnvM7bXXmrZFA+klS4r/SmLBguLHe/Ro/zZFf5XiGS3vO/3rX/6Pr7/e/3FJJ2/cbstavdrbrmFD81lwCN/zai5eewAAgOqJMPwkOfFF+aabzGDEjz+unF/Goxravbv0N8v+/Za1Zo0JbBYvNoGOx2+/mdGAGzaYx/n5/kFFWdPDD1vWxo1mXbfbv9yLZEb1WpYJ0ubONUHg/Pmm7ItlmRGl+fmm/6+/bkKZefPM/HfeKb6/W24xYWagvpx2mhnx+scfgZcfO2YCWN8R2IEC1O7dLeuKK8p/DE5katvWBHJ331128Neqlf99j/x8/wBv9mzzOk+Z4r/+m2+Wr0+1apn3g2X5n4g47zwzrzKOQ1JS8Xk5ORW3fc/Ibd/Jc+zOP79i9lG/fvn7fOaZlhURYULb11838/78Z2/5pbKmK680JxceeaR8fxt+/dWc3HrzTfNeK7q9F1807VJSii/zlJLxTJ06+f8i5t//9l8+a5b5W/Dii+U/drGx5iRcQoJlNWhg5kVHe0u4lGf6/vvio6x9p6lTvc/nl1/M35oePcq37aIh8MSJlnX55d7HHTtaVr16pW9j3DjL6tXL3B892nw+ly8v3oebb/Z/3KOH9zWUip+U++ILcxLCM79Dh8D7L3ri7M47zXuidWszonzjxor5HDz1lP/j0FDTd19vvx04WJcsKznZlDXznMCwCYFozcVrDwAAUD0Rhp8ku78oHzzon91ccEHpv8IGKs0vv5jyFt27W9Y11/iHFunplvXaa6YcQlG//eYNO/7975PrQ2amCYTGjvWWkXG7zeT7QfGUkvnlF++6zzxjgvP69U3w41v73LJMSLNliwmqioYynlIV335rgkvJjI4tWkJBKj4SszzTuecWP3nhCRBbtCge0O/ZY55PVJQZdevrwAFTTmfyZO82jx83wefixd5SBb4BW2l9a9Cg+AjPa64x2/j+e8u67TZTHiEyMvD6t9128qFat27F5wUKtcuaLrmkeO19+fzzdeyYGVFbEUGgZZkwr7ztp071lnz5299MuaRgf5Hx73+bEfx9+ljWu++aQD4311yTIFBZkkCT74msQ4fKH2QnJJhSOOXt6z/+YVl/+pP/vLi4ijn2wU59+3pPIq1bV/wEXteu3jr+dkx33WX+zhYdAd6woXk9PSO7Y2K85U7uvNP7Pg70HndiWr3avKevvda/ZFSzZiZ4L/r6lzTVqmXrSHEC0ZqL1x4AAKB6Igw/SXZ/Uc7ONgOnhg/3Vhbo0cPMB2zn+5P+L77wBjRlKSjwX7cybNliwsBXXil9X2X14/BhU45g6lQTkBe9ON/evSZo9IyY9q0BPmqUuQji44+bkge3327C6qIXl3zpJTN/40ZTbsE3tA9kwwZvmNmypfc5+I7oD9a335p9Z2ebcL2smuC+U6Ca4jt2mMBqyRJTG9nT9uWXza8ARo82wXijRv7batTIsv7+9+CDtqVLTYh59dUm/PfUFA80XX+9eQ0sy/+iqp07m/Invo4dM6+tZ+RuSdPkyeYE0Nat5jW56y7LmjmzeDDpOa6XXmpOSixbZlmffmpqR4eF+W/TM6L42WfNullZ5j3tKZlix1T0pExBgTmJMmqU98Kvb755YhenveACc6zOP9/7Hv70U/Prhs8+M+VHytpG9+6mpr7vxYLbtDHB/QsvnNxzr13bfB58R8z/5z+mn0eOWFbTpqWv7zlpNXCgOUGYk2Nqawfbj82bvce/tHYNGphfAa1e7f93LS/Psh57zPsLksGDg+9DSRf5HTbMfJZ+/dWyPvzQO3/lSv92ERH+76Xffy9enst3uu46U5rpxRcD/zrnb38r39+1CkAgWnPx2gMAAFRPlfU9z2VZlqUaICsrSzExMcrMzFR0dLSt+960SerVSzp4ULrxRmnOHMnlsrULgL/PP5fOOkuKj3e6J845elRas0b6+mvp3nul2rUDt1uzRjr9dOnMM0/8g3vokBQaKtWte6K9Ld1550k//CDdcot0551SVpb07bemv++8I6Wlmedx1lllb+vtt6VPPpFefFEKDy++PCbGbP+nn6T27c28bdukoUPN+6okPXtKt98u3XZb8WXPPy+NGGHu//Wv0vLl5jV5+GFvm927pWbNzP3S/tn65RfpT3+Sdu0yx/zpp01///EPKTfXHIcuXbzb8bymv/wiNW8uRUZ6t/Xdd2ZebKz/PrKzpQ8/lAYO9J+/fr3UoYP38YED0ubNUrt2UmamVK+emf/VV1J0tPTGG9Krr5p59eubfySC8eCD0rPPSjfd5N1OIAcPmucVFWUeb9tmjm1Ghvc1i4oynwlJ6txZ6t5datPGPM977pH69jXHM9BnYOVK84+cJCUnS1ddZd7zV10lXX+92d/cudINN5g2330nLV4sPfSQVKuWmZeVJT36qPSvf0n9+0stWkiJiVK3blKfPtLvv5f8/G66SXrzTdMmOVnKy5PWrfM+3127pIULpSNHpLvukmbMkMaNk44dk665RnrrLbNu69b+z2/NGumll6QJE6T586VJk6QBA8zjBx80r+3KlaZtr17mfRsWZh5/+qn5DDZpIg0b5t/f+++Xpkwp+flIUk6OFBFhtt+smRQSYr489OsnNWggzZolvfyydMUV5m/X55+b5/P119L27ebzcuWV3u1NmyaNHm3uFxSYz25Ojnnff/21dN990oUXmteg6Pv94EHzenneI7Nnm/706GE+p/Xr+7fPyjJ/O156yRxvzzGpZE5+z4OzeO0BAACqp8r6nkcYbpMvv5QuvVTKzzfZTEqK7V0AUF39/rsJ9IYN8wauvgoKTJBZEQ4ckPbskc4913++ZUmHD5sAr6BAWrrUhJ5PPWVCscGDS9/uwYPmD6UndA1k/Xrz/Jo3L31bOTkm+Kxf3xtu7ttnQrpWrcr1NMvls89M2HvsmAlw33gj+BMmbrcJFiVz/B56yBy/c881QeqePdKtt5pw+a67TDgtSfv3m1D0+HETKJ/o6/vHH9KPP0oXXST99pt5Hbp3D347L7xgQttrrvGf/9tvJoy+6KIT659kjktGhnTGGdL775v3SFSUCdi/+EIaO9Z7Yq+gwLwGnmNaEssy/yCHhZ3c2en166XTTiv9fbVli7Rhg/lsdOokNW164vsLhttt/iZ8+KEJy885x7vs+HFzDEo6CXgKcvp7HpzDaw8AAFA9EYafpKrwRfm556RRo8z/0V95RRoyxJFuAIB9fEdfV0e5ueb5eUY4V6ajR82o5D59pKuvrvz9oXqo7p/B/68qfM+DM3jtAQAAqqfK+p5XxtApVKQRI6S//90M1rrtNunuu80gSwCotqp7CBcebk8QLpnR0FOmEIQjONX9MwgAAAAAQTihMHzGjBlKSEhQZGSkEhMTtXr16lLbz5s3T23btlVkZKQ6dOigRYsW+S23LEvjx49XkyZNVLt2bSUlJenXX3/1a3Pw4EHdfPPNio6OVr169TR06FAdOXLkRLrvGJfL/JJ87Fjz+F//Mr+Cv+46UzJ37VpvyVYAAAAAAAAAQMUJ+qpGc+fOVUpKimbOnKnExERNnz5dycnJ2rx5sxo3blys/cqVKzVw4EClpqbq6quv1pw5c9SvXz+tXbtW5/7/mrNPPfWUnn32Wf373//WmWeeqXHjxik5OVk///yzIv//xcxuvvlm7dmzR0uWLFFeXp6GDBmi4cOHa86cOSd5COzlcklPPGHKpz7yiLnm3fz5ZvIsb9jQlD9t00aKizOPTz/dlPaMiDBlb8PCzLyoKFMqtkEDU0rX5fKW7K1d2/yCPyrKlDT1DF50u81tWSVVAQAAAAAAAKC6CLpmeGJiorp3767nnntOkuR2uxUfH69Ro0bpoYceKtZ+wIABys7O1kcffVQ47/zzz1fnzp01c+ZMWZalpk2b6r777tP9998vScrMzFRsbKxmz56tG2+8URs3blT79u313XffqVu3bpKkxYsX68orr9Tvv/+upuW4GFVVrSf444/SvHnSsmXSL7+Ya6JVlrD/f+rDc82wuDhzPzfXXG8uNNQE5vXqmXl165oQ3e024Xq9eqZN3brm2lv5+aZCwGmnmWuu5eRI2dnmem9RUVKLFma9w4dNMB8dLUVGmvUlMwo+L8/s0xPYHzsmpaWZoD8316wXFWWmWrXMPj3XRzt61NweOWJOAERGek8YWJbpt+/0xx/mpEF4uGnruW5abq55XocPmxMEx4+bfniuwRYWZvZ//Lh5fgUFZvuxsWZbYWHmebhcZn5oqPdadpmZ5r6nf2FhZp1jx8z2MjJMu1q1zPEKCzPLoqOl3bvN842LM88rP1+KiTGldUJDvcc+L89sq25d0ye327wWubmmr6edZu7n5prlYWGmX7t2me3Wrm36Xb++OZb79pltxcWZ5+95Pp77O3eabebne1+nPXtMf3JyzLbatZPq1DHtPcfL87qEhpr9HDhgnndYmHT22d73p6/t283zO/dc0+fdu81r6Hkf5+WZ7TVsaF6bqCjzvjh2zDzX48dNn9xu065OHdMmM9O0adTIe13FyEjvezQ01KwbGWn6ffSoWTc721zH0O02z0Uy7erVM9uQzPxmzcx+PK+Ny+V93+fmmr5nZpr3XPPm5rmHhppjUreueW0aNDD7zM832wwPP7kTWJbl/bx5qibk5prtehw/bvp1+un+8+3g+Rx7FK084nab/uflmcfffWeuzVivnpSUZD4bwfB8jmJivMcjO9u81hER5r0QjLw8897IzjYnMyvqeqVl8bw/3G7p00/NvEaNzK+Omjf3Xju1OlfK8PyNL/o3pKqWyvZcuzIiQnr5ZfN5u+gi8zfXrv76XrfVV0GB+VxlZUldupxaJ82r6vc8VD5eewAAgOqpsr7nBTUyPDc3V2vWrNFYT50PSSEhIUpKStKqVasCrrNq1SqlpKT4zUtOTtaCBQskSdu2bVN6erqSkpIKl8fExCgxMVGrVq3SjTfeqFWrVqlevXqFQbgkJSUlKSQkRN9++62uvfbaYvvNyclRTk5O4eMsT1JVxXToYKbHHjOP9+83Qdj27d5wfP9+E+Tm5HiDtfx8Ex5lZ5v/VP/xh7nvdnuDXMsy/5H1jATPz/fuNz9f+v33wH2qqDrmv/1WMdsBqgLfz9KJtK9Xz3xGPWFueXg+xx5hYd7HLpf/FGie529FvXomqD1+3ARwtWqZ7WRnm9A9L8/8ffH5kymXywTideuadXJyvCc2JDPPtz++t4HmlbbMskz/PGFmSIjZb26uNwAvKCj9WNWvX/KxyM83/fecoMjNNc9dMkFx7drmvufvqWTmhYT4b8fzODTUtDt2zJwwiYw0fzc9x89zksdzIiw83Ltfz6918vLM+6NOHe/+PceivFNBgTluJZ3SrlvXnHDxnNALCTF9Dgsz+wwP9570OHzYe9InIsK09ZxAycz0npgsKPCeSPQcZ89rFuj96DmheOiQ+TerSRNznLKyzOQ5gen5ZZNkjqfnBG1BgTlpGBvrDfV99+F2S7/+ao6r52SX58RJdrY5MeX72hW9LWlZXp45yRcR4T2Z+f9/qKb8fO8vr44f9762nveq50SkZE7UeX7B5en/5s3m2Naubf7t9oiP9z8OBw+a7YWEeE9Geu77Tr7zLcvs37LM+7BWLXPcPSeXPa/Lpk1mf56TTp73686d5r0gmRNCnhPIYWHe7blc5sTk6aeXXi4/PFz629/M9VIAAAAAoKoIKgzfv3+/CgoKFBsb6zc/NjZWmzZtCrhOenp6wPbp6emFyz3zSmtTtARLWFiY6tevX9imqNTUVE2aNKmcz6zqaNjQTJ06nfg2cnLMf1CPH/f+h/3oUW/4ExlpHmdkeMOQiAjvf4A9ofrRo97/1B8/7g3CDh3yjhg/dszM9/yHOT9fatvW/Od7xw7Tj9BQE+h7wrTsbNOHqChvSOTpn8tlQo+8PNPm2DGzzHcUeWiouV+3rulDdLTZ9rFjZsrJCRwYREaafuXnewMMz4hhz8hdt9sci9q1zTy323v8atc2z1syIY5n1LEnsHS5TP8KCrzHKibG3B4/7t1nTo53FHtsrPcY79jhDUiyskxoFBZmXqfjx72BkstlQh/JPJ9atcz2MjOlvXtNO08AJ5ljEhJi2h04YPYVHS01bmzWl8w29+83+/cEXp7Xy/PLAM/UsKFZLyrK9M3tlhISvO+jI0ekbdv8w1OXyxsO5eSY/Xv2lZ1d8gkYz3P1vDcbNvSOtg8LM5Pn2HqCs9BQczw87/fwcG8g6huoesLqWrVMf7KzzXZK4gm2PaGRbyDnCUU9r1N+fvHg/NCh4tv0hMxFebZRNOT0PaEVDN8/k0VD75KOvWUVb1vZfJ9fWSfl4uKkli1NsHjggAkOyxLoMhMFBYHnHztW9vakwNd58B3BHmjbvu8zz/u0ItSpY35lceCACSs9n5v8fO8vFyTvr3gCKel5HztWvmNclsxM/8ee47RvX+nrBfr8FPX/2rv3oKiuOw7gX0B2FyIvpbCAoGiMWNFgJJJFDZPKFCOTaJtpjKGGWBtrxRZDx/eDtkZhTNKxTYxG2yadqUpjm9jUUizFWmtCfBAwoobEQsQ6WXzCIhpe++sf23thBRIX98Hufj8zTOTcs/f+7v6Wzbm/c/fsjRs9n+/z5+84NIf4svze/n5z4YLlx1nq63tvV97LlAmLvvQ1od7dI4/0LzYiIiIiIiJHsXnNcHexatUqqzvSTSYTYmNjXRiR8yhLDQQGdv2u1Vru4uouLs6xcUyb5tj9k2dQ7mS8fXkApRCs0VgKZSEh1stoKDo7LcU+pbDs728pdCvLbih3LiqTEjdvdi2B0dRkmcjpPvGhTFgEBXVNsAQHW/oGB3dNRHSnTJYoEzaNjZbHKcvQNDVZConK0kMaTddSNyZT16SDTmcpYOr1lhiuXrWci7Jv5S7R2+8QVs6vt3az2RKP8r0CTU1dkwLKkjnKcjDKJI1yrNZWSzzKBE73ZSiUyQ6g513Ct98dfCfbgoKsJ/KUJVyUu1G7/7S2Wt7PlImoy5ctz1Vvd6ArS2hoNJZ4lX2Eh1vO+erVrk/TBAd3TfZcvtzz+VSWx+nosPS/556uibiQEMsElkZjKUQrk0fKkkX+/l2vx+Zmy/Pp42MplioF6N7u7P+yu/99fS15VZaQ6n6nrslkiWPIEEsub9zoWjZJmRBU7lRXJhUDAy2xKXfiK5NO99zTNfnn69uV99ufb+X5uf05Uz6d4OtruRNaWTYrKMgSlzIZqkxQtbZ2ffeFckdyS0vX0jO3TzRFRlqee2W5ru53SZtMPWNT/v1VbTExlt+VCTflNa/RWGJWltxSXqfd/wtYYlYmYLtPKMbEWM7l+nVg1ChL7FevAqdPdy3rZDZbcqvTdU1I3r70V2/tgOW5UyYA29osz+HgwV3Ls7S1We4Kv36963WlvI+Gh1uWubp1yzKpqUyKK8tStbd39bt5s+9PbCjHuffe3rcTERERERG5ik3F8PDwcPj5+aFBWej4/xoaGqDX63t9jF6v/9L+yn8bGhoQFRVl1ScpKUntc+nSJat9dHR04Nq1a30eV6vVQttb5YyIBhTlS1+/TC/fzavqvkZ79/3cXrT28ekqTivCw7v+rXx6oPu69sryDUDXpwJ6o0w8KUWwsDDrySedzlKw642yf4Vy7IAAYNiwvo/ZX7dPirmj7rnw8bG8Pr7sNfJlYmJ6tgUFdeWhPwbCvGtwsO3rnrs75fsE3NHQocDDD7s6ii5BQcCECa6OgoiIiIiIyP5s+mokjUaDSZMmoaysTG0zm80oKyuDwWDo9TEGg8GqPwCUlpaq/ePj46HX6636mEwmHD16VO1jMBjQ2NiIiooKtc/BgwdhNpuRkpJiyykQERERERERERERkReyeZmUvLw8ZGdnIzk5GZMnT8aWLVvQ0tKC+fPnAwCeeeYZxMTEoKCgAACQm5uLtLQ0vPzyy8jMzERRURFOnDiBHTt2AAB8fHywdOlSvPDCCxg9ejTi4+Oxbt06REdHY/bs2QCAsWPHYsaMGXjuueewfft2tLe3Y8mSJXjqqacQHR1tp6eCiIiIiIiIiIiIiDyVzcXwOXPm4PLly1i/fj2MRiOSkpJQUlKifgFmfX09fH27bjhPTU3F7t27sXbtWqxevRqjR4/Gvn37kJiYqPZZvnw5WlpasHDhQjQ2NmLq1KkoKSmBTqdT++zatQtLlizB9OnT4evriyeeeAK/+tWv7ubciYiIiIiIiIiIiMhL+IgoX4Hl2UwmE0JCQtDU1IRgb1tIlYiIiMiDcZznvZh7IiIiIs/kqHGeTWuGExERERERERERERG5IxbDiYiIiIiIiIiIiMjjsRhORERERERERERERB6PxXAiIiIiIiIiIiIi8ngshhMRERERERERERGRx2MxnIiIiIiIiIiIiIg8HovhREREREREREREROTxWAwnIiIiIiIiIiIiIo/HYjgREREREREREREReTwWw4mIiIiIiIiIiIjI47EYTkREREREdrF161aMGDECOp0OKSkpOHbs2Jf237t3LxISEqDT6TB+/HgUFxc7KVIiIiIi8kYshhMRERER0V37wx/+gLy8POTn5+PDDz/E/fffj4yMDFy6dKnX/u+//z7mzp2LBQsWoLKyErNnz8bs2bNRXV3t5MiJiIiIyFv4iIi4OghnMJlMCAkJQVNTE4KDg10dDhERERHZCcd5A0NKSgoefPBBvPrqqwAAs9mM2NhY/OhHP8LKlSt79J8zZw5aWlqwf/9+te2hhx5CUlIStm/ffkfHZO6JiIiIPJOjxnmD7LanAU6p+ZtMJhdHQkRERET2pIzvvOQejwGpra0NFRUVWLVqldrm6+uL9PR0lJeX9/qY8vJy5OXlWbVlZGRg3759fR6ntbUVra2t6u9NTU0AOMYnIiIi8jSOGuN7TTG8ubkZABAbG+viSIiIiIjIEZqbmxESEuLqMLzSlStX0NnZicjISKv2yMhIfPzxx70+xmg09trfaDT2eZyCggL87Gc/69HOMT4RERGRZ7p69apdx/heUwyPjo7GhQsXEBQUBB8fH6cc02QyITY2FhcuXODHNt0Y8+gZmEf3xxx6BubRMwy0PIoImpubER0d7epQyMFWrVpldTd5Y2Mjhg8fjvr6ek6EeJmB9j5EzsPcey/m3jsx796rqakJcXFxGDJkiF336zXFcF9fXwwbNswlxw4ODuYfrAdgHj0D8+j+mEPPwDx6hoGURxZCXSs8PBx+fn5oaGiwam9oaIBer+/1MXq93qb+AKDVaqHVanu0h4SEDJjXIjnXQHofIudi7r0Xc++dmHfv5evra9/92XVvRERERETkdTQaDSZNmoSysjK1zWw2o6ysDAaDodfHGAwGq/4AUFpa2md/IiIiIqK75TV3hhMRERERkePk5eUhOzsbycnJmDx5MrZs2YKWlhbMnz8fAPDMM88gJiYGBQUFAIDc3FykpaXh5ZdfRmZmJoqKinDixAns2LHDladBRERERB6MxXAH0mq1yM/P7/WjnOQ+mEfPwDy6P+bQMzCPnoF5pN7MmTMHly9fxvr162E0GpGUlISSkhL1SzLr6+utPuaampqK3bt3Y+3atVi9ejVGjx6Nffv2ITEx8Y6Pydei92LuvRdz772Ye+/EvHsvR+XeR0TErnskIiIiIiIiIiIiIhpguGY4EREREREREREREXk8FsOJiIiIiIiIiIiIyOOxGE5EREREREREREREHo/FcCIiIiIiIiIiIiLyeCyGO9DWrVsxYsQI6HQ6pKSk4NixY64Oif6voKAADz74IIKCghAREYHZs2ejpqbGqs8XX3yBnJwcDB06FIMHD8YTTzyBhoYGqz719fXIzMxEYGAgIiIisGzZMnR0dDjzVOj/CgsL4ePjg6VLl6ptzKF7uHjxIr773e9i6NChCAgIwPjx43HixAl1u4hg/fr1iIqKQkBAANLT0/Hpp59a7ePatWvIyspCcHAwQkNDsWDBAty4ccPZp+K1Ojs7sW7dOsTHxyMgIACjRo3Chg0b0P07upnHgefw4cN47LHHEB0dDR8fH+zbt89qu71y9tFHH2HatGnQ6XSIjY3F5s2bHX1q5GFsHVPv3bsXCQkJ0Ol0GD9+PIqLi50UKdmbLbnfuXMnpk2bhrCwMISFhSE9PZ3XX26sv9fSRUVF8PHxwezZsx0bIDmErXlvbGxETk4OoqKioNVqcd999/E9303ZmvstW7ZgzJgxCAgIQGxsLJ5//nl88cUXToqW7OWrrkd6c+jQITzwwAPQarW499578eabb9p+YCGHKCoqEo1GI7/97W/l9OnT8txzz0loaKg0NDS4OjQSkYyMDHnjjTekurpaqqqqZObMmRIXFyc3btxQ+yxatEhiY2OlrKxMTpw4IQ899JCkpqaq2zs6OiQxMVHS09OlsrJSiouLJTw8XFatWuWKU/Jqx44dkxEjRsiECRMkNzdXbWcOB75r167J8OHD5dlnn5WjR49KbW2tHDhwQM6dO6f2KSwslJCQENm3b5+cPHlSHn/8cYmPj5dbt26pfWbMmCH333+/fPDBB/Lvf/9b7r33Xpk7d64rTskrbdy4UYYOHSr79++Xuro62bt3rwwePFh++ctfqn2Yx4GnuLhY1qxZI2+//bYAkHfeecdquz1y1tTUJJGRkZKVlSXV1dWyZ88eCQgIkNdff91Zp0luztYx9XvvvSd+fn6yefNmOXPmjKxdu1b8/f3l1KlTTo6c7patuX/66adl69atUllZKWfPnpVnn31WQkJC5L///a+TI6e71d9r6bq6OomJiZFp06bJrFmznBMs2Y2teW9tbZXk5GSZOXOmHDlyROrq6uTQoUNSVVXl5Mjpbtma+127dolWq5Vdu3ZJXV2dHDhwQKKiouT55593cuR0t77qeuR2tbW1EhgYKHl5eXLmzBl55ZVXxM/PT0pKSmw6LovhDjJ58mTJyclRf+/s7JTo6GgpKChwYVTUl0uXLgkA+de//iUiIo2NjeLv7y979+5V+5w9e1YASHl5uYhY/mh9fX3FaDSqfbZt2ybBwcHS2trq3BPwYs3NzTJ69GgpLS2VtLQ0tRjOHLqHFStWyNSpU/vcbjabRa/Xy4svvqi2NTY2ilarlT179oiIyJkzZwSAHD9+XO3zt7/9TXx8fOTixYuOC55UmZmZ8r3vfc+q7dvf/rZkZWWJCPPoDm4ffNorZ6+99pqEhYVZvaeuWLFCxowZ4+AzIk9h65j6ySeflMzMTKu2lJQU+cEPfuDQOMn+7vZ6qqOjQ4KCguR3v/udo0IkB+lP7js6OiQ1NVV+/etfS3Z2NovhbsjWvG/btk1GjhwpbW1tzgqRHMTW3Ofk5Mg3vvENq7a8vDyZMmWKQ+Mkx7qTYvjy5ctl3LhxVm1z5syRjIwMm47FZVIcoK2tDRUVFUhPT1fbfH19kZ6ejvLychdGRn1pamoCAAwZMgQAUFFRgfb2dqscJiQkIC4uTs1heXk5xo8fj8jISLVPRkYGTCYTTp8+7cTovVtOTg4yMzOtcgUwh+7i3XffRXJyMr7zne8gIiICEydOxM6dO9XtdXV1MBqNVnkMCQlBSkqKVR5DQ0ORnJys9klPT4evry+OHj3qvJPxYqmpqSgrK8Mnn3wCADh58iSOHDmCRx99FADz6I7slbPy8nI8/PDD0Gg0ap+MjAzU1NTg+vXrTjobclf9GVOXl5f3GBNkZGRwDO5m7HE9dfPmTbS3t6vje3IP/c39z3/+c0RERGDBggXOCJPsrD95f/fdd2EwGJCTk4PIyEgkJiZi06ZN6OzsdFbYZAf9yX1qaioqKirUpVRqa2tRXFyMmTNnOiVmch17jfMG2TMosrhy5Qo6OzutCmwAEBkZiY8//thFUVFfzGYzli5diilTpiAxMREAYDQaodFoEBoaatU3MjISRqNR7dNbjpVt5HhFRUX48MMPcfz48R7bmEP3UFtbi23btiEvLw+rV6/G8ePH8eMf/xgajQbZ2dlqHnrLU/c8RkREWG0fNGgQhgwZwjw6ycqVK2EymZCQkAA/Pz90dnZi48aNyMrKAgDm0Q3ZK2dGoxHx8fE99qFsCwsLc0j85Bn6M6bu6//tfB9xL/a4nlqxYgWio6N7XDTTwNaf3B85cgS/+c1vUFVV5YQIyRH6k/fa2locPHgQWVlZKC4uxrlz57B48WK0t7cjPz/fGWGTHfQn908//TSuXLmCqVOnQkTQ0dGBRYsWYfXq1c4ImVyor3GeyWTCrVu3EBAQcEf7YTGcvF5OTg6qq6tx5MgRV4dCNrhw4QJyc3NRWloKnU7n6nCon8xmM5KTk7Fp0yYAwMSJE1FdXY3t27cjOzvbxdHRnXrrrbewa9cu7N69G+PGjUNVVRWWLl2K6Oho5pGIiJyusLAQRUVFOHToEMeJHq65uRnz5s3Dzp07ER4e7upwyInMZjMiIiKwY8cO+Pn5YdKkSbh48SJefPFFFsM93KFDh7Bp0ya89tprSElJwblz55Cbm4sNGzZg3bp1rg6P3ACL4Q4QHh4OPz8/NDQ0WLU3NDRAr9e7KCrqzZIlS7B//34cPnwYw4YNU9v1ej3a2trQ2NhodWdx9xzq9foe33Cs5Jx5dryKigpcunQJDzzwgNrW2dmJw4cP49VXX8WBAweYQzcQFRWFr3/961ZtY8eOxZ/+9CcAXXloaGhAVFSU2qehoQFJSUlqn0uXLlnto6OjA9euXWMenWTZsmVYuXIlnnrqKQDA+PHjcf78eRQUFCA7O5t5dEP2ypler+91PNT9GER96c+Yuq/XHF9v7uVurqdeeuklFBYW4h//+AcmTJjgyDDJAWzN/X/+8x989tlneOyxx9Q2s9kMwPJppZqaGowaNcqxQdNd68/ffFRUFPz9/eHn56e2jR07FkajEW1tbVZLtNHA1Z/cr1u3DvPmzcP3v/99AJZrj5aWFixcuBBr1qyBry9XhPZUfY3zgoOD7/iucADgK8QBNBoNJk2ahLKyMrXNbDajrKwMBoPBhZGRQkSwZMkSvPPOOzh48GCPj3BPmjQJ/v7+VjmsqalBfX29mkODwYBTp05ZFQJKS0sRHBzco7hH9jd9+nScOnUKVVVV6k9ycjKysrLUfzOHA9+UKVNQU1Nj1fbJJ59g+PDhAID4+Hjo9XqrPJpMJhw9etQqj42NjaioqFD7HDx4EGazGSkpKU44C7p582aPQaefn596Mco8uh975cxgMODw4cNob29X+5SWlmLMmDFcIoW+Un/G1AaDwao/YHnNcQzuXvp7PbV582Zs2LABJSUlVt9nQO7D1twnJCT0uCZ4/PHH8cgjj6CqqgqxsbHODJ/6qT9/81OmTMG5c+fU8SZguY6IiopiIdyN9Cf3fV17AJZaD3kuu43zbPq6TbpjRUVFotVq5c0335QzZ87IwoULJTQ0VIxGo6tDIxH54Q9/KCEhIXLo0CH5/PPP1Z+bN2+qfRYtWiRxcXFy8OBBOXHihBgMBjEYDOr2jo4OSUxMlG9+85tSVVUlJSUl8rWvfU1WrVrlilMiEUlLS5Pc3Fz1d+Zw4Dt27JgMGjRINm7cKJ9++qns2rVLAgMD5fe//73ap7CwUEJDQ+XPf/6zfPTRRzJr1iyJj4+XW7duqX1mzJghEydOlKNHj8qRI0dk9OjRMnfuXFecklfKzs6WmJgY2b9/v9TV1cnbb78t4eHhsnz5crUP8zjwNDc3S2VlpVRWVgoA+cUvfiGVlZVy/vx5EbFPzhobGyUyMlLmzZsn1dXVUlRUJIGBgfL66687/XzJPX3VmHrevHmycuVKtf97770ngwYNkpdeeknOnj0r+fn54u/vL6dOnXLVKVA/2Zr7wsJC0Wg08sc//tFqfN/c3OyqU6B+sjX3t8vOzpZZs2Y5KVqyF1vzXl9fL0FBQbJkyRKpqamR/fv3S0REhLzwwguuOgXqJ1tzn5+fL0FBQbJnzx6pra2Vv//97zJq1Ch58sknXXUK1E9fdT2ycuVKmTdvntq/trZWAgMDZdmyZXL27FnZunWr+Pn5SUlJiU3HZTHcgV555RWJi4sTjUYjkydPlg8++MDVIdH/Aej154033lD73Lp1SxYvXixhYWESGBgo3/rWt+Tzzz+32s9nn30mjz76qAQEBEh4eLj85Cc/kfb2diefDSluL4Yzh+7hL3/5iyQmJopWq5WEhATZsWOH1Xaz2Szr1q2TyMhI0Wq1Mn36dKmpqbHqc/XqVZk7d64MHjxYgoODZf78+bz4dSKTySS5ubkSFxcnOp1ORo4cKWvWrJHW1la1D/M48Pzzn//s9f+F2dnZImK/nJ08eVKmTp0qWq1WYmJipLCw0FmnSB7iy8bUaWlp6mtW8dZbb8l9990nGo1Gxo0bJ3/961+dHDHZiy25Hz58eK/vafn5+c4PnO6arX/33bEY7r5szfv7778vKSkpotVqZeTIkbJx40bp6OhwctRkD7bkvr29XX7605/KqFGjRKfTSWxsrCxevFiuX7/u/MDprnzV9Uh2drakpaX1eExSUpJoNBoZOXKkVR3vTvmI8DMEREREREREREREROTZuGY4EREREREREREREXk8FsOJiIiIiIiIiIiIyOOxGE5EREREREREREREHo/FcCIiIiIiIiIiIiLyeCyGExEREREREREREZHHYzGciIiIiIiIiIiIiDwei+FERERERERERERE5PFYDCciIiIiIiIiIiIij8diOBERERERERERERF5PBbDiYiIiIiIiIiIiMjjsRhORERERERERERERB6PxXAiIiIiIiIiIiIi8nj/A50Uketu/2AOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,6))\n",
    "axs[0].plot(history.history['loss'], color='b', label='Training loss')\n",
    "axs[0].plot(history.history['val_loss'], color='r', label='Validation loss')\n",
    "axs[0].set_title(\"Loss curves\")\n",
    "axs[0].legend(loc='best', shadow=True)\n",
    "print(history.history)\n",
    "if 'accuracy' in history.history:\n",
    "    axs[1].plot(history.history['accuracy'], color='b', label='Training accuracy')\n",
    "    axs[1].plot(history.history['val_accuracy'], color='r', label='Validation accuracy')\n",
    "    axs[1].set_title(\"Accuracy curves\")\n",
    "    axs[1].legend(loc='best', shadow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 創建模型\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(4, input_shape=(1, 23)))\n",
    "\n",
    "# # 編譯模型\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # 訓練模型\n",
    "# model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    " \n",
    "# # 生成預測\n",
    "# trainpred = model.predict(x_train)\n",
    " \n",
    "# # 將標準化後的數據轉換為原始數據\n",
    "# trainpred = scaler.inverse_transform(trainpred)\n",
    " \n",
    "# # 計算 RMSE\n",
    "# trainScore = math.sqrt(mean_squared_error(y_train[0], trainpred[:, 0]))\n",
    " \n",
    "# # 訓練預測\n",
    "# trainpredPlot = np.empty_like(dataset)\n",
    " \n",
    "# # 測試預測\n",
    " \n",
    "# # 繪製所有預測\n",
    "# inversetransform = plt.plot(scaler.inverse_transform(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # 將資料組成變好看一點\n",
    "# x_train = np.reshape(x_train, (x_train.shape[0],1,X.shape[1]))\n",
    "# x_test = np.reshape(x_test, (x_test.shape[0],1,X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (30500, 4, 20)\n",
      "y_train size: (30500,)\n",
      "X_test size: (7624, 4, 20)\n",
      "y_test size: (7624,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train size: {}\".format(x_train.shape))\n",
    "print(\"y_train size: {}\".format(y_train.shape))\n",
    "print(\"X_test size: {}\".format(x_test.shape))\n",
    "print(\"y_test size: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 1, 8)              1024      \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 8)                 544       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,586\n",
      "Trainable params: 1,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential  #打開空白的網路神經網路機\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "model = Sequential() \n",
    "# model.add(Dense(8, input_shape=(x_train.shape[0], x_train.shape[1])))\n",
    "\n",
    "model.add(LSTM(8, input_shape=(1,23),activation=\"relu\",return_sequences=True)) \n",
    "# model.add(LSTM(84, return_sequences= True)) \n",
    "# model.add(Dropout(0.2))\n",
    "model.add(LSTM(8,activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5)) # Dropout層隨機斷開輸入神經元，用於防止過度擬合，斷開比例:0.5\n",
    "#model.add(LSTM(100,return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(50))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "# from keras.optimizers import SGD\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy']) \n",
    "#以compile函數定義損失函數(loss)、優化函數(optimizer)及成效衡量指標(mertrics)\n",
    "model.summary()\n",
    "\n",
    "#3*(32+1)\n",
    "# model = Sequential()model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\n",
    "# 输出还是10个向量，但是输出的维度由16变成了8，也就是（32,10,8）\n",
    "#batch_size: 批尺寸，一次喂给lstm的数据量\n",
    "#time_size: 时间步长，序列数据的长度\n",
    "#feature_size: 特征维度，每个时间点输入数据的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# history = model.fit(x_train, y_train, epochs = epochs, batch_size = batch_size,validation_data= (x_test, y_test),verbose=2)\n",
    "# score, acc = model.evaluate(x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy \n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>year</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>200903</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>200906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>200909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>200912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>201003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b</td>\n",
       "      <td>200903</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b</td>\n",
       "      <td>200906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b</td>\n",
       "      <td>200909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b</td>\n",
       "      <td>200912</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b</td>\n",
       "      <td>201003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b</td>\n",
       "      <td>201006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>c</td>\n",
       "      <td>200903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>c</td>\n",
       "      <td>200906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>c</td>\n",
       "      <td>200909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>c</td>\n",
       "      <td>200912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>c</td>\n",
       "      <td>201003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>c</td>\n",
       "      <td>201006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>c</td>\n",
       "      <td>201009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>c</td>\n",
       "      <td>200812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>d</td>\n",
       "      <td>200903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>d</td>\n",
       "      <td>200906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>d</td>\n",
       "      <td>200909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>d</td>\n",
       "      <td>200912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>d</td>\n",
       "      <td>201003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>e</td>\n",
       "      <td>201012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>e</td>\n",
       "      <td>200903</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>e</td>\n",
       "      <td>200906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>e</td>\n",
       "      <td>200909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    N    year  Y\n",
       "0   a  200903  1\n",
       "1   a  200906  1\n",
       "2   a  200909  1\n",
       "3   a  200912  0\n",
       "4   a  201003  1\n",
       "5   b  200903  1\n",
       "6   b  200906  1\n",
       "7   b  200909  1\n",
       "8   b  200912  1\n",
       "9   b  201003  1\n",
       "10  b  201006  1\n",
       "11  c  200903  0\n",
       "12  c  200906  1\n",
       "13  c  200909  0\n",
       "14  c  200912  0\n",
       "15  c  201003  1\n",
       "16  c  201006  1\n",
       "17  c  201009  1\n",
       "18  c  200812  0\n",
       "19  d  200903  0\n",
       "20  d  200906  0\n",
       "21  d  200909  0\n",
       "22  d  200912  0\n",
       "23  d  201003  0\n",
       "24  e  201012  0\n",
       "25  e  200903  1\n",
       "26  e  200906  1\n",
       "27  e  200909  1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'N' : ['a', 'a', 'a','a','a','b','b','b','b','b','b','c','c','c','c','c','c','c','c','d','d','d','d','d','e','e','e','e'],\n",
    "                   'year':['200903','200906','200909','200912','201003','200903','200906','200909','200912','201003','201006',\n",
    "                           '200903','200906','200909','200912','201003','201006','201009','200812','200903','200906','200909',\n",
    "                           '200912','201003','201012','200903','200906','200909'],\n",
    "                   'Y' : [1,1,1,0,1,1,1,1,1,1,1,0,1,0,0,1,1,1,0,0,0,0,0,0,0,1,1,1]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    N    year  Y  L  D\n",
      "0   a  200903  1  1  0\n",
      "1   a  200906  1  1  0\n",
      "2   a  200909  1  2  0\n",
      "3   a  200912  0  2  0\n",
      "4   a  201003  1  2  0\n",
      "5   b  200903  1  1  0\n",
      "6   b  200906  1  1  0\n",
      "7   b  200909  1  1  0\n",
      "8   b  200912  1  2  0\n",
      "9   b  201003  1  2  0\n",
      "10  b  201006  1  2  0\n",
      "11  c  200903  0  0  0\n",
      "12  c  200906  1  1  0\n",
      "13  c  200909  0  2  0\n",
      "14  c  200912  0  2  0\n",
      "15  c  201003  1  1  0\n",
      "16  c  201006  1  2  0\n",
      "17  c  201009  1  2  0\n",
      "18  c  200812  0  2  0\n",
      "19  d  200903  0  1  0\n",
      "20  d  200906  0  1  0\n",
      "21  d  200909  0  2  0\n",
      "22  d  200912  0  2  0\n",
      "23  d  201003  0  2  0\n",
      "24  e  201012  0  0  0\n",
      "25  e  200903  1  2  0\n",
      "26  e  200906  1  2  0\n",
      "27  e  200909  1  2  0\n"
     ]
    }
   ],
   "source": [
    "x = 4\n",
    "n = 1\n",
    "o = 0\n",
    "t = 0\n",
    "tt = 0\n",
    "\n",
    "df[\"L\"]=0\n",
    "df[\"D\"]=0\n",
    "for name, group in df.groupby('N'):      \n",
    "    if(numpy.any(group['Y']!= 0) ):\n",
    "        if(len(group)-(x-1)<0):\n",
    "            group [\"L\"]=0\n",
    "        else:\n",
    "            \n",
    "            for i in range(t,t+len(group)-x+1):    \n",
    "                if (numpy.all(group[i-t:i+n-t]['Y']==1)):\n",
    "                    group.at [i,\"L\"]=1\n",
    "                else:\n",
    "                    group.at [i,\"L\"]=0                         \n",
    "    else:\n",
    "\n",
    "        if(len(group)-(x-1)<0):\n",
    "            group [\"L\"]= 0\n",
    "        else:\n",
    "            for i in range(t,t+len(group)-x+1):    \n",
    "                if (numpy.all(group[i-t:i+x-t]['Y']==0)):\n",
    "                    group.at [i,\"L\"]= 1\n",
    "                else:\n",
    "                    group.at [i,\"L\"]= 0    \n",
    "    df[t:t+len(group)]=group[:len(group)]        \n",
    "    t+=len(group)\n",
    "\n",
    "for name, group in df.groupby('N'):\n",
    "    for d in range(len(group)):\n",
    "        va_a = group.Y[d:d+1]\n",
    "        if va_a.loc[d+o] == 1 or 2:\n",
    "#             print(d+3)\n",
    "#             print(group)\n",
    "            if d+3 < len(group):\n",
    "                for j in range(d+1,d+4):\n",
    "                    va_l = group.L[j:j+1]\n",
    "#                     print(j+t)\n",
    "                    if va_l.loc[j+o] == 0:\n",
    "                        group.at [j+o,\"L\"]= 2\n",
    "    df[o:o+len(group)]=group[:len(group)] \n",
    "    o = o+len(group) \n",
    "    \n",
    "print (df)       \n",
    "\n",
    "#     print(t+=len(group))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>year</th>\n",
       "      <th>Y</th>\n",
       "      <th>L</th>\n",
       "      <th>D</th>\n",
       "      <th>L</th>\n",
       "      <th>L</th>\n",
       "      <th>L</th>\n",
       "      <th>D</th>\n",
       "      <th>D</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>200903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>201003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b</td>\n",
       "      <td>200903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b</td>\n",
       "      <td>200906</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>c</td>\n",
       "      <td>200903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>c</td>\n",
       "      <td>200912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>c</td>\n",
       "      <td>200812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>d</td>\n",
       "      <td>200903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    N    year    Y    L    D  L    L    L  D    D    D\n",
       "1   a  200903  1.0  1.0  0.0  1  2.0  1.0  0  0.0  0.0\n",
       "5   a  201003  1.0  2.0  0.0  1  1.0  1.0  0  0.0  0.0\n",
       "6   b  200903  1.0  1.0  0.0  1  1.0  1.0  0  0.0  0.0\n",
       "7   b  200906  1.0  1.0  0.0  1  2.0  1.0  0  0.0  0.0\n",
       "12  c  200903  0.0  0.0  0.0  1  2.0  1.0  0  0.0  0.0\n",
       "15  c  200912  0.0  2.0  0.0  1  2.0  1.0  0  0.0  0.0\n",
       "19  c  200812  0.0  2.0  0.0  1  1.0  1.0  0  0.0  0.0\n",
       "20  d  200903  0.0  1.0  0.0  1  2.0  1.0  0  0.0  0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################111111\n",
    "window_size = 3\n",
    "series_s = df.copy()\n",
    "# gg=0\n",
    "\n",
    "\n",
    "indexNames = df[ df['L'] != 1 ].index\n",
    "df.drop(indexNames , inplace=True)\n",
    "\n",
    "# for mm,group in df.groupby('N'):\n",
    "#     for kk in range(len(group)):\n",
    "for ii in range(window_size):\n",
    "    df = pd.concat([series_s.shift((ii-1)),df[['L','D']]], axis = 1)\n",
    "                \n",
    "df.dropna(axis=0, inplace=True)\n",
    "#             df[gg:gg+len(group)]=group[:len(group)]\n",
    "#     gg+=len(group) \n",
    "df\n",
    "#     df.dropna(axis=0, inplace=True)\n",
    "# group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
